{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Wikipedia articles for search\n",
    "\n",
    "This notebook shows how we prepared a dataset of Wikipedia articles for search, used in [Question_answering_using_embeddings.ipynb](Question_answering_using_embeddings.ipynb).\n",
    "\n",
    "Procedure:\n",
    "\n",
    "0. Prerequisites: Import libraries, set API key (if needed)\n",
    "1. Collect: We download a few hundred Wikipedia articles about the 2022 Olympics\n",
    "2. Chunk: Documents are split into short, semi-self-contained sections to be embedded\n",
    "3. Embed: Each section is embedded with the OpenAI API\n",
    "4. Store: Embeddings are saved in a CSV file (for large datasets, use a vector database)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Prerequisites\n",
    "\n",
    "##### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T11:40:45.748077Z",
     "start_time": "2025-05-06T11:40:45.738438Z"
    }
   },
   "source": [
    "import mwclient\n",
    "import mwparserfromhell\n",
    "from openai import OpenAI\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import tiktoken"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T11:40:53.628147Z",
     "start_time": "2025-05-06T11:40:52.393077Z"
    }
   },
   "cell_type": "code",
   "source": [
    "_key = os.getenv( 'OPENAI_API_KEY' )\n",
    "client = OpenAI( api_key=_key )"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install any missing libraries with `pip install` in your terminal. E.g.,\n",
    "\n",
    "```zsh\n",
    "pip install openai\n",
    "```\n",
    "\n",
    "(You can also do this in a notebook cell with `!pip install openai`.)\n",
    "\n",
    "If you install any libraries, be sure to restart the notebook kernel."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Collect documents\n",
    "\n",
    "In this example, we'll download a few hundred Wikipedia articles related to the 2022 Winter Olympics."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T11:41:34.976757Z",
     "start_time": "2025-05-06T11:41:32.806567Z"
    }
   },
   "source": [
    "# get Wikipedia pages about the 2022 Winter Olympics\n",
    "\n",
    "CATEGORY_TITLE = 'Category:2022 Winter Olympics'\n",
    "WIKI_SITE = 'en.wikipedia.org'\n",
    "\n",
    "\n",
    "def titles_from_category(\n",
    "\tcategory: mwclient.listing.Category, max_depth: int\n",
    ") -> set[ str ]:\n",
    "\t'''\n",
    "\n",
    "\t\tReturn a pairs of page titles in a given Wiki category and its subcategories.\n",
    "\n",
    "\t'''\n",
    "\ttitles = set( )\n",
    "\tfor cm in category.members( ):\n",
    "\t\tif type( cm ) == mwclient.page.Page:\n",
    "\t\t\t# ^type() used instead of isinstance() to catch match w/ no inheritance\n",
    "\t\t\ttitles.add( cm.name )\n",
    "\t\telif isinstance( cm, mwclient.listing.Category ) and max_depth > 0:\n",
    "\t\t\tdeeper_titles = titles_from_category( cm, max_depth=max_depth - 1 )\n",
    "\t\t\ttitles.update( deeper_titles )\n",
    "\treturn titles\n",
    "\n",
    "\n",
    "site = mwclient.Site( WIKI_SITE )\n",
    "category_page = site.pages[ CATEGORY_TITLE ]\n",
    "titles = titles_from_category( category_page, max_depth=1 )\n",
    "# ^note: max_depth=1 means we go one level deep in the category tree\n",
    "print( f'Found {len( titles )} article titles in {CATEGORY_TITLE}.' )\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 179 article titles in Category:2022 Winter Olympics.\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Chunk documents\n",
    "\n",
    "Now that we have our reference documents, we need to prepare them for search.\n",
    "\n",
    "Because GPT can only read a limited amount of text at once, we'll split each document into chunks short enough to be read.\n",
    "\n",
    "For this specific example on Wikipedia articles, we'll:\n",
    "- Discard less relevant-looking sections like External Links and Footnotes\n",
    "- Clean up the text by removing reference tags (e.g., <ref>), whitespace, and super short sections\n",
    "- Split each article into sections\n",
    "- Prepend titles and subtitles to each section's text, to help GPT understand the context\n",
    "- If a section is long (say, > 1,600 tokens), we'll recursively split it into smaller sections, trying to split along semantic boundaries like paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T11:42:01.471032Z",
     "start_time": "2025-05-06T11:42:01.445708Z"
    }
   },
   "source": [
    "# define functions to split Wikipedia pages into sections\n",
    "\n",
    "SECTIONS_TO_IGNORE = [\n",
    "\t'See also',\n",
    "\t'References',\n",
    "\t'External links',\n",
    "\t'Further reading',\n",
    "\t'Footnotes',\n",
    "\t'Bibliography',\n",
    "\t'Sources',\n",
    "\t'Citations',\n",
    "\t'Literature',\n",
    "\t'Footnotes',\n",
    "\t'Notes and references',\n",
    "\t'Photo gallery',\n",
    "\t'Works cited',\n",
    "\t'Photos',\n",
    "\t'Gallery',\n",
    "\t'Notes',\n",
    "\t'References and sources',\n",
    "\t'References and notes',\n",
    "]\n",
    "\n",
    "\n",
    "def all_subsections_from_section(\n",
    "\tsection: mwparserfromhell.wikicode.Wikicode,\n",
    "\tparent_titles: list[ str ],\n",
    "\tsections_to_ignore: set[ str ],\n",
    ") -> list[ tuple[ list[ str ], str ] ]:\n",
    "\t'''\n",
    "\n",
    "        From a Wikipedia section, return a flattened list of all nested subsections.\n",
    "        Each subsection is a tuple, where:\n",
    "            - the first element is a list of parent subtitles, starting with the page title\n",
    "            - the second element is the pages of the subsection (but not any children)\n",
    "\n",
    "    '''\n",
    "\theadings = [ str( h ) for h in section.filter_headings( ) ]\n",
    "\ttitle = headings[ 0 ]\n",
    "\tif title.strip( '=' + ' ' ) in sections_to_ignore:\n",
    "\t\t# ^wiki headings are wrapped like '== Heading =='\n",
    "\t\treturn [ ]\n",
    "\ttitles = parent_titles + [ title ]\n",
    "\tfull_text = str( section )\n",
    "\tsection_text = full_text.split( title )[ 1 ]\n",
    "\tif len( headings ) == 1:\n",
    "\t\treturn [ (titles, section_text) ]\n",
    "\telse:\n",
    "\t\tfirst_subtitle = headings[ 1 ]\n",
    "\t\tsection_text = section_text.split( first_subtitle )[ 0 ]\n",
    "\t\tresults = [ (titles, section_text) ]\n",
    "\t\tfor subsection in section.get_sections( levels=[ len( titles ) + 1 ] ):\n",
    "\t\t\tresults.extend( all_subsections_from_section( subsection, titles, sections_to_ignore ) )\n",
    "\t\treturn results\n",
    "\n",
    "\n",
    "def all_subsections_from_title(\n",
    "\ttitle: str,\n",
    "\tsections_to_ignore: set[ str ] = SECTIONS_TO_IGNORE,\n",
    "\tsite_name: str = WIKI_SITE,\n",
    ") -> list[ tuple[ list[ str ], str ] ]:\n",
    "\t'''\n",
    "\n",
    "        From a Wikipedia page title, return a flattened list of all nested subsections.\n",
    "        Each subsection is a tuple, where:\n",
    "            - the first element is a list of parent subtitles, starting with the page title\n",
    "            - the second element is the pages of the subsection (but not any children)\n",
    "\n",
    "    '''\n",
    "\tsite = mwclient.Site( site_name )\n",
    "\tpage = site.pages[ title ]\n",
    "\ttext = page.text( )\n",
    "\tparsed_text = mwparserfromhell.parse( text )\n",
    "\theadings = [ str( h ) for h in parsed_text.filter_headings( ) ]\n",
    "\tif headings:\n",
    "\t\tsummary_text = str( parsed_text ).split( headings[ 0 ] )[ 0 ]\n",
    "\telse:\n",
    "\t\tsummary_text = str( parsed_text )\n",
    "\tresults = [ ([ title ], summary_text) ]\n",
    "\tfor subsection in parsed_text.get_sections( levels=[ 2 ] ):\n",
    "\t\tresults.extend( all_subsections_from_section( subsection, [ title ], sections_to_ignore ) )\n",
    "\treturn results\n"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T11:44:51.987030Z",
     "start_time": "2025-05-06T11:42:06.899194Z"
    }
   },
   "source": [
    "# split pages into sections\n",
    "# may take ~1 minute per 100 articles\n",
    "wikipedia_sections = [ ]\n",
    "for title in titles:\n",
    "\twikipedia_sections.extend( all_subsections_from_title( title ) )\n",
    "print( f'Found {len( wikipedia_sections )} sections in {len( titles )} pages.' )\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1856 sections in 179 pages.\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T11:45:05.727956Z",
     "start_time": "2025-05-06T11:45:05.701152Z"
    }
   },
   "source": [
    "# clean pages\n",
    "def clean_section( section: tuple[ list[ str ], str ] ) -> tuple[ list[ str ], str ]:\n",
    "\t'''\n",
    "\n",
    "        Return a cleaned_lines up section with:\n",
    "            - <ref>xyz</ref> patterns removed\n",
    "            - leading/trailing whitespace removed\n",
    "\n",
    "    '''\n",
    "\ttitles, text = section\n",
    "\ttext = re.sub( r'<ref.*?</ref>', '', text )\n",
    "\ttext = text.strip( )\n",
    "\treturn (titles, text)\n",
    "\n",
    "\n",
    "wikipedia_sections = [ clean_section( ws ) for ws in wikipedia_sections ]\n",
    "\n",
    "\n",
    "# filter out short/blank sections\n",
    "def keep_section( section: tuple[ list[ str ], str ] ) -> bool:\n",
    "\t'''\n",
    "\n",
    "        Return True if the section should be kept, False otherwise.\n",
    "\n",
    "    '''\n",
    "\ttitles, text = section\n",
    "\tif len( text ) < 16:\n",
    "\t\treturn False\n",
    "\telse:\n",
    "\t\treturn True\n",
    "\n",
    "\n",
    "original_num_sections = len( wikipedia_sections )\n",
    "wikipedia_sections = [ ws for ws in wikipedia_sections if keep_section( ws ) ]\n",
    "print(\n",
    "\tf'Filtered out {original_num_sections - len( wikipedia_sections )} sections, leaving {len( wikipedia_sections )} sections.' )\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered out 0 sections, leaving 1765 sections.\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T11:45:09.874045Z",
     "start_time": "2025-05-06T11:45:09.829428Z"
    }
   },
   "source": [
    "# print example data\n",
    "for ws in wikipedia_sections[ :5 ]:\n",
    "\tprint( ws[ 0 ] )\n",
    "\tdisplay( ws[ 1 ][ :77 ] + '...' )\n",
    "\tprint( )\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Uzbekistan at the 2022 Winter Olympics']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'{{infobox country at games\\n|NOC = UZB\\n|NOCname = [[National Olympic Committee...'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "['Uzbekistan at the 2022 Winter Olympics', '==Competitors==']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The following is the list of number of competitors participating at the Games...'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "['Uzbekistan at the 2022 Winter Olympics', '==Alpine skiing==']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'{{main article|Alpine skiing at the 2022 Winter Olympics|Alpine skiing at the...'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "['Chile at the 2022 Winter Olympics']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'{{infobox country at games\\n|NOC = CHI\\n|NOCname = [[Chilean Olympic Committee]...'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "['Chile at the 2022 Winter Olympics', '==Competitors==']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The following is the list of number of competitors who participated at the Ga...'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll recursively split long sections into smaller sections.\n",
    "\n",
    "There's no perfect recipe for splitting text into sections.\n",
    "\n",
    "Some tradeoffs include:\n",
    "- Longer sections may be better for questions that require more context\n",
    "- Longer sections may be worse for retrieval, as they may have more topics muddled together\n",
    "- Shorter sections are better for reducing costs (which are proportional to the number of tokens)\n",
    "- Shorter sections allow more sections to be retrieved, which may help with recall\n",
    "- Overlapping sections may help prevent answers from being cut by section boundaries\n",
    "\n",
    "Here, we'll use a simple approach and limit sections to 1,600 tokens each, recursively halving any sections that are too long. To avoid cutting in the middle of useful sentences, we'll split along paragraph boundaries when possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_MODEL = 'gpt-4o-mini'\n",
    "\n",
    "\n",
    "def num_tokens( text: str, model: str = GPT_MODEL ) -> int:\n",
    "\t'''Return the num of tokens in a string.'''\n",
    "\tencoding = tiktoken.encoding_for_model( model )\n",
    "\treturn len( encoding.encode( text ) )\n",
    "\n",
    "\n",
    "def halved_by_delimiter( string: str, delimiter: str = '\\n' ) -> list[ str, str ]:\n",
    "\t'''Split a string in two, on a delimiter, trying to balance tokens on each side.'''\n",
    "\tchunks = string.split( delimiter )\n",
    "\tif len( chunks ) == 1:\n",
    "\t\treturn [ string, '' ]  # no delimiter found\n",
    "\telif len( chunks ) == 2:\n",
    "\t\treturn chunks  # no need to search for halfway point\n",
    "\telse:\n",
    "\t\ttotal_tokens = num_tokens( string )\n",
    "\t\thalfway = total_tokens // 2\n",
    "\t\tbest_diff = halfway\n",
    "\t\tfor i, chunk in enumerate( chunks ):\n",
    "\t\t\tleft = delimiter.join( chunks[ : i + 1 ] )\n",
    "\t\t\tleft_tokens = num_tokens( left )\n",
    "\t\t\tdiff = abs( halfway - left_tokens )\n",
    "\t\t\tif diff >= best_diff:\n",
    "\t\t\t\tbreak\n",
    "\t\t\telse:\n",
    "\t\t\t\tbest_diff = diff\n",
    "\t\tleft = delimiter.join( chunks[ :i ] )\n",
    "\t\tright = delimiter.join( chunks[ i: ] )\n",
    "\t\treturn [ left, right ]\n",
    "\n",
    "\n",
    "def truncated_string(\n",
    "\tstring: str,\n",
    "\tmodel: str,\n",
    "\tmax_tokens: int,\n",
    "\tprint_warning: bool = True,\n",
    ") -> str:\n",
    "\t'''Truncate a string to a maximum num of tokens.'''\n",
    "\tencoding = tiktoken.encoding_for_model( model )\n",
    "\tencoded_string = encoding.encode( string )\n",
    "\ttruncated_string = encoding.decode( encoded_string[ :max_tokens ] )\n",
    "\tif print_warning and len( encoded_string ) > max_tokens:\n",
    "\t\tprint(\n",
    "\t\t\tf'Warning: Truncated string from {len( encoded_string )} tokens to {max_tokens} tokens.' )\n",
    "\treturn truncated_string\n",
    "\n",
    "\n",
    "def split_strings_from_subsection(\n",
    "\tsubsection: tuple[ list[ str ], str ],\n",
    "\tmax_tokens: int = 1000,\n",
    "\tmodel: str = GPT_MODEL,\n",
    "\tmax_recursion: int = 5,\n",
    ") -> list[ str ]:\n",
    "\t'''\n",
    "    Split a subsection into a list of subsections, each with no more than max_tokens.\n",
    "    Each subsection is a tuple of parent titles [H1, H2, ...] and pages (str).\n",
    "    '''\n",
    "\ttitles, text = subsection\n",
    "\tstring = '\\n\\n'.join( titles + [ text ] )\n",
    "\tnum_tokens_in_string = num_tokens( string )\n",
    "\t# if min is fine, return string\n",
    "\tif num_tokens_in_string <= max_tokens:\n",
    "\t\treturn [ string ]\n",
    "\t# if recursion hasn't found a split after X iterations, just trunc\n",
    "\telif max_recursion == 0:\n",
    "\t\treturn [ truncated_string( string, model=model, max_tokens=max_tokens ) ]\n",
    "\t# otherwise, split in half and recurse\n",
    "\telse:\n",
    "\t\ttitles, text = subsection\n",
    "\t\tfor delimiter in [ '\\n\\n', '\\n', '. ' ]:\n",
    "\t\t\tleft, right = halved_by_delimiter( text, delimiter=delimiter )\n",
    "\t\t\tif left == '' or right == '':\n",
    "\t\t\t\t# if either half is empty, retry with a more fine-grained delimiter\n",
    "\t\t\t\tcontinue\n",
    "\t\t\telse:\n",
    "\t\t\t\t# recurse on each half\n",
    "\t\t\t\tresults = [ ]\n",
    "\t\t\t\tfor half in [ left, right ]:\n",
    "\t\t\t\t\thalf_subsection = (titles, half)\n",
    "\t\t\t\t\thalf_strings = split_strings_from_subsection(\n",
    "\t\t\t\t\t\thalf_subsection,\n",
    "\t\t\t\t\t\tmax_tokens=max_tokens,\n",
    "\t\t\t\t\t\tmodel=model,\n",
    "\t\t\t\t\t\tmax_recursion=max_recursion - 1,\n",
    "\t\t\t\t\t)\n",
    "\t\t\t\t\tresults.extend( half_strings )\n",
    "\t\t\t\treturn results\n",
    "\t# otherwise no split was found, so just trunc (should be very rare)\n",
    "\treturn [ truncated_string( string, model=model, max_tokens=max_tokens ) ]\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# split sections into chunks\n",
    "MAX_TOKENS = 1600\n",
    "wikipedia_strings = [ ]\n",
    "for section in wikipedia_sections:\n",
    "\twikipedia_strings.extend( split_strings_from_subsection( section, max_tokens=MAX_TOKENS ) )\n",
    "\n",
    "print(\n",
    "\tf'{len( wikipedia_sections )} Wikipedia sections split into {len( wikipedia_strings )} strings.' )\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# print example data\n",
    "print( wikipedia_strings[ 1 ] )\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Embed document chunks\n",
    "\n",
    "Now that we've split our library into shorter self-contained strings, we can compute embeddings for each.\n",
    "\n",
    "(For large embedding jobs, use a script like [api_request_parallel_processor.py](api_request_parallel_processor.py) to parallelize requests while throttling to stay under rate limits.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0 to 999\n",
      "Batch 1000 to 1999\n",
      "Batch 2000 to 2999\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_MODEL = 'text-embedding-3-small'\n",
    "BATCH_SIZE = 1000  # you can submit up to 2048 embedding inputs per request\n",
    "\n",
    "embeddings = [ ]\n",
    "for batch_start in range( 0, len( wikipedia_strings ), BATCH_SIZE ):\n",
    "\tbatch_end = batch_start + BATCH_SIZE\n",
    "\tbatch = wikipedia_strings[ batch_start:batch_end ]\n",
    "\tprint( f'Batch {batch_start} to {batch_end - 1}' )\n",
    "\tresponse = client.embeddings.create( model=EMBEDDING_MODEL, input=batch )\n",
    "\tfor i, be in enumerate( response.data ):\n",
    "\t\tassert i == be.index  # double check vectors are in same order as text\n",
    "\tbatch_embeddings = [ e.embedding for e in response.data ]\n",
    "\tembeddings.extend( batch_embeddings )\n",
    "\n",
    "df = pd.DataFrame( { 'pages': wikipedia_strings, 'embedding': embeddings } )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Store document chunks and embeddings\n",
    "\n",
    "Because this example only uses a few thousand strings, we'll store them in a CSV file.\n",
    "\n",
    "(For larger datasets, use a vector database, which will be more performant.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save document chunks and vectors\n",
    "\n",
    "SAVE_PATH = 'data/winter_olympics_2022.csv'\n",
    "\n",
    "df.to_csv( SAVE_PATH, index=False )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
