{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# NLTK\n",
    "___\n"
   ],
   "id": "700c6019c3802579"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 📦 One-Time Setup (NLTK Resources)",
   "id": "17b64a5a35525229"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import io\n",
    "\n",
    "# NLTK needs to download items onces\n",
    "import nltk\n",
    "from isort.format import remove_whitespace\n",
    "\n",
    "\n",
    "nltk.download( 'punkt' )\n",
    "nltk.download( 'punkt_tab' )\n",
    "nltk.download( 'stopwords' )\n",
    "nltk.download( 'wordnet' )\n",
    "nltk.download( 'omw-1.4' )\n"
   ],
   "id": "ea993d341972b4d9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Load Dependencies",
   "id": "3957678e92123d8f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T14:30:27.888896Z",
     "start_time": "2025-05-14T14:30:27.871333Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "import re\n",
    "import os\n",
    "from pathlib import Path\n",
    "import openai as OpenAI\n",
    "import pandas as pd\n",
    "import time\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "import pickle\n",
    "import unicodedata\n",
    "from bs4 import BeautifulSoup\n",
    "from textblob import Word\n",
    "from collections import Counter\n",
    "from typing import Any, List, Tuple, Optional, Union, Dict\n",
    "import ipywidgets as widgets, IPython, platform, ipywidgets, jupyterlab\n"
   ],
   "id": "9a5c2c52dd74d494",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Text Processing",
   "id": "ee23478183ced671"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### ✅ Checklist\n",
    "\n",
    "| Step | Task                     | Function/Library               |\n",
    "|------|--------------------------|--------------------------------|\n",
    "| 1    | Load Text                | `open()`, `pandas.read_csv()` |\n",
    "| 2    | Convert to Lowercase     | `.lower()`                     |\n",
    "| 3    | Remove Punctuation       | `string.punctuation`           |\n",
    "| 4    | Remove Numbers           | `re.sub()`                     |\n",
    "| 5    | Trim Whitespaces         | `' '.join()`                   |\n",
    "| 6    | Tokenization             | `nltk.word_tokenize()`         |\n",
    "| 7    | Remove Stopwords         | `nltk.corpus.stopwords`        |\n",
    "| 8    | Lemmatization/Stemming   | `WordNetLemmatizer`, `PorterStemmer` |\n",
    "| 9    | Reconstruct Clean Text   | `' '.join()`                   |\n",
    "| 10   | (Optional) Spellcheck    | `TextBlob.correct()`           |\n",
    "| 11   | Vectorization for ML     | `TfidfVectorizer`, `CountVectorizer`, Word Embeddings |\n",
    "\n",
    "\n",
    "___"
   ],
   "id": "947b8eb1368fe0c4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1.  Load File",
   "id": "36fbc578426999c8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T14:27:09.611342Z",
     "start_time": "2025-05-14T14:27:09.599830Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === Load Raw Text ===\n",
    "file_path = '<url to file>'\n",
    "_rawtext = ''\n",
    "\n",
    "\n",
    "def load_text( file_path ):\n",
    "\twith open( file_path, 'r', encoding='utf-8' ) as f:\n",
    "\t\t_rawtext = f.read( )\n",
    "\t\treturn _rawtext\n"
   ],
   "id": "e3d5cc42b6dd4e3b",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2.  Clean Whitespace\n",
    "- Consecutive whitespace reduced to a single space\n",
    "- Leading/trailing spaces removed\n",
    "- Blank lines removed"
   ],
   "id": "ea7db4bf069f99e1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T14:27:13.905553Z",
     "start_time": "2025-05-14T14:27:13.895540Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def clean_whitespace( text: str ) -> str:\n",
    "\t\"\"\"\n",
    "\n",
    "        Removes extra spaces and blank tokens from the path pages.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        pages : str\n",
    "            The raw path pages path to be cleaned_lines.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        str\n",
    "            A cleaned_lines pages path with:\n",
    "                - Consecutive whitespace reduced to a single space\n",
    "                - Leading/trailing spaces removed\n",
    "                - Blank tokens removed\n",
    "\n",
    "    \"\"\"\n",
    "\t# Replace multiple spaces or tabs with a single space\n",
    "\ttext = re.sub( r'[ \\t]+', ' ', text )\n",
    "\n",
    "\t# Remove empty tokens\n",
    "\tcleaned_lines = [ line for line in text if line ]\n",
    "\n",
    "\t# Join tokens back into a single string\n",
    "\tcleaned_text = '\\n'.join( cleaned_lines )\n",
    "\n",
    "\treturn cleaned_text"
   ],
   "id": "14c5765f6bf2af38",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3. Normalize",
   "id": "63bd713fbba95faa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T16:00:21.027664Z",
     "start_time": "2025-05-14T16:00:21.018301Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def normalize( text: str ) -> str:\n",
    "\t\"\"\"\n",
    "\n",
    "        Normalizes the path pages path.\n",
    "\n",
    "        This function:\n",
    "          - Converts pages to lowercase\n",
    "          - Removes accented characters (e.g., é -> e)\n",
    "          - Removes leading/trailing spaces\n",
    "          - Collapses multiple whitespace characters into a single space\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        pages : str\n",
    "            The raw path pages path to be normalized.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        str\n",
    "            A normalized, cleaned_lines version of the path path.\n",
    "\n",
    "    \"\"\"\n",
    "\treturn unicodedata.normalize( 'NFKD', text ).encode( 'ascii', 'ignore' ).decode( 'utf-8' )"
   ],
   "id": "82515d328d466a74",
   "outputs": [],
   "execution_count": 157
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T14:27:18.966357Z",
     "start_time": "2025-05-14T14:27:18.956062Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### 3. Collapse Space\n",
    "def collapse_space( text: str ) -> str:\n",
    "\t\"\"\"\n",
    "\n",
    "        Collapses two or more\n",
    "        consecutive whitespace characters into a single space.\n",
    "\n",
    "        This function:\n",
    "          - Converts pages to lowercase\n",
    "          - Removes accented characters (e.g., é -> e)\n",
    "          - Removes leading/trailing spaces\n",
    "          - Collapses multiple whitespace characters into a single space\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        text : str\n",
    "            The raw text.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        str\n",
    "            A collapsed string version of the text.\n",
    "\n",
    "    \"\"\"\n",
    "\tparts = [ ]\n",
    "\tin_space = False\n",
    "\tfor char in text:\n",
    "\t\tif char == ' ':\n",
    "\t\t\tif not in_space:\n",
    "\t\t\t\tparts.append( char )\n",
    "\t\t\t\tin_space = True\n",
    "\t\telse:\n",
    "\t\t\tparts.append( char )\n",
    "\t\t\tin_space = False\n",
    "\n",
    "\treturn ''.join( parts )"
   ],
   "id": "139e8a9deec449a9",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 4. Remove Punctuation",
   "id": "5e0ee21b82c3c43"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T14:27:21.673716Z",
     "start_time": "2025-05-14T14:27:21.667498Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def remove_punctuation( text: str ) -> str:\n",
    "\t\"\"\"\n",
    "\n",
    "        Replace all punctuation\n",
    "        in the input text with a space using regular expressions.\n",
    "\n",
    "        Args:\n",
    "            text (str): The input string to process.\n",
    "\n",
    "        Returns:\n",
    "            str: The processed string with punctuation replaced by spaces.\n",
    "\n",
    "    \"\"\"\n",
    "\treturn re.sub( r\"[^\\w\\s]\", ' ', text )"
   ],
   "id": "3ec4704593ac2991",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 5. Trim Whitespace\n",
    "\n",
    "- Removes leading and trailing whitespace\n",
    "- Replaces multiple internal spaces with a single space"
   ],
   "id": "5234418835bd42fa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T14:27:24.355704Z",
     "start_time": "2025-05-14T14:27:24.312668Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def trim_whitespace( text: str ) -> str:\n",
    "\t\"\"\"\n",
    "\n",
    "        Trims whitespace from the path pages path.\n",
    "\n",
    "        This function:\n",
    "          - Removes leading and trailing whitespace\n",
    "          - Replaces multiple internal spaces with a single space\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        pages : str\n",
    "            The raw path path with potential extra whitespace.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        str\n",
    "            The cleaned_lines path with trimmed and normalized whitespace.\n",
    "\n",
    "    \"\"\"\n",
    "\t# Replace multiple whitespace characters (spaces, tabs, etc.) with a single space\n",
    "\tcleaned_text = re.sub( r'\\s+', ' ', text )\n",
    "\treturn cleaned_text"
   ],
   "id": "37ade6f5b1d34922",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 6. Lemmatize\n",
    "- Reduces words to their base or root form.\n",
    "- Converts text to lowercase\n",
    "- Tokenizes the text into words\n",
    "- Lemmatizes each token using WordNetLemmatizer\n",
    "- Reconstructs the lemmatized tokens into a single string"
   ],
   "id": "5f4db8f82f926ae"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T14:27:27.030809Z",
     "start_time": "2025-05-14T14:27:27.015358Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def lemmatize( text: str ) -> str:\n",
    "\t\"\"\"\n",
    "\n",
    "        Performs lemmatization on the path pages path.\n",
    "\n",
    "        This function:\n",
    "          - Converts pages to lowercase\n",
    "          - Tokenizes the pages into words\n",
    "          - Lemmatizes each token using WordNetLemmatizer\n",
    "          - Reconstructs the lemmatized tokens into a single path\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        pages : str\n",
    "            The path pages path to be lemmatized.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        str\n",
    "            A path with all words lemmatized.\n",
    "\n",
    "    \"\"\"\n",
    "\t# Initialize lemmatizer\n",
    "\tlemmatizer = WordNetLemmatizer( )\n",
    "\n",
    "\tlower_case = text.lower( )\n",
    "\t# Convert to lowercase and tokenize_text\n",
    "\ttokens = word_tokenize( lower_case )\n",
    "\n",
    "\t# Lemmatize each token\n",
    "\tlemmatized_tokens = [ lemmatizer.lemmatize( token ) for token in tokens ]\n",
    "\n",
    "\t# Join tokens back to a path\n",
    "\tlemmatized_text = ' '.join( lemmatized_tokens )\n",
    "\n",
    "\treturn lemmatized_text"
   ],
   "id": "93dcb509d45e16f5",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 7. Tokenize\n",
    "\n",
    "- Converts text to lowercase\n",
    "- Uses NLTK's word_tokenize to split the text into words and punctuation tokens"
   ],
   "id": "76b7a3f02f7d210"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T14:27:30.806122Z",
     "start_time": "2025-05-14T14:27:30.798797Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def tokenize( text: str ) -> list:\n",
    "\t\"\"\"\n",
    "\n",
    "        Tokenizes the path pages path into individual word tokens.\n",
    "\n",
    "        This function:\n",
    "          - Converts pages to lowercase\n",
    "          - Uses NLTK's word_tokenize to split the pages into words and punctuation tokens\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        pages : str\n",
    "            The raw path pages path to be tokenized.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        get_list\n",
    "            A get_list of tokens (words and punctuation) extracted from the pages.\n",
    "\n",
    "    \"\"\"\n",
    "\t# Convert to lowercase\n",
    "\t_lower = text.lower( )\n",
    "\n",
    "\t# Tokenize\n",
    "\ttokens = word_tokenize( _lower )\n",
    "\n",
    "\treturn tokens\n"
   ],
   "id": "3e56143f4b97ab1f",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 8. Remove Special Characters\n",
    "- Retains only alphanumeric characters and whitespace\n",
    "- Removes symbols like @, #, $, %, &, etc.\n",
    "- Preserves letters, numbers, and spaces"
   ],
   "id": "1ffdd326d8a48815"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T14:27:33.907495Z",
     "start_time": "2025-05-14T14:27:33.898378Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def remove_special( text: str ) -> str:\n",
    "\t\"\"\"\n",
    "\n",
    "        Remove all special characters\n",
    "        from the input string, keeping only letters, digits, and whitespace.\n",
    "\n",
    "        Args:\n",
    "            text (str): The input string to process.\n",
    "\n",
    "        Returns:\n",
    "            str: The processed string with special characters removed.\n",
    "\n",
    "    \"\"\"\n",
    "\tcleaned = [ ]\n",
    "\tfor char in text:\n",
    "\t\tif char.isalnum( ) or char.isspace( ):\n",
    "\t\t\tcleaned.append( char )\n",
    "\treturn ''.join( cleaned )"
   ],
   "id": "d915bd40a8960596",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 9. Remove HTML\n",
    "- Parses the text as HTML\n",
    "- Extracts and returns only the visible content without tags"
   ],
   "id": "3e5583a42894f537"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T14:27:37.980861Z",
     "start_time": "2025-05-14T14:27:37.972973Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def remove_html_tags( text: str ) -> str:\n",
    "\t\"\"\"\n",
    "\n",
    "        Removes HTML tags from the path pages path.\n",
    "\n",
    "        This function:\n",
    "          - Parses the pages as HTML\n",
    "          - Extracts and returns only the visible content without tags\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        pages : str\n",
    "            The path pages containing HTML tags.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        str\n",
    "            A cleaned_lines path with all HTML tags removed.\n",
    "\n",
    "    \"\"\"\n",
    "\t# Parse HTML and extract pages\n",
    "\tsoup = BeautifulSoup( text, \"raw_html.parser\" )\n",
    "\tcleaned_text = soup.get_text( separator=' ', strip=True )\n",
    "\n",
    "\treturn cleaned_text"
   ],
   "id": "d5e9b16f93ccfed8",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 10. Chunk Tokens\n",
    "- Tokenizes the text into words\n",
    "- Groups them into consecutive word chunks\n",
    "- Returns a list of strings (each chunk_tokens)"
   ],
   "id": "b9e77cd71f1df62e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T12:17:05.691405Z",
     "start_time": "2025-05-14T12:17:05.675921Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def chunk_tokens( text: list, chunk_size: int = 50 ) -> list:\n",
    "\t\"\"\"\n",
    "\n",
    "\n",
    "        Breaks a list of cleaned_lines,\n",
    "        tokenized strings into\n",
    "        chunks of a specified num of tokens.\n",
    "\n",
    "        This function:\n",
    "          - Flattens the path get_list of tokenized strings (i.e., get_list of lists)\n",
    "          - Groups tokens into chunks of min `chunk_size`\n",
    "          - Returns a get_list of token chunks, each as a get_list of tokens\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        pages : get_list of tokenizd words\n",
    "            The path get_list where each element is a get_list of tokens (words).\n",
    "\n",
    "        chunk_size : int, optional (default=50)\n",
    "            Number of tokens per chunk_tokens.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        get_list\n",
    "            A get_list of token chunks. Each chunk_tokens is a get_list of tokens.\n",
    "\n",
    "    \"\"\"\n",
    "\t# Flatten the get_list of token lists into a single get_list\n",
    "\tall_tokens = [ token for sublist in text for token in sublist ]\n",
    "\n",
    "\t# Create chunks of tokens\n",
    "\tchunks = [\n",
    "\t\tall_tokens[ i:i + chunk_size ]\n",
    "\t\tfor i in range( 0, len( all_tokens ), chunk_size )\n",
    "\t]\n",
    "\n",
    "\treturn chunks"
   ],
   "id": "a6edbb0eb8fa5720",
   "outputs": [],
   "execution_count": 56
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 11. Chunk Text\n",
    "\n",
    "- Converts text to lowercase\n",
    "- Tokenizes text using NLTK's word_tokenize\n",
    "- Breaks tokens into chunks of a specified size\n",
    "- Optionally joins tokens into strings (for transformer models)"
   ],
   "id": "e93a4c6067321b6d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T12:02:54.384229Z",
     "start_time": "2025-05-14T12:02:54.373068Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def chunk_tokens( text: str, chunk_size: int = 50, return_as_string: bool = True ) -> list:\n",
    "\t\"\"\"\n",
    "\n",
    "        Tokenizes cleaned_lines pages and breaks it into chunks for downstream vectors.\n",
    "\n",
    "        This function:\n",
    "          - Converts pages to lowercase\n",
    "          - Tokenizes pages using NLTK's word_tokenize\n",
    "          - Breaks tokens into chunks of a specified size\n",
    "          - Optionally joins tokens into strings (for transformer models)\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        pages : str\n",
    "            The cleaned_lines path pages to be tokenized and chunked.\n",
    "\n",
    "        chunk_size : int, optional (default=50)\n",
    "            Number of tokens per chunk_tokens.\n",
    "\n",
    "        return_string : bool, optional (default=True)\n",
    "            If True, returns each chunk_tokens as a path; otherwise, returns a get_list of tokens.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        get_list\n",
    "            A get_list of token chunks. Each chunk_tokens is either a get_list of tokens or a path.\n",
    "\n",
    "    \"\"\"\n",
    "\t# Download tokenizer models (only once)\n",
    "\tnltk.download( 'punkt', quiet=True )\n",
    "\n",
    "\t# Tokenize the pages into words\n",
    "\ttokens = word_tokenize( text.lower( ) )\n",
    "\n",
    "\t# Create chunks of specified token min\n",
    "\ttoken_chunks = [\n",
    "\t\ttokens[ i:i + chunk_size ]\n",
    "\t\tfor i in range( 0, len( tokens ), chunk_size )\n",
    "\t]\n",
    "\n",
    "\t# Optionally join tokens into strings\n",
    "\tif return_as_string:\n",
    "\t\treturn [ ' '.join( chunk ) for chunk in token_chunks ]\n",
    "\telse:\n",
    "\t\treturn token_chunks"
   ],
   "id": "f6b1ee045b318fd5",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 12. Remove Errors\n",
    "- Converts text to lowercase\n",
    "- Tokenizes the text into words\n",
    "- Filters out words not recognized as valid English using TextBlob\n",
    "- Returns a string with only correctly spelled words"
   ],
   "id": "b1e7c36fc4666c45"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T14:27:43.604879Z",
     "start_time": "2025-05-14T14:27:43.593349Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def remove_errors( text: str ) -> str:\n",
    "\t\"\"\"\n",
    "\n",
    "        Removes misspelled or non-English words from the path pages.\n",
    "\n",
    "        This function:\n",
    "          - Converts pages to lowercase\n",
    "          - Tokenizes the pages into words\n",
    "          - Filters out words not recognized as valid English using TextBlob\n",
    "          - Returns a path with only correctly spelled words\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        pages : str\n",
    "            The path pages to clean.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        str\n",
    "            A cleaned_lines path containing only valid English words.\n",
    "\n",
    "    \"\"\"\n",
    "\t# Convert to lowercase and tokenize_text\n",
    "\ttokens = word_tokenize( text )\n",
    "\n",
    "\t# Keep only correctly spelled words (as per Word dictionary in TextBlob)\n",
    "\tcleaned_tokens = [ word for word in tokens if Word( word ).spellcheck( )[ 0 ][ 0 ] > 0.9 ]\n",
    "\n",
    "\t# Return cleaned_lines path\n",
    "\treturn ' '.join( cleaned_tokens )"
   ],
   "id": "818357851dd8bf54",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 13. Correct Errors\n",
    "- Converts text to lowercase\n",
    "- Tokenizes the text into words\n",
    "- Applies spelling correction using TextBlob\n",
    "- Reconstructs and returns the corrected text"
   ],
   "id": "5c6cdef587829c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T14:27:46.300979Z",
     "start_time": "2025-05-14T14:27:46.291274Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def correct_errors( text: str ) -> str:\n",
    "\t\"\"\"\n",
    "\n",
    "        Corrects misspelled words in the path pages path.\n",
    "\n",
    "        This function:\n",
    "          - Converts pages to lowercase\n",
    "          - Tokenizes the pages into words\n",
    "          - Applies spelling correction using TextBlob\n",
    "          - Reconstructs and returns the corrected pages\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        pages : str\n",
    "            The path pages path with potential spelling mistakes.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        str\n",
    "            A corrected version of the path path with proper English words.\n",
    "\n",
    "    \"\"\"\n",
    "\t# Convert to lowercase and tokenize_text\n",
    "\ttokens = word_tokenize( text.lower( ) )\n",
    "\n",
    "\t# Apply spelling correction to each token\n",
    "\tcorrected_tokens = [ str( Word( word ).correct( ) ) for word in tokens ]\n",
    "\n",
    "\t# Join the corrected words into a single path\n",
    "\tcorrected_text = ' '.join( corrected_tokens )\n",
    "\n",
    "\treturn corrected_text"
   ],
   "id": "c7d8dfb08dfbf717",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 14.  Remove Headers\n",
    "- Assumes repeated lines at the top or bottom (like titles, page numbers)\n",
    "- Removes lines that are common across multiple pages (heuristic)\n",
    "- Returns cleaned text with main body content only"
   ],
   "id": "d8489c8d8d9d0af4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T14:27:49.244696Z",
     "start_time": "2025-05-14T14:27:49.229453Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def remove_headers_footers( text: str ) -> str:\n",
    "\t\"\"\"\n",
    "    Removes common headers and footers from a pages document.\n",
    "\n",
    "    This function:\n",
    "      - Assumes repeated tokens at the top or bottom (like titles, page numbers)\n",
    "      - Removes tokens that are common across multiple pages (heuristic)\n",
    "      - Returns cleaned_lines pages with main body content only\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    pages : str\n",
    "        The path pages potentially containing headers/footers.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    str\n",
    "        The cleaned_lines pages with headers and footers removed.\n",
    "    \"\"\"\n",
    "\t# Split the pages into tokens\n",
    "\tlines = text.splitlines( )\n",
    "\n",
    "\t# Remove empty tokens and trim whitespace\n",
    "\tlines = [ line.strip( ) for line in lines if line.strip( ) ]\n",
    "\n",
    "\t# Count line frequencies to identify repeated headers/footers\n",
    "\tline_counts = Counter( lines )\n",
    "\n",
    "\t# Identify frequent tokens (appear in >1% of total tokens)\n",
    "\tthreshold = max( 1, int( len( lines ) * 0.01 ) )\n",
    "\trepeated_lines = { line for line, count in line_counts.items( ) if count > threshold }\n",
    "\n",
    "\t# Remove tokens that are likely headers or footers\n",
    "\tbody_lines = [ line for line in lines if line not in repeated_lines ]\n",
    "\n",
    "\t# Reconstruct the cleaned_lines pages\n",
    "\tcleaned_text = '\\n'.join( body_lines )\n",
    "\n",
    "\treturn cleaned_text\n"
   ],
   "id": "f9c1ba26ea66c445",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 15.  Remove Formatting\n",
    "- Strips HTML tags\n",
    "- Removes Markdown syntax (e.g., *, #, [], etc.)\n",
    "- Collapses whitespace (newlines, tabs)\n",
    "- Optionally removes special characters for clean unformatted text"
   ],
   "id": "76f3936ac0e24e17"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T14:27:54.319238Z",
     "start_time": "2025-05-14T14:27:54.308726Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def remove_formatting( text: str ) -> str:\n",
    "\t\"\"\"\n",
    "\n",
    "        Removes formatting artifacts (Markdown, HTML, control characters) from pages.\n",
    "\n",
    "        This function:\n",
    "          - Strips HTML tags\n",
    "          - Removes Markdown syntax (e.g., *, #, [], etc.)\n",
    "          - Collapses whitespace (newlines, tabs)\n",
    "          - Optionally removes special characters for clean unformatted pages\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        pages : str\n",
    "            The formatted path pages.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        str\n",
    "            A cleaned_lines version of the pages with formatting removed.\n",
    "\n",
    "    \"\"\"\n",
    "\t# Remove HTML tags\n",
    "\ttext = BeautifulSoup( text, \"raw_html.parser\" ).get_text( separator=' ', strip=True )\n",
    "\n",
    "\t# Remove Markdown syntax\n",
    "\ttext = re.sub( r'\\[.*?\\]\\(.*?\\)', '', text )  # Markdown links\n",
    "\ttext = re.sub( r'[`_*#~>-]', '', text )  # Markdown chars\n",
    "\ttext = re.sub( r'!\\[.*?\\]\\(.*?\\)', '', text )  # Markdown images\n",
    "\n",
    "\t# Remove control characters and normalize whitespace\n",
    "\ttext = re.sub( r'[\\r\\n\\t]+', ' ', text )  # Newlines, tabs\n",
    "\ttext = re.sub( r'\\s+', ' ', text ).strip( )  # Collapse multiple spaces\n",
    "\n",
    "\treturn text"
   ],
   "id": "be4039da12904496",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 16. Remove Stopwords\n",
    "- Tokenizes the input text\n",
    "- Removes common stopwords (e.g., \"the\", \"is\", \"and\", etc.)\n",
    "- Returns the text with only meaningful words"
   ],
   "id": "1c75687f6c08c2c5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T14:27:56.843449Z",
     "start_time": "2025-05-14T14:27:56.830489Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def remove_stopwords( text: str ) -> str:\n",
    "\t\"\"\"\n",
    "\n",
    "        Removes English stopwords from the path pages path.\n",
    "\n",
    "        This function:\n",
    "          - Tokenizes the path pages\n",
    "          - Removes common stopwords (e.g., \"the\", \"is\", \"and\", etc.)\n",
    "          - Returns the pages with only meaningful words\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        pages : str\n",
    "            The path pages path.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        str\n",
    "            A cleaned_lines version of the path pages without stopwords.\n",
    "\n",
    "    \"\"\"\n",
    "\t# Define English stopword pairs\n",
    "\tstop_words = set( stopwords.words( 'english' ) )\n",
    "\n",
    "\t# Tokenize and lowercase\n",
    "\ttokens = word_tokenize( text.lower( ) )\n",
    "\n",
    "\t# Remove stopwords\n",
    "\tfiltered_tokens = [ word for word in tokens if word.isalnum( ) and word not in stop_words ]\n",
    "\n",
    "\t# Join tokens back into a path\n",
    "\tcleaned_text = ' '.join( filtered_tokens )\n",
    "\n",
    "\treturn cleaned_text\n"
   ],
   "id": "2e51da785742262a",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 🔍 Usage",
   "id": "f4c860851a941aeb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T14:27:59.643984Z",
     "start_time": "2025-05-14T14:27:59.634400Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def clean_text( text: str ) -> str:\n",
    "\t# Step 1: Normalize normalized\n",
    "\ttext = text.replace( '\\r\\n', '\\n' ).replace( '\\r', '\\n' )\n",
    "\ttext = re.sub( r'\\n\\s*\\d+\\s*\\n', '\\n', text )\n",
    "\ttext = re.sub( r'(\\w+)-\\n(\\w+)', r'\\1\\2', text )\n",
    "\ttext = re.sub( r'(?<!\\n)\\n(?![\\n])', '', text )\n",
    "\ttext = re.sub( r'\\s+', '', text )\n",
    "\n",
    "\treturn text"
   ],
   "id": "6ffc1b387c68d803",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 🔍 Pipeline",
   "id": "f9435a341c128b29"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T14:28:06.587921Z",
     "start_time": "2025-05-14T14:28:06.568106Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === Preprocessing Configuration ===\n",
    "EN_STOPWORDS = set( stopwords.words( 'english' ) )\n",
    "LEMMATIZER = WordNetLemmatizer( )\n",
    "STEMMER = SnowballStemmer( 'english' )\n",
    "\n",
    "\n",
    "def preprocess_line( line, lower=True, punctuation=True,\n",
    "                     stopwords=False, lemmatize=True, stem=False ):\n",
    "\t'''\n",
    "    Process a single line of documents with optional steps:\n",
    "    - lower\n",
    "    - punctuation removal\n",
    "    - stopword removal\n",
    "    - lemmatization\n",
    "    - stemming (optional)\n",
    "    Returns the cleaned_lines line as a path.\n",
    "    '''\n",
    "\ttokens = word_tokenize( line )\n",
    "\tprocessed = [ ]\n",
    "\tfor token in tokens:\n",
    "\t\tif lower:\n",
    "\t\t\ttoken = token.lower( )\n",
    "\n",
    "\t\tif punctuation and token in string.punctuation:\n",
    "\t\t\tcontinue\n",
    "\n",
    "\t\tif stopwords and token in EN_STOPWORDS:\n",
    "\t\t\tcontinue\n",
    "\n",
    "\t\tif lemmatize:\n",
    "\t\t\ttoken = LEMMATIZER.lemmatize( token )\n",
    "\n",
    "\t\tif stem:\n",
    "\t\t\ttoken = STEMMER.stem( token )\n",
    "\n",
    "\t\tprocessed.append( token )\n",
    "\n",
    "\treturn ' '.join( processed )\n",
    "\n",
    "\n",
    "def process_file( file_path, **preprocess_kwargs ):\n",
    "\t'''\n",
    "        Read a documents file line-by-line, apply preprocessing pipeline to each line,\n",
    "        and return a get_list of cleaned_lines tokens (original order preserved).\n",
    "        Pass any keyword args supported by `preprocess_line`.\n",
    "    '''\n",
    "\tcleaned_lines = [ ]\n",
    "\twith open( file_path, 'r', encoding='utf-8' ) as file:\n",
    "\t\tfor line in file:\n",
    "\t\t\tcleaned = preprocess_line( line, **preprocess_kwargs )\n",
    "\t\t\tcleaned_lines.append( cleaned )\n",
    "\treturn cleaned_lines\n"
   ],
   "id": "af7f3fd7b2990387",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "91981ee2b140d645"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "c7c6e615a319e113"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "\n",
   "id": "d901568b8ef44067"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "c6aca1d3fff8a8ac"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a0478bcf04eb5543"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#  Vectorization (Preparation for ML Models)\n",
    "___\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "ba0f01ba423430f4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Load Dependencies",
   "id": "d3e02f4dbedcc22e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T14:28:33.019077Z",
     "start_time": "2025-05-14T14:28:11.236323Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch"
   ],
   "id": "6ea810a20c6020f2",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T14:28:48.909398Z",
     "start_time": "2025-05-14T14:28:48.898906Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === Load Raw Text ===\n",
    "file_path = '<url to file>'\n",
    "_rawtext = ''\n",
    "\n",
    "\n",
    "def load_text( file_path ):\n",
    "\twith open( file_path, 'r', encoding='utf-8' ) as f:\n",
    "\t\t_rawtext = f.read( )\n",
    "\t\treturn _rawtext"
   ],
   "id": "f930499beb91e45f",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "4cd82d1886e5243"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 🧮 1. Bag of Words (BoW) using CountVectorizer",
   "id": "93554b0db878266a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T14:28:54.724229Z",
     "start_time": "2025-05-14T14:28:54.690912Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "corpus = [ 'Bro loves clean code.', 'Code is life.' ]\n",
    "vectorizer = CountVectorizer( )\n",
    "X = vectorizer.fit_transform( corpus )\n",
    "\n",
    "print( vectorizer.get_feature_names_out( ) )\n",
    "print( X.toarray( ) )\n"
   ],
   "id": "df2a01ee327d57d6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bro' 'clean' 'code' 'is' 'life' 'loves']\n",
      "[[1 1 1 0 0 1]\n",
      " [0 0 1 1 1 0]]\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 📊 2. TF-IDF using TfidfVectorizer",
   "id": "606ca62c6f083d04"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T14:28:57.191288Z",
     "start_time": "2025-05-14T14:28:57.159437Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "corpus = [ 'Bro writes awesome code.', 'Code must be clean and clear.' ]\n",
    "vectorizer = TfidfVectorizer( )\n",
    "X = vectorizer.fit_transform( corpus )\n",
    "\n",
    "print( vectorizer.get_feature_names_out( ) )\n",
    "print( X.toarray( ) )\n"
   ],
   "id": "58a4cf4ec025e261",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and' 'awesome' 'be' 'bro' 'clean' 'clear' 'code' 'must' 'writes']\n",
      "[[0.         0.53404633 0.         0.53404633 0.         0.\n",
      "  0.37997836 0.         0.53404633]\n",
      " [0.4261596  0.         0.4261596  0.         0.4261596  0.4261596\n",
      "  0.30321606 0.4261596  0.        ]]\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 🧠 3. Word2Vec using gensim",
   "id": "cd392a973450c93a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T14:29:00.261683Z",
     "start_time": "2025-05-14T14:29:00.196554Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "sentences = [ [ 'bro', 'loves', 'python' ], [ 'clean', 'code', 'rocks' ] ]\n",
    "model = Word2Vec( sentences, vector_size=100, window=5, min_count=1, workers=4 )\n",
    "\n",
    "# Vector for the word 'bro'\n",
    "vector = model.wv[ 'bro' ]\n",
    "print( vector )\n"
   ],
   "id": "a20f65018fd9c119",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-8.7274825e-03  2.1301615e-03 -8.7354420e-04 -9.3190884e-03\n",
      " -9.4281426e-03 -1.4107180e-03  4.4324086e-03  3.7040710e-03\n",
      " -6.4986930e-03 -6.8730675e-03 -4.9994122e-03 -2.2868442e-03\n",
      " -7.2502876e-03 -9.6033178e-03 -2.7436293e-03 -8.3628409e-03\n",
      " -6.0388758e-03 -5.6709289e-03 -2.3441375e-03 -1.7069972e-03\n",
      " -8.9569986e-03 -7.3519943e-04  8.1525063e-03  7.6904297e-03\n",
      " -7.2061159e-03 -3.6668312e-03  3.1185520e-03 -9.5707225e-03\n",
      "  1.4764392e-03  6.5244664e-03  5.7464195e-03 -8.7630618e-03\n",
      " -4.5171441e-03 -8.1401607e-03  4.5956374e-05  9.2636338e-03\n",
      "  5.9733056e-03  5.0673080e-03  5.0610625e-03 -3.2429171e-03\n",
      "  9.5521836e-03 -7.3564244e-03 -7.2703874e-03 -2.2653891e-03\n",
      " -7.7856064e-04 -3.2161034e-03 -5.9258583e-04  7.4888230e-03\n",
      " -6.9751858e-04 -1.6249407e-03  2.7443992e-03 -8.3591007e-03\n",
      "  7.8558037e-03  8.5361041e-03 -9.5840869e-03  2.4462664e-03\n",
      "  9.9049713e-03 -7.6658037e-03 -6.9669187e-03 -7.7365171e-03\n",
      "  8.3959233e-03 -6.8133592e-04  9.1444086e-03 -8.1582209e-03\n",
      "  3.7430846e-03  2.6350426e-03  7.4271322e-04  2.3276759e-03\n",
      " -7.4690939e-03 -9.3583735e-03  2.3545765e-03  6.1484552e-03\n",
      "  7.9856887e-03  5.7358947e-03 -7.7733636e-04  8.3061643e-03\n",
      " -9.3363142e-03  3.4061326e-03  2.6675343e-04  3.8572443e-03\n",
      "  7.3857834e-03 -6.7251669e-03  5.5844807e-03 -9.5222248e-03\n",
      " -8.0445886e-04 -8.6887367e-03 -5.0986730e-03  9.2892265e-03\n",
      " -1.8582619e-03  2.9144264e-03  9.0712793e-03  8.9381328e-03\n",
      " -8.2084350e-03 -3.0123137e-03  9.8866057e-03  5.1044310e-03\n",
      " -1.5880871e-03 -8.6920215e-03  2.9615164e-03 -6.6758976e-03]\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 🌍 4. GloVe using gensim (with pre-trained vectors)\n",
   "id": "a920cc11594a37ac"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T14:29:06.272941Z",
     "start_time": "2025-05-14T14:29:04.092027Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "# Load GloVe vec (convert .txt to .word2vec format beforehand if needed)\n",
    "glove_file = 'glove.6B.100d.word2vec.txt'\n",
    "model = KeyedVectors.load_word2vec_format( glove_file, binary=False )\n",
    "\n",
    "# Vector for the word 'code'\n",
    "vector = model[ 'code' ]\n",
    "print( vector )\n"
   ],
   "id": "95c58a17aeefed99",
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'glove.6B.100d.word2vec.txt'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mFileNotFoundError\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[25]\u001B[39m\u001B[32m, line 3\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;66;03m# Load GloVe vec (convert .txt to .word2vec format beforehand if needed)\u001B[39;00m\n\u001B[32m      2\u001B[39m glove_file = \u001B[33m'\u001B[39m\u001B[33mglove.6B.100d.word2vec.txt\u001B[39m\u001B[33m'\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m3\u001B[39m model = \u001B[43mKeyedVectors\u001B[49m\u001B[43m.\u001B[49m\u001B[43mload_word2vec_format\u001B[49m\u001B[43m(\u001B[49m\u001B[43m \u001B[49m\u001B[43mglove_file\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbinary\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m      5\u001B[39m \u001B[38;5;66;03m# Vector for the word 'code'\u001B[39;00m\n\u001B[32m      6\u001B[39m vector = model[ \u001B[33m'\u001B[39m\u001B[33mcode\u001B[39m\u001B[33m'\u001B[39m ]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\source\\compilers\\py\\anaconda\\Lib\\site-packages\\gensim\\models\\keyedvectors.py:1719\u001B[39m, in \u001B[36mKeyedVectors.load_word2vec_format\u001B[39m\u001B[34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header)\u001B[39m\n\u001B[32m   1672\u001B[39m \u001B[38;5;129m@classmethod\u001B[39m\n\u001B[32m   1673\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mload_word2vec_format\u001B[39m(\n\u001B[32m   1674\u001B[39m         \u001B[38;5;28mcls\u001B[39m, fname, fvocab=\u001B[38;5;28;01mNone\u001B[39;00m, binary=\u001B[38;5;28;01mFalse\u001B[39;00m, encoding=\u001B[33m'\u001B[39m\u001B[33mutf8\u001B[39m\u001B[33m'\u001B[39m, unicode_errors=\u001B[33m'\u001B[39m\u001B[33mstrict\u001B[39m\u001B[33m'\u001B[39m,\n\u001B[32m   1675\u001B[39m         limit=\u001B[38;5;28;01mNone\u001B[39;00m, datatype=REAL, no_header=\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[32m   1676\u001B[39m     ):\n\u001B[32m   1677\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Load KeyedVectors from a file produced by the original C word2vec-tool format.\u001B[39;00m\n\u001B[32m   1678\u001B[39m \n\u001B[32m   1679\u001B[39m \u001B[33;03m    Warnings\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m   1717\u001B[39m \n\u001B[32m   1718\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1719\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_load_word2vec_format\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1720\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mcls\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfvocab\u001B[49m\u001B[43m=\u001B[49m\u001B[43mfvocab\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbinary\u001B[49m\u001B[43m=\u001B[49m\u001B[43mbinary\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mencoding\u001B[49m\u001B[43m=\u001B[49m\u001B[43mencoding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43municode_errors\u001B[49m\u001B[43m=\u001B[49m\u001B[43municode_errors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1721\u001B[39m \u001B[43m        \u001B[49m\u001B[43mlimit\u001B[49m\u001B[43m=\u001B[49m\u001B[43mlimit\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdatatype\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdatatype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mno_header\u001B[49m\u001B[43m=\u001B[49m\u001B[43mno_header\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1722\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\source\\compilers\\py\\anaconda\\Lib\\site-packages\\gensim\\models\\keyedvectors.py:2048\u001B[39m, in \u001B[36m_load_word2vec_format\u001B[39m\u001B[34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header, binary_chunk_size)\u001B[39m\n\u001B[32m   2045\u001B[39m             counts[word] = \u001B[38;5;28mint\u001B[39m(count)\n\u001B[32m   2047\u001B[39m logger.info(\u001B[33m\"\u001B[39m\u001B[33mloading projection weights from \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[33m\"\u001B[39m, fname)\n\u001B[32m-> \u001B[39m\u001B[32m2048\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[43mutils\u001B[49m\u001B[43m.\u001B[49m\u001B[43mopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mrb\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m fin:\n\u001B[32m   2049\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m no_header:\n\u001B[32m   2050\u001B[39m         \u001B[38;5;66;03m# deduce both vocab_size & vector_size from 1st pass over file\u001B[39;00m\n\u001B[32m   2051\u001B[39m         \u001B[38;5;28;01mif\u001B[39;00m binary:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\source\\compilers\\py\\anaconda\\Lib\\site-packages\\smart_open\\smart_open_lib.py:177\u001B[39m, in \u001B[36mopen\u001B[39m\u001B[34m(uri, mode, buffering, encoding, errors, newline, closefd, opener, compression, transport_params)\u001B[39m\n\u001B[32m    174\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m transport_params \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    175\u001B[39m     transport_params = {}\n\u001B[32m--> \u001B[39m\u001B[32m177\u001B[39m fobj = \u001B[43m_shortcut_open\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    178\u001B[39m \u001B[43m    \u001B[49m\u001B[43muri\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    179\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    180\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcompression\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcompression\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    181\u001B[39m \u001B[43m    \u001B[49m\u001B[43mbuffering\u001B[49m\u001B[43m=\u001B[49m\u001B[43mbuffering\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    182\u001B[39m \u001B[43m    \u001B[49m\u001B[43mencoding\u001B[49m\u001B[43m=\u001B[49m\u001B[43mencoding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    183\u001B[39m \u001B[43m    \u001B[49m\u001B[43merrors\u001B[49m\u001B[43m=\u001B[49m\u001B[43merrors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    184\u001B[39m \u001B[43m    \u001B[49m\u001B[43mnewline\u001B[49m\u001B[43m=\u001B[49m\u001B[43mnewline\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    185\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    186\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m fobj \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    187\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m fobj\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\source\\compilers\\py\\anaconda\\Lib\\site-packages\\smart_open\\smart_open_lib.py:375\u001B[39m, in \u001B[36m_shortcut_open\u001B[39m\u001B[34m(uri, mode, compression, buffering, encoding, errors, newline)\u001B[39m\n\u001B[32m    372\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m errors \u001B[38;5;129;01mand\u001B[39;00m \u001B[33m'\u001B[39m\u001B[33mb\u001B[39m\u001B[33m'\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m mode:\n\u001B[32m    373\u001B[39m     open_kwargs[\u001B[33m'\u001B[39m\u001B[33merrors\u001B[39m\u001B[33m'\u001B[39m] = errors\n\u001B[32m--> \u001B[39m\u001B[32m375\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_builtin_open\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlocal_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbuffering\u001B[49m\u001B[43m=\u001B[49m\u001B[43mbuffering\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mopen_kwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[31mFileNotFoundError\u001B[39m: [Errno 2] No such file or directory: 'glove.6B.100d.word2vec.txt'"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 🤖 5. BERT / Transformer-based Embeddings using transformers + torch\n",
   "id": "8117a17e891dc4c0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T14:29:39.335270Z",
     "start_time": "2025-05-14T14:29:19.139141Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained( 'bert-base-uncased' )\n",
    "model = BertModel.from_pretrained( 'bert-base-uncased' )\n",
    "\n",
    "sentence = \"Bro's code always works.\"\n",
    "inputs = tokenizer( sentence, return_tensors='pt' )\n",
    "outputs = model( **inputs )\n",
    "\n",
    "# Get the vector for [CLS] token (sentence embedding)\n",
    "sentence_embedding = outputs.last_hidden_state[ :, 0, : ]\n",
    "print( sentence_embedding.shape )\n"
   ],
   "id": "8a9d362295bf0536",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "96e5110e37c94d5e9ff66a1b6e89c4b2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c00dfcb4f39d4df3ac7aa17fb8ce9cf0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b529ab93da2f4aa0baf347dc0f7fb170"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f62f9e3a11e24d84ae68cb2977a34891"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6d521379b09a4ebe842dfda0e69a1e6b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 768])\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Clean Document",
   "id": "5e61d59191d25e5c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T14:29:43.071753Z",
     "start_time": "2025-05-14T14:29:43.062471Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def clean_text( text: str ) -> str:\n",
    "\t# Step 1: Normalize\n",
    "\ttext = text.replace( '\\r\\n', '\\n' ).replace( '\\r', '\\n' )\n",
    "\n",
    "\t# Step 2: Remove page headers and footers (Public Law-specific)\n",
    "\ttext = re.sub( r'PUBLIC LAW 118–32.*?\\n', '', text, flags=re.IGNORECASE )\n",
    "\ttext = re.sub( r'\\n\\s*\\d+\\s*\\n', '\\n', text )  # Remove page numbers between tokens\n",
    "\n",
    "\t# Step 3: Remove hyphenation at line breaks (e.g., 'appropria-\\ntion')\n",
    "\ttext = re.sub( r'(\\w+)-\\n(\\w+)', r'\\1\\2', text )\n",
    "\n",
    "\t# Step 4: Merge broken tokens where sentence continues\n",
    "\ttext = re.sub( r'(?<!\\n)\\n(?![\\n])', ' ', text )\n",
    "\n",
    "\t# Step 5: Collapse excessive whitespace\n",
    "\ttext = re.sub( r'\\s+', ' ', text )\n",
    "\n",
    "\treturn text.strip( )\n"
   ],
   "id": "42c34418a1218398",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 3. Chunk File",
   "id": "9bf93b3631a66b6a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T14:29:45.926732Z",
     "start_time": "2025-05-14T14:29:45.915633Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Simple chunking by words assuming ~1.3 words per token\n",
    "def chunk_tokens( text, max_tokens=512 ):\n",
    "\twords = text.split( )\n",
    "\tchunk_size = int( max_tokens * 1.3 )\n",
    "\tchunks = [ ' '.join( words[ i:i + chunk_size ] ) for i in range( 0, len( words ), chunk_size ) ]\n",
    "\treturn chunks\n"
   ],
   "id": "bdf2dd9223fede76",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "fc9d08f36dde47ba"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "#  OpenAI Embedding\n",
    "___"
   ],
   "id": "9f097164e0f1b335"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### API key",
   "id": "10f8b0039c619d8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T14:30:00.095942Z",
     "start_time": "2025-05-14T14:30:00.000224Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create client\n",
    "client = OpenAI( )\n",
    "client.api_key = os.getenv( 'OPENAI_API_KEY' )"
   ],
   "id": "6f7c70b192899b4b",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'OpenAI' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[30]\u001B[39m\u001B[32m, line 2\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;66;03m# Create client\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m2\u001B[39m client = \u001B[43mOpenAI\u001B[49m( )\n\u001B[32m      3\u001B[39m client.api_key = os.getenv( \u001B[33m'\u001B[39m\u001B[33mOPENAI_API_KEY\u001B[39m\u001B[33m'\u001B[39m )\n",
      "\u001B[31mNameError\u001B[39m: name 'OpenAI' is not defined"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 1. Define embedding function",
   "id": "e1c76c3548b771af"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T14:30:51.638210Z",
     "start_time": "2025-05-14T14:30:51.622433Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def embed_texts( texts, model='text-embedding-3-small', batch_size=10, sleep=1 ):\n",
    "\tembeddings = [ ]\n",
    "\tfor i in range( 0, len( texts ), batch_size ):\n",
    "\t\tbatch = texts[ i:i + batch_size ]\n",
    "\t\ttry:\n",
    "\t\t\tresponse = openai.embeddings.create( input=batch, model=model )\n",
    "\t\t\tbatch_embeddings = [ e.embedding for e in response.data ]\n",
    "\t\t\tembeddings.extend( batch_embeddings )\n",
    "\t\texcept Exception as e:\n",
    "\t\t\tprint( f'Error at batch {i}: {e}' )\n",
    "\t\t\t# Retry or sleep to avoid rate limits\n",
    "\t\t\ttime.sleep( sleep )\n",
    "\t\t\tcontinue\n",
    "\n",
    "\treturn embeddings\n"
   ],
   "id": "3d19dce1b72312ca",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 2. Embed chunks",
   "id": "7a1cbc3787a81ed2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 2. Embed chunks\n",
    "embeddings = embed_texts( chunks )"
   ],
   "id": "2e8dd3b9c7453b0c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 3.  Create DataFrame",
   "id": "7b29c98e70fdc973"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 3. Create DataFrame\n",
    "df_embeddings = pd.DataFrame( { chunks, embeddings } )\n"
   ],
   "id": "998cc4a4eb1c122f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 3. Save\n",
   "id": "ca13ccc289f545b4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 3. Save\n",
    "df_embeddings.to_parquet( 'public_law_118_32_embeddings.parquet', index=False )\n"
   ],
   "id": "eac78df8b7b91593"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 4. Preview",
   "id": "ae06c79ed3506313"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 4. Preview\n",
    "df_embeddings.head( 2 )\n"
   ],
   "id": "e9935e9a89b6aac5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3. Generate Embeddings\n",
    "- Use a language model (e.g., OpenAI, HuggingFace) to create vector representations of each chunk_tokens."
   ],
   "id": "f62aab776049a508"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "\n",
    "model = SentenceTransformer( 'all-MiniLM-L6-v2' )\n",
    "embeddings = model.encode( chunks, show_progress_bar=True )\n"
   ],
   "id": "a67fd85d25aafd1c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 4. Create SQLite Database",
   "id": "d76b8831817cd4d9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- Design a table that links text chunks to their embeddings.",
   "id": "f06349ccd853f770"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "conn = sqlite3.connect( 'vectors.values' )\n",
    "cursor = conn.cursor( )\n",
    "sql_create = '''\n",
    "CREATE TABLE IF NOT EXISTS Law_Embeddings\n",
    "(\n",
    "    Id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    Chunk_Tokens TEXT NOT NULL,\n",
    "    Embedding BLOB NOT NULL\n",
    ")\n",
    "'''\n",
    "\n",
    "cursor.execute( sql_create )\n",
    "\n",
    "for chunk, vector in zip( chunks, embeddings ):\n",
    "\tblob = pickle.dumps( vector )\n",
    "\tcursor.execute( 'INSERT INTO Law_Embeddings ( Chunk_Tokens, Embedding ) VALUES (?, ?)',\n",
    "\t\t(chunk, blob) )\n",
    "\n",
    "conn.commit( )\n",
    "conn.close( )\n"
   ],
   "id": "28ca2f108b874b20"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "###  Retrieval (Vector Search in SQLite)",
   "id": "ca5ad81f911f7250"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- You can perform semantic search by encoding a query and comparing via cosine similarity\n",
   "id": "12113e4d3566bd2f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def cosine_similarity( a, b ):\n",
    "\treturn np.dot( a, b ) / (np.linalg.norm( a ) * np.linalg.norm( b ))"
   ],
   "id": "70665fe56a643b3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "query = 'Appropriations for Department of Defense'\n",
    "query_vec = model.encode( [ query ] )[ 0 ]\n",
    "\n",
    "conn = sqlite3.connect( 'vectors.values' )\n",
    "cursor = conn.cursor( )\n",
    "cursor.execute( 'SELECT Id, Chunk_Tokens, Embedding FROM Law_Embeddings' )\n",
    "\n",
    "results = [ ]\n",
    "for row in cursor.fetchall( ):\n",
    "\tchunk_id, chunk_text, blob = row\n",
    "\tstored_vec = pickle.loads( blob )\n",
    "\tsim = cosine_similarity( query_vec, stored_vec )\n",
    "\tresults.append( (sim, chunk_text) )\n",
    "\n",
    "# Sort and get top N\n",
    "top_matches = sorted( results, key=lambda x: x[ 0 ], reverse=True )[ :5 ]\n"
   ],
   "id": "675d18dac5bd124d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#   Embedding-Pipeline Script\n",
    "___"
   ],
   "id": "3a7f735556cd0d3f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Load Dependencies\n",
    "\n"
   ],
   "id": "e593513dcb937f5f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T12:08:03.310828Z",
     "start_time": "2025-05-14T12:08:03.304763Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "import sqlite3\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import openai\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n"
   ],
   "id": "cf12b8bc356c9181",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Configuration",
   "id": "99ef339bdfea1fd8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "a764c56d86acecd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Define paths\n",
    "TEXT_FILE = 'PublicLaw_118-42.txt'\n",
    "DB_FILE = 'law_embeddings.values'\n",
    "EMBEDDING_MODEL = 'all-MiniLM-L6-v2'\n",
    "CHUNK_SIZE = 1000\n",
    "CHUNK_OVERLAP = 200\n"
   ],
   "id": "e8ee553da46c407c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Load and Clean Raw Text",
   "id": "842a900a1b80cde5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def load_and_clean_text( filepath ):\n",
    "\twith open( filepath, 'r', encoding='utf-8' ) as file:\n",
    "\t\traw_text = file.read( )\n",
    "\n",
    "\t# Basic normalization\n",
    "\ttext = re.sub( r'\\f+', ' ', raw_text )\n",
    "\ttext = re.sub( r'\\n+', ' ', text )\n",
    "\ttext = re.sub( r'\\s{2,}', ' ', text )\n",
    "\treturn text.strip( )\n"
   ],
   "id": "c202cc689865f32"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Generate Embeddings",
   "id": "ae3ddba5e85404c6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def get_embedding( text, model=OPENAI_MODEL ):\n",
    "\tresponse = openai.Embedding.generate_text( input=text, model=model )\n",
    "\treturn response[ 'values'[ 0 ][ 'embedding' ] ]\n",
    "\n",
    "\n",
    "def embed_chunks( chunks ):\n",
    "\tembeddings = [ ]\n",
    "\tfor chunk in tqdm( chunks, desc='EmbeddingRequest chunks via OpenAI' ):\n",
    "\t\ttry:\n",
    "\t\t\tembedding = get_embedding( chunk )\n",
    "\t\t\tembeddings.append( embedding )\n",
    "\t\texcept Exception as e:\n",
    "\t\t\tprint( f'Error embedding chunk_tokens: {e}' )\n",
    "\t\t\tembeddings.append( [ 0.0 ] * 1536 )  # Placeholder for failed requests\n",
    "\treturn embeddings\n"
   ],
   "id": "8fa7587e7a3094be"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Create SQLite DB\n",
    "\n"
   ],
   "id": "3c985e6d3ecd5e8f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def create_and_populate_db( chunks, embeddings, db_path ):\n",
    "\tconn = sqlite3.connect( db_path )\n",
    "\tcursor = conn.cursor( )\n",
    "\tsql_create = '''\n",
    "    CREATE TABLE IF NOT EXISTS Law_Embeddings\n",
    "    (\n",
    "        Id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "        Chunk_Tokens TEXT NOT NULL,\n",
    "        Embedding BLOB NOT NULL\n",
    "    )\n",
    "    '''\n",
    "\n",
    "\tcursor.execute( sql_create )\n",
    "\tfor chunk, vector in zip( chunks, embeddings ):\n",
    "\t\tblob = pickle.dumps( vector )\n",
    "\t\tsql_insert = 'INSERT INTO Law_Embeddings ( Chunk_Tokens, Embedding ) VALUES ( ?, ? )'\n",
    "\t\tcursor.execute( sql_insert, (chunk, blob) )\n",
    "\n",
    "\tconn.commit( )\n",
    "\tconn.close( )\n"
   ],
   "id": "ea15943e543e0d70"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Script",
   "id": "e82751929d4fd563"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# === MAIN ===\n",
    "def main( ):\n",
    "\tprint( 'Step 1: Load and clean documents' )\n",
    "\tcleaned_text = load_and_clean_text( TEXT_FILE )\n",
    "\n",
    "\tprint( 'Step 2: Chunking documents' )\n",
    "\tchunks = chunk_text( cleaned_text )\n",
    "\tprint( f'Total chunks: {len( chunks )}' )\n",
    "\n",
    "\tprint( 'Step 3: EmbeddingRequest with OpenAI API' )\n",
    "\tembeddings = embed_chunks( chunks )\n",
    "\n",
    "\tprint( 'Step 4: Saving to SQLite' )\n",
    "\tcreate_and_populate_db( chunks, embeddings, DB_FILE )\n",
    "\n",
    "\tprint( f'Pipeline complete. Embeddings stored in: {DB_FILE}' )\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\tmain( )"
   ],
   "id": "5f2d269aaa88212b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Embeddings",
   "id": "de305bd9c7ccbd96"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# === 1. Load Model ===\n",
    "# You can try other models like 'all-MiniLM-L6-v2', 'all-mpnet-base-v2', or 'multi-qa-MiniLM-L6-cos-v1'\n",
    "model = SentenceTransformer( 'all-MiniLM-L6-v2' )\n",
    "\n",
    "\n",
    "# === 2. Embed Chunks ===\n",
    "def embed_with_sentence_transformers( texts, model ):\n",
    "\treturn model.encode( texts, show_progress_bar=True, convert_to_numpy=True )\n",
    "\n",
    "\n",
    "local_embeddings = embed_with_sentence_transformers( chunks, model )\n",
    "\n",
    "# === 3. Save in a DataFrame ===\n",
    "df_local = pd.DataFrame( {\n",
    "\t'chunk_tokens': chunks,\n",
    "\t'embedding': list( local_embeddings )  # numpy arrays to get_list for DataFrame compatibility\n",
    "} )\n",
    "\n",
    "# === 4. Save to Disk ===\n",
    "df_local.to_parquet( 'public_law_118_32_local_embeddings.parquet', index=False )\n",
    "\n",
    "# === 5. Preview ===\n",
    "df_local.head( 2 )\n",
    "\n"
   ],
   "id": "47df028a12155604"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Fine-Tuning",
   "id": "e1efd28f65f33d39"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import json\n",
    "import openai\n",
    "import os\n",
    "import pandas as pd\n",
    "from pprint import pprint\n"
   ],
   "id": "51f033fc98145091"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "client = openai.OpenAI(\n",
    "\tapi_key=os.environ.get( 'OPENAI_API_KEY' ),\n",
    "\torganization='<org id>',\n",
    "\tproject='<project id>',\n",
    ")"
   ],
   "id": "b11d00974dbaa480"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Read in the dataset we'll use for this task.\n",
    "# This will be the RecipesNLG dataset, which we've cleaned_lines to only contain documents from www.cookbooks.com\n",
    "recipe_df = pd.read_csv( 'data/cookbook_recipes_nlg_10k.csv' )\n",
    "recipe_df.head( )"
   ],
   "id": "aa84f0c255158f4f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "system_message = 'You are a helpful recipe assistant. You are to extract the generic ingredients from each of the recipes provided.'\n",
    "\n",
    "def create_user_message( row ):\n",
    "\treturn f'Title: {row[ 'title' ]}\\n\\nIngredients: {row[ 'ingredients' ]}\\n\\nGeneric ingredients: '\n",
    "\n",
    "\n",
    "def prepare_example_conversation( row ):\n",
    "\treturn {\n",
    "\t\t'messages': [\n",
    "\t\t\t{ 'role': 'system', 'content': system_message },\n",
    "\t\t\t{ 'role': 'user', 'content': create_user_message( row ) },\n",
    "\t\t\t{ 'role': 'assistant', 'content': row[ 'NER' ] },\n",
    "\t\t]\n",
    "\t}\n",
    "\n"
   ],
   "id": "d29e9579f6f0e9dd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# use the first 100 rows of the dataset for training\n",
    "training_df = recipe_df.loc[ 0:100 ]\n",
    "\n",
    "# apply the prepare_example_conversation function to each row of the training_df\n",
    "training_data = training_df.apply( prepare_example_conversation, axis=1 ).tolist( )\n",
    "\n",
    "for example in training_data[ :5 ]:\n",
    "\tprint( example )"
   ],
   "id": "23bdc89006166778"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "validation_df = recipe_df.loc[ 101:200 ]\n",
    "validation_data = validation_df.apply(\n",
    "\tprepare_example_conversation, axis=1 ).tolist( )"
   ],
   "id": "4773228f51c07750"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def write_jsonl( data_list: list, filename: str ) -> None:\n",
    "\twith open( filename, 'w' ) as out:\n",
    "\t\tfor ddict in data_list:\n",
    "\t\t\tjout = json.dumps( ddict ) + '\\n'\n",
    "\t\t\tout.write( jout )"
   ],
   "id": "6dcf445d39eee404"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "652e2134d8966009"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "training_file_name = 'tmp_recipe_finetune_training.jsonl'\n",
    "write_jsonl( training_data, training_file_name )\n",
    "\n",
    "validation_file_name = 'tmp_recipe_finetune_validation.jsonl'\n",
    "write_jsonl( validation_data, validation_file_name )"
   ],
   "id": "a492a14231147287"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "170bad89408062ff"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def upload_file( file_name: str, purpose: str ) -> str:\n",
    "\twith open( file_name, 'rb' ) as file_fd:\n",
    "\t\tresponse = client.files.create( file=file_fd, purpose=purpose )\n",
    "\treturn response.id"
   ],
   "id": "10f941082775954c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "751aceb620d9c7bb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "training_file_id = upload_file( training_file_name, 'fine-tune' )\n",
    "validation_file_id = upload_file( validation_file_name, 'fine-tune' )"
   ],
   "id": "c8dbd2395ef59d03"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "MODEL = 'openai-4o-mini-2024-07-18'\n",
    "\n",
    "response = client.fine_tuning.jobs.create(\n",
    "\ttraining_file=training_file_id,\n",
    "\tvalidation_file=validation_file_id,\n",
    "\tmodel=MODEL,\n",
    "\tsuffix='recipe-ner',\n",
    ")\n",
    "\n",
    "job_id = response.id"
   ],
   "id": "5ca162663d00cad3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Text Cleaning",
   "id": "dcabaa53b3a578c0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T12:01:30.524756Z",
     "start_time": "2025-05-14T12:01:30.463783Z"
    }
   },
   "cell_type": "code",
   "source": "import tigrr as tgr",
   "id": "1dd53ce7908a5042",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Preprocessing",
   "id": "f8d915c3f2d4b28f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T14:31:46.613979Z",
     "start_time": "2025-05-14T14:31:46.595732Z"
    }
   },
   "cell_type": "code",
   "source": [
    "source = r'C:\\Users\\terry\\Desktop\\Text'\n",
    "destination = r'C:\\Users\\terry\\Desktop\\Text\\cleaned'"
   ],
   "id": "a59ab34bb40814",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T12:23:00.801461Z",
     "start_time": "2025-05-14T12:23:00.787130Z"
    }
   },
   "cell_type": "code",
   "source": [
    "docs = [ ]\n",
    "files = os.listdir( source )\n",
    "for i in files:\n",
    "\tdocs = [ source + f'\\\\{i}' for i in files ]"
   ],
   "id": "1195a17b56c1132a",
   "outputs": [],
   "execution_count": 70
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T14:31:30.342263Z",
     "start_time": "2025-05-14T14:31:30.332790Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for d in docs:\n",
    "\tprint( d )"
   ],
   "id": "af2bd8b593c87d8b",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T14:50:28.461874Z",
     "start_time": "2025-05-14T14:50:28.054346Z"
    }
   },
   "cell_type": "code",
   "source": [
    "file = r'C:\\Users\\terry\\Desktop\\Text\\Senate Report 109-293.txt'\n",
    "cleaned_lines = [ ]\n",
    "filename = os.path.basename( file )\n",
    "dirty = open( file, 'rt' ).readlines( )\n",
    "for d in dirty:\n",
    "\tlower = d.lower( )\n",
    "\tcleaned = trim_whitespace( lower )\n",
    "\tspec = remove_special( cleaned )\n",
    "\tslim = collapse_space( spec )\n",
    "\tcleaned_lines.append( slim )"
   ],
   "id": "bc7745c00f12cd65",
   "outputs": [],
   "execution_count": 72
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T14:50:53.522141Z",
     "start_time": "2025-05-14T14:50:51.691583Z"
    }
   },
   "cell_type": "code",
   "source": [
    "new = r'C:\\Users\\terry\\Desktop\\Text\\cleaned' + '\\\\' + filename\n",
    "folder = open( new, 'wt+' )\n",
    "for c in cleaned_lines:\n",
    "\tfolder.writelines( c )\n",
    "\n",
    "folder.close( )"
   ],
   "id": "feb3b69467ab7c9c",
   "outputs": [],
   "execution_count": 74
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T16:11:58.401640Z",
     "start_time": "2025-05-14T16:11:58.393317Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def process_documents(  ):\n",
    "\tsource = r'C:\\Users\\terry\\Desktop\\Budget\\Guidance\\Regulations\\Text\\\\'\n",
    "\tdestination = r'C:\\Users\\terry\\Desktop\\Budget\\Guidance\\Regulations\\Cleaned\\\\'\n",
    "\tdocs = [ ]\n",
    "\tfiles = os.listdir( source )\n",
    "\tfor f in files:\n",
    "\t\tprocessed = [ ]\n",
    "\t\tfilename = os.path.basename( f )\n",
    "\t\tdirty = open( source + filename, 'r', encoding='utf-8', errors='ignore' ).readlines( )\n",
    "\t\tfor d in dirty:\n",
    "\t\t\tlower = d.lower( )\n",
    "\t\t\tcleaned = trim_whitespace( lower )\n",
    "\t\t\tspec = remove_special( cleaned )\n",
    "\t\t\tslim = collapse_space( spec )\n",
    "\t\t\tprocessed.append( slim )\n",
    "\n",
    "\t\tclean = open( destination + filename, 'wt', encoding='utf-8', errors='ignore' )\n",
    "\t\tfor p in processed:\n",
    "\t\t\tclean.write( p )"
   ],
   "id": "d956391d0d022ad5",
   "outputs": [],
   "execution_count": 175
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T16:12:15.968048Z",
     "start_time": "2025-05-14T16:12:01.276966Z"
    }
   },
   "cell_type": "code",
   "source": "process_documents( )",
   "id": "9cb07dd3284ab3e2",
   "outputs": [],
   "execution_count": 176
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
