{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# NLTK\n",
    "___\n"
   ],
   "id": "700c6019c3802579"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### üì¶ One-Time Setup (NLTK Resources)",
   "id": "17b64a5a35525229"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "import nltk\n",
    "nltk.download( 'punkt' )\n",
    "nltk.download( 'stopwords' )\n",
    "nltk.download( 'wordnet' )\n",
    "nltk.download( 'omw-1.4' )\n"
   ],
   "id": "ea993d341972b4d9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Load Dependencies",
   "id": "3957678e92123d8f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer"
   ],
   "id": "9a5c2c52dd74d494"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### üß† Full Pipeline",
   "id": "dda1854408000616"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# === Preprocessing Configuration ===\n",
    "EN_STOPWORDS = set( stopwords.words( 'english' ) )\n",
    "LEMMATIZER = WordNetLemmatizer( )\n",
    "STEMMER = SnowballStemmer( 'english' )\n",
    "\n",
    "def preprocess_line( line, lower=True, punctuation=True,\n",
    "                     stopwords=False, lemmatize=True, stem=False ):\n",
    "    '''\n",
    "    Process a single line of documents with optional steps:\n",
    "    - lower\n",
    "    - punctuation removal\n",
    "    - stopword removal\n",
    "    - lemmatization\n",
    "    - stemming (optional)\n",
    "    Returns the cleaned line as a string.\n",
    "    '''\n",
    "    tokens = word_tokenize( line )\n",
    "    processed = [ ]\n",
    "    for token in tokens:\n",
    "        if lower:\n",
    "            token = token.lower( )\n",
    "\n",
    "        if punctuation and token in string.punctuation:\n",
    "            continue\n",
    "\n",
    "        if stopwords and token in EN_STOPWORDS:\n",
    "            continue\n",
    "\n",
    "        if lemmatize:\n",
    "            token = LEMMATIZER.lemmatize( token )\n",
    "\n",
    "        if stem:\n",
    "            token = STEMMER.stem( token )\n",
    "\n",
    "        processed.append( token )\n",
    "\n",
    "    return ' '.join( processed )\n",
    "\n",
    "\n",
    "def process_file( file_path, **preprocess_kwargs ):\n",
    "    '''\n",
    "        Read a documents file line-by-line, apply preprocessing pipeline to each line,\n",
    "        and return a list of cleaned lines (original order preserved).\n",
    "        Pass any keyword args supported by `preprocess_line`.\n",
    "    '''\n",
    "    cleaned_lines = []\n",
    "    with open( file_path, 'r', encoding='utf-8' ) as file:\n",
    "        for line in file:\n",
    "            cleaned = preprocess_line( line, **preprocess_kwargs )\n",
    "            cleaned_lines.append( cleaned )\n",
    "    return cleaned_lines\n"
   ],
   "id": "af7f3fd7b2990387"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### üîç Usage",
   "id": "91981ee2b140d645"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "file_path = 'path/to/Public_Law_118-32.txt'\n",
    "cleaned_lines = process_file( file_path, lowercase=True, remove_punct=True,\n",
    "    remove_stopwords=False, lemmatize=True, stem=True )\n",
    "\n",
    "print( f'Total lines: {len( cleaned_lines )}' )\n",
    "print( 'Example cleaned line:', cleaned_lines[ 0 ] )"
   ],
   "id": "c7c6e615a319e113"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Text Processing\n",
   "id": "2463270042d983cf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Load Dependencies",
   "id": "6e0771597592ec33"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-31T19:50:18.568643Z",
     "start_time": "2025-03-31T19:50:18.559980Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "import os\n",
    "from pathlib import Path\n",
    "import openai\n",
    "import pandas as pd\n",
    "import time\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n"
   ],
   "id": "63d1f338a9f2a8ca",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## ‚úÖ Summary Checklist for Text Cleaning\n",
    "\n",
    "| Step | Task                     | Function/Library               |\n",
    "|------|--------------------------|--------------------------------|\n",
    "| 1    | Load Text                | `open()`, `pandas.read_csv()` |\n",
    "| 2    | Convert to Lowercase     | `.lower()`                     |\n",
    "| 3    | Remove Punctuation       | `string.punctuation`           |\n",
    "| 4    | Remove Numbers           | `re.sub()`                     |\n",
    "| 5    | Trim Whitespaces         | `' '.join()`                   |\n",
    "| 6    | Tokenization             | `nltk.word_tokenize()`         |\n",
    "| 7    | Remove Stopwords         | `nltk.corpus.stopwords`        |\n",
    "| 8    | Lemmatization/Stemming   | `WordNetLemmatizer`, `PorterStemmer` |\n",
    "| 9    | Reconstruct Clean Text   | `' '.join()`                   |\n",
    "| 10   | (Optional) Spellcheck    | `TextBlob.correct()`           |\n",
    "| 11   | Vectorization for ML     | `TfidfVectorizer`, `CountVectorizer`, Word Embeddings |\n"
   ],
   "id": "d901568b8ef44067"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1. Load File",
   "id": "c6aca1d3fff8a8ac"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# === Load Raw Text ===\n",
    "file_path = '<path to file>'\n",
    "_rawtext = ''\n",
    "\n",
    "\n",
    "def load_text( file_path ):\n",
    "\twith open( file_path, 'r', encoding='utf-8' ) as f:\n",
    "\t\t_rawtext = f.read( )\n",
    "\t\treturn _rawtext"
   ],
   "id": "a0478bcf04eb5543"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2. Convert to Lowercase\n",
    "- Ensures uniformity in comparison and tokenization."
   ],
   "id": "460ac7080c8e2c8f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "_lower = _rawtext.lower( )",
   "id": "2270243884d2c7df"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3. Remove Punctuation\n",
    "- Strips out punctuation which often has no semantic meaning."
   ],
   "id": "6d7cc99b8285edd9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import string\n",
    "_unpunc = _lower.translate( str.maketrans('', '', string.punctuation ) )"
   ],
   "id": "d55d91d0009317c9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 4. Remove Numbers (if necessary)\n",
    "- Optional, depending on the task (e.g., sentiment analysis may not need them)."
   ],
   "id": "824b50e81202342c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import re\n",
    "\n",
    "\n",
    "_undigit = re.sub( r'\\d+', '', _unpunc )"
   ],
   "id": "a301e5ecb8004ee1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 5. Remove Extra Whitespaces\n",
    "- Reduces redundancy and inconsistencies."
   ],
   "id": "ff2cda9ae5fc544"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "_nospace = ' '.join( _undigit.split( ) )",
   "id": "5100c3f3c80d9e3a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 6. Tokenization\n",
    "- Splits text into words or tokens."
   ],
   "id": "ce74ff42a936c3f2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Tokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "_tokens = word_tokenize( _unspace )"
   ],
   "id": "ec7dbda255898f44"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 7. Remove Stopwords\n",
    "- Eliminates common words that may not contribute much meaning."
   ],
   "id": "53368261515b891e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Stopwords\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "stop_words = set( stopwords.words( 'english' ) )\n",
    "_unstop = [ word for word in _tokens if word not in stop_words ]"
   ],
   "id": "75da0d5e1740caef"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 8. Lemmatization (or Stemming)\n",
    "- Reduces words to their base or root form"
   ],
   "id": "3dd78e37ae8f7475"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# WordNet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "_lemma = [ lemmatizer.lemmatize( word ) for word in _tokens ]\n",
    "\n",
    "# Porter\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "_stemm = [ stemmer.stem( word ) for word in _tokens ]"
   ],
   "id": "72adcfef7ce5df9d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 9. Reconstruct Clean Text (if needed)\n",
    "- Recombine tokens into a single string if your model expects it."
   ],
   "id": "88f1dc39bf3f1a7a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "_joined = ' '.join( _tokens )",
   "id": "779b09c8c315fedf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 10. Optional: Spelling Correction\n",
    "- Useful if working with user-generated content."
   ],
   "id": "5ca6acd86b8890d9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from textblob import TextBlob\n",
    "_spelling = str( TextBlob( _joined ).correct( ) )"
   ],
   "id": "85b66bef67935937"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#  Vectorization (Preparation for ML Models)\n",
    "___\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "ba0f01ba423430f4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### üßÆ 1. Bag of Words (BoW) using CountVectorizer",
   "id": "22d2be7d1a9bd6e0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = ['Bro loves clean code.', 'Code is life.']\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "print( vectorizer.get_feature_names_out( ) )\n",
    "print( X.toarray() )"
   ],
   "id": "df2a01ee327d57d6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### üìä 2. TF-IDF using TfidfVectorizer",
   "id": "606ca62c6f083d04"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = [ 'Bro writes awesome code.', 'Code must be clean and clear.' ]\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print(X.toarray())"
   ],
   "id": "58a4cf4ec025e261"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### üß† 3. Word2Vec using gensim",
   "id": "cd392a973450c93a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "sentences = [ [ 'bro', 'loves', 'python'], ['clean', 'code', 'rocks' ] ]\n",
    "model = Word2Vec( sentences, vector_size=100, window=5, min_count=1, workers=4 )\n",
    "\n",
    "# Vector for the word 'bro'\n",
    "vector = model.wv[ 'bro' ]\n",
    "print(vector)"
   ],
   "id": "a20f65018fd9c119"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### üåç 4. GloVe using gensim (with pre-trained vectors)\n",
   "id": "a920cc11594a37ac"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Load GloVe vectors (convert .txt to .word2vec format beforehand if needed)\n",
    "glove_file = 'glove.6B.100d.word2vec.txt'\n",
    "model = KeyedVectors.load_word2vec_format(glove_file, binary=False)\n",
    "\n",
    "# Vector for the word 'code'\n",
    "vector = model['code']\n",
    "print(vector)\n"
   ],
   "id": "95c58a17aeefed99"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### ü§ñ 5. BERT / Transformer-based Embeddings using transformers + torch\n",
   "id": "8117a17e891dc4c0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "sentence = \"Bro's code always works.\"\n",
    "inputs = tokenizer(sentence, return_tensors='pt')\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# Get the vector for [CLS] token (sentence embedding)\n",
    "sentence_embedding = outputs.last_hidden_state[:, 0, :]\n",
    "print(sentence_embedding.shape)\n"
   ],
   "id": "8a9d362295bf0536"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Clean Document",
   "id": "5e61d59191d25e5c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def clean_text( text: str ) -> str:\n",
    "    # Step 1: Normalize normalized\n",
    "    text = text.replace('\\r\\n', '\\n').replace('\\r', '\\n')\n",
    "\n",
    "    # Step 2: Remove page headers and footers (Public Law-specific)\n",
    "    text = re.sub(r'PUBLIC LAW 118‚Äì32.*?\\n', '', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\n\\s*\\d+\\s*\\n', '\\n', text)  # Remove page numbers between lines\n",
    "\n",
    "    # Step 3: Remove hyphenation at line breaks (e.g., 'appropria-\\ntion')\n",
    "    text = re.sub(r'(\\w+)-\\n(\\w+)', r'\\1\\2', text)\n",
    "\n",
    "    # Step 4: Merge broken lines where sentence continues\n",
    "    text = re.sub(r'(?<!\\n)\\n(?![\\n])', ' ', text)\n",
    "\n",
    "    # Step 5: Collapse excessive whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "    return text.strip()\n"
   ],
   "id": "42c34418a1218398"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 3. Chunk File",
   "id": "9bf93b3631a66b6a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Simple chunking by words assuming ~1.3 words per token\n",
    "def chunk_text( text, max_tokens=512 ):\n",
    "\twords = text.split( )\n",
    "\tchunk_size = int( max_tokens * 1.3 )\n",
    "\tchunks = [ ' '.join( words[ i:i + chunk_size ] ) for i in range( 0, len( words ), chunk_size ) ]\n",
    "\treturn chunks\n"
   ],
   "id": "bdf2dd9223fede76"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### üîç Example",
   "id": "842abb4747e1ed6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "file_path = 'path_to/Public_Law_118-32.txt'\n",
    "raw_text = load_text( file_path )\n",
    "cleaned_text = clean_text( raw_text )\n",
    "chunks = chunk_text( cleaned_text )\n",
    "print( f'Total Chunks: {len( chunks )}' )\n",
    "print( 'Sample chunk:\\n', chunks[ 0 ][ :1000 ] )"
   ],
   "id": "1620253dd31900a3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "fc9d08f36dde47ba"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "#  OpenAI Embedding\n",
    "___"
   ],
   "id": "9f097164e0f1b335"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### API key",
   "id": "10f8b0039c619d8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create client\n",
    "client = OpenAI( )\n",
    "client.api_key = os.getenv( 'OPENAI_API_KEY' )"
   ],
   "id": "6f7c70b192899b4b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 1. Define embedding function",
   "id": "e1c76c3548b771af"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def embed_texts( texts, model='documents-embedding-3-small', batch_size=10, sleep=1 ):\n",
    "    embeddings = []\n",
    "    for i in range( 0, len( texts ), batch_size ):\n",
    "        batch = texts[ i:i+batch_size ]\n",
    "        try:\n",
    "            response = openai.embeddings.create( input=batch, model=model )\n",
    "            batch_embeddings = [ e.embedding for e in response.data ]\n",
    "            embeddings.extend( batch_embeddings )\n",
    "        except Exception as e:\n",
    "            print( f'Error at batch {i}: {e}' )\n",
    "            # Retry or sleep to avoid rate limits\n",
    "            time.sleep( sleep )\n",
    "            continue\n",
    "\n",
    "    return embeddings\n"
   ],
   "id": "3d19dce1b72312ca"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 2. Embed chunks",
   "id": "7a1cbc3787a81ed2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 2. Embed chunks\n",
    "embeddings = embed_texts( chunks )"
   ],
   "id": "2e8dd3b9c7453b0c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 3.  Create DataFrame",
   "id": "7b29c98e70fdc973"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 3. Create DataFrame\n",
    "df_embeddings = pd.DataFrame( { chunks, embeddings } )\n"
   ],
   "id": "998cc4a4eb1c122f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 3. Save\n",
   "id": "ca13ccc289f545b4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 3. Save\n",
    "df_embeddings.to_parquet( 'public_law_118_32_embeddings.parquet', index=False )\n"
   ],
   "id": "eac78df8b7b91593"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 4. Preview",
   "id": "ae06c79ed3506313"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 4. Preview\n",
    "df_embeddings.head(2)\n"
   ],
   "id": "e9935e9a89b6aac5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Text Cleaning\n",
    "___"
   ],
   "id": "1f2312a84c08528a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Load Dependencies",
   "id": "b6af68746614991a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import re\n",
    "import sentence_transformers\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "import pickle"
   ],
   "id": "27a64e0eee5267f6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Normalization",
   "id": "cabad6571d4af082"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import unicodedata\n",
    "import re\n",
    "\n",
    "\n",
    "def normalize_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "\n",
    "        Normalizes the input text string.\n",
    "\n",
    "        This function:\n",
    "          - Converts text to lowercase\n",
    "          - Removes accented characters (e.g., √© -> e)\n",
    "          - Removes leading/trailing spaces\n",
    "          - Collapses multiple whitespace characters into a single space\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        text : str\n",
    "            The raw input text string to be normalized.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        str\n",
    "            A normalized, cleaned version of the input string.\n",
    "\n",
    "    \"\"\"\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove accented characters using Unicode normalization\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8')\n",
    "\n",
    "    # Trim leading/trailing spaces and collapse internal whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text\n"
   ],
   "id": "3cae9b193c6a9d01"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "###  1.  Clean Whitespace\n",
    "- Consecutive whitespace reduced to a single space\n",
    "- Leading/trailing spaces removed\n",
    "- Blank lines removed"
   ],
   "id": "f57da739bd434166"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import re\n",
    "\n",
    "def clean_whitespace( text: str ) -> str:\n",
    "    \"\"\"\n",
    "        Removes extra spaces and blank lines from the input text.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        text : str\n",
    "            The raw input text string to be cleaned.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        str\n",
    "            A cleaned text string with:\n",
    "                - Consecutive whitespace reduced to a single space\n",
    "                - Leading/trailing spaces removed\n",
    "                - Blank lines removed\n",
    "    \"\"\"\n",
    "    # Replace multiple spaces or tabs with a single space\n",
    "    text = re.sub(r'[ \\t]+', ' ', text)\n",
    "\n",
    "    # Remove leading/trailing spaces from each line\n",
    "    lines = [line.strip() for line in text.splitlines()]\n",
    "\n",
    "    # Remove empty lines\n",
    "    cleaned_lines = [line for line in lines if line]\n",
    "\n",
    "    # Join lines back into a single string\n",
    "    cleaned_text = '\\n'.join(cleaned_lines)\n",
    "\n",
    "    return cleaned_text"
   ],
   "id": "c0fb80a1fb7b3285"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2. Remove Punctuation",
   "id": "cd5a9f9c062a0b4e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import string\n",
    "\n",
    "def remove_punctuation(text: str) -> str:\n",
    "    \"\"\"\n",
    "\n",
    "        Removes all punctuation characters from the input text string.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        text : str\n",
    "            The input text string to be cleaned.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        str\n",
    "            The text string with all punctuation removed.\n",
    "\n",
    "    \"\"\"\n",
    "    # Create a translation table that maps punctuation to None\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "    # Apply the translation to the text\n",
    "    cleaned_text = text.translate(translator)\n",
    "\n",
    "    return cleaned_text\n"
   ],
   "id": "de9dfae36da44dc8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3. Trim Whitespace\n",
    "\n",
    "- Removes leading and trailing whitespace\n",
    "- Replaces multiple internal spaces with a single space"
   ],
   "id": "83590d02f6e0f3b3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import re\n",
    "\n",
    "def trim_whitespace(text: str) -> str:\n",
    "    \"\"\"\n",
    "\n",
    "        Trims whitespace from the input text string.\n",
    "\n",
    "        This function:\n",
    "          - Removes leading and trailing whitespace\n",
    "          - Replaces multiple internal spaces with a single space\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        text : str\n",
    "            The raw input string with potential extra whitespace.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        str\n",
    "            The cleaned string with trimmed and normalized whitespace.\n",
    "\n",
    "    \"\"\"\n",
    "    # Strip leading and trailing whitespace\n",
    "    text = text.strip()\n",
    "\n",
    "    # Replace multiple whitespace characters (spaces, tabs, etc.) with a single space\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "    return cleaned_text\n",
    "\n"
   ],
   "id": "1cc6251ecd5750cd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 4. Stemming\n",
    "- Reduces words to their base or root form.\n",
    "- Converts text to lowercase\n",
    "- Tokenizes the text into words\n",
    "- Lemmatizes each token using WordNetLemmatizer\n",
    "- Reconstructs the lemmatized tokens into a single string"
   ],
   "id": "7f09df5aca70423d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download required resources (only needed once)\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('omw-1.4', quiet=True)\n",
    "\n",
    "def lemmatize(text: str) -> str:\n",
    "    \"\"\"\n",
    "\n",
    "        Performs lemmatization on the input text string.\n",
    "\n",
    "        This function:\n",
    "          - Converts text to lowercase\n",
    "          - Tokenizes the text into words\n",
    "          - Lemmatizes each token using WordNetLemmatizer\n",
    "          - Reconstructs the lemmatized tokens into a single string\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        text : str\n",
    "            The input text string to be lemmatized.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        str\n",
    "            A string with all words lemmatized.\n",
    "\n",
    "    \"\"\"\n",
    "    # Initialize lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    lower_case = text.lower( )\n",
    "    # Convert to lowercase and tokenize\n",
    "    tokens = word_tokenize( lower_case )\n",
    "\n",
    "    # Lemmatize each token\n",
    "    lemmatized_tokens = [ lemmatizer.lemmatize( token ) for token in tokens ]\n",
    "\n",
    "    # Join tokens back to a string\n",
    "    lemmatized_text = ' '.join( lemmatized_tokens )\n",
    "\n",
    "    return lemmatized_text\n"
   ],
   "id": "69263da16288972e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 5. Tokenization\n",
    "\n",
    "- Converts text to lowercase\n",
    "- Uses NLTK's word_tokenize to split the text into words and punctuation tokens"
   ],
   "id": "af577d4ed4d069cd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "def tokenize( text: str ) -> list:\n",
    "    \"\"\"\n",
    "\n",
    "        Tokenizes the input text string into individual word tokens.\n",
    "\n",
    "        This function:\n",
    "          - Converts text to lowercase\n",
    "          - Uses NLTK's word_tokenize to split the text into words and punctuation tokens\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        text : str\n",
    "            The raw input text string to be tokenized.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        list\n",
    "            A list of tokens (words and punctuation) extracted from the text.\n",
    "\n",
    "    \"\"\"\n",
    "    nltk.download( 'punkt', quiet=True )\n",
    "    # Convert to lowercase\n",
    "    text = text.lower( )\n",
    "\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize( text )\n",
    "\n",
    "    return tokens\n"
   ],
   "id": "4adbe3ad023582b9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 6. Remove Special Characters\n",
    "- Retains only alphanumeric characters and whitespace\n",
    "- Removes symbols like @, #, $, %, &, etc.\n",
    "- Preserves letters, numbers, and spaces"
   ],
   "id": "4e87bc455ba93355"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import re\n",
    "\n",
    "def remove_special( text: str ) -> str:\n",
    "    \"\"\"\n",
    "\n",
    "        Removes special characters from the input text string.\n",
    "\n",
    "        This function:\n",
    "          - Retains only alphanumeric characters and whitespace\n",
    "          - Removes symbols like @, #, $, %, &, etc.\n",
    "          - Preserves letters, numbers, and spaces\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        text : str\n",
    "            The raw input text string potentially containing special characters.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        str\n",
    "            A cleaned string containing only letters, numbers, and spaces.\n",
    "\n",
    "    \"\"\"\n",
    "    # Use regex to replace all non-alphanumeric characters (excluding spaces) with empty string\n",
    "    cleaned_text = re.sub(r'[^A-Za-z0-9\\s]', '', text)\n",
    "\n",
    "    return cleaned_text\n"
   ],
   "id": "936a3035cb982382"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 7. Remove HTML\n",
    "- Parses the text as HTML\n",
    "- Extracts and returns only the visible content without tags"
   ],
   "id": "1b48b927bcfd1268"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def remove_html_tags(text: str) -> str:\n",
    "    \"\"\"\n",
    "\n",
    "        Removes HTML tags from the input text string.\n",
    "\n",
    "        This function:\n",
    "          - Parses the text as HTML\n",
    "          - Extracts and returns only the visible content without tags\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        text : str\n",
    "            The input text containing HTML tags.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        str\n",
    "            A cleaned string with all HTML tags removed.\n",
    "\n",
    "    \"\"\"\n",
    "    # Parse HTML and extract text\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    cleaned_text = soup.get_text(separator=' ', strip=True)\n",
    "\n",
    "    return cleaned_text\n"
   ],
   "id": "b73b90227a64d3ee"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 8. Chunk the Tokens\n",
    "- Tokenizes the text into words\n",
    "- Groups them into consecutive word chunks\n",
    "- Returns a list of strings (each chunk)"
   ],
   "id": "19cbc1c90cb94e79"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import re\n",
    "\n",
    "def chunk_tokens( text: list, chunk_size: int = 50) -> list:\n",
    "    \"\"\"\n",
    "\n",
    "        Breaks a list of cleaned, tokenized strings into chunks of a specified number of tokens.\n",
    "\n",
    "        This function:\n",
    "          - Flattens the input list of tokenized strings (i.e., list of lists)\n",
    "          - Groups tokens into chunks of length `chunk_size`\n",
    "          - Returns a list of token chunks, each as a list of tokens\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        text : list of tokenizd words\n",
    "            The input list where each element is a list of tokens (words).\n",
    "\n",
    "        chunk_size : int, optional (default=50)\n",
    "            Number of tokens per chunk.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        list\n",
    "            A list of token chunks. Each chunk is a list of tokens.\n",
    "\n",
    "    \"\"\"\n",
    "    # Flatten the list of token lists into a single list\n",
    "    all_tokens = [ token for sublist in text for token in sublist ]\n",
    "\n",
    "    # Create chunks of tokens\n",
    "    chunks = [\n",
    "        all_tokens[i:i + chunk_size]\n",
    "        for i in range(0, len(all_tokens), chunk_size)\n",
    "    ]\n",
    "\n",
    "    return chunks\n"
   ],
   "id": "40beb2e1042742e5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 8. Chunk the Cleaned Text\n",
    "\n",
    "- Converts text to lowercase\n",
    "- Tokenizes text using NLTK's word_tokenize\n",
    "- Breaks tokens into chunks of a specified size\n",
    "- Optionally joins tokens into strings (for transformer models)"
   ],
   "id": "58c6c891c9e8bfe4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "\n",
    "def chunk_text( text: str, chunk_size: int = 50, return_as_string: bool = True ) -> list:\n",
    "    \"\"\"\n",
    "\n",
    "        Tokenizes cleaned text and breaks it into chunks for downstream embeddings.\n",
    "\n",
    "        This function:\n",
    "          - Converts text to lowercase\n",
    "          - Tokenizes text using NLTK's word_tokenize\n",
    "          - Breaks tokens into chunks of a specified size\n",
    "          - Optionally joins tokens into strings (for transformer models)\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        text : str\n",
    "            The cleaned input text to be tokenized and chunked.\n",
    "\n",
    "        chunk_size : int, optional (default=50)\n",
    "            Number of tokens per chunk.\n",
    "\n",
    "        return_as_string : bool, optional (default=True)\n",
    "            If True, returns each chunk as a string; otherwise, returns a list of tokens.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        list\n",
    "            A list of token chunks. Each chunk is either a list of tokens or a string.\n",
    "\n",
    "    \"\"\"\n",
    "    # Download tokenizer models (only once)\n",
    "    nltk.download('punkt', quiet=True)\n",
    "\n",
    "    # Tokenize the text into words\n",
    "    tokens = word_tokenize( text.lower( ) )\n",
    "\n",
    "    # Create chunks of specified token length\n",
    "    token_chunks = [\n",
    "        tokens[i:i + chunk_size]\n",
    "        for i in range(0, len(tokens), chunk_size)\n",
    "    ]\n",
    "\n",
    "    # Optionally join tokens into strings\n",
    "    if return_as_string:\n",
    "        return [' '.join(chunk) for chunk in token_chunks]\n",
    "    else:\n",
    "        return token_chunks\n"
   ],
   "id": "592f630717d48d99"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "110d90c073758dfb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3. Generate Embeddings\n",
    "- Use a language model (e.g., OpenAI, HuggingFace) to create vector representations of each chunk."
   ],
   "id": "f62aab776049a508"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "\n",
    "model = SentenceTransformer( 'all-MiniLM-L6-v2' )\n",
    "embeddings = model.encode( chunks, show_progress_bar=True )\n"
   ],
   "id": "a67fd85d25aafd1c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 4. Create SQLite Database",
   "id": "d76b8831817cd4d9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- Design a table that links text chunks to their embeddings.",
   "id": "f06349ccd853f770"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "conn = sqlite3.connect( 'embeddings.data' )\n",
    "cursor = conn.cursor( )\n",
    "sql_create = '''\n",
    "CREATE TABLE IF NOT EXISTS law_embeddings\n",
    "(\n",
    "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    chunk TEXT NOT NULL,\n",
    "    embedding BLOB NOT NULL\n",
    ")\n",
    "'''\n",
    "\n",
    "cursor.execute( sql_create )\n",
    "\n",
    "for chunk, vector in zip( chunks, embeddings ):\n",
    "    blob = pickle.dumps( vector )\n",
    "    cursor.execute( 'INSERT INTO law_embeddings ( chunk, embedding ) VALUES (?, ?)', ( chunk, blob ) )\n",
    "\n",
    "conn.commit( )\n",
    "conn.close( )\n"
   ],
   "id": "28ca2f108b874b20"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "###  Retrieval (Vector Search in SQLite)",
   "id": "ca5ad81f911f7250"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- You can perform semantic search by encoding a query and comparing via cosine similarity\n",
   "id": "12113e4d3566bd2f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def cosine_similarity( a, b ):\n",
    "    return np.dot( a, b ) / ( np.linalg.norm( a ) * np.linalg.norm( b ) )"
   ],
   "id": "70665fe56a643b3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "query = 'Appropriations for Department of Defense'\n",
    "query_vec = model.encode( [ query ] )[ 0 ]\n",
    "\n",
    "conn = sqlite3.connect( 'embeddings.data' )\n",
    "cursor = conn.cursor( )\n",
    "cursor.execute( 'SELECT id, chunk, embedding FROM law_embeddings' )\n",
    "\n",
    "results = []\n",
    "for row in cursor.fetchall( ):\n",
    "    chunk_id, chunk_text, blob = row\n",
    "    stored_vec = pickle.loads( blob )\n",
    "    sim = cosine_similarity( query_vec, stored_vec )\n",
    "    results.append( ( sim, chunk_text ) )\n",
    "\n",
    "# Sort and get top N\n",
    "top_matches = sorted( results, key=lambda x: x[ 0 ], reverse=True )[ :5 ]\n"
   ],
   "id": "675d18dac5bd124d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#   Embedding-Pipeline Script\n",
    "___"
   ],
   "id": "3a7f735556cd0d3f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Load Dependencies\n",
    "\n"
   ],
   "id": "e593513dcb937f5f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-31T20:28:22.663413Z",
     "start_time": "2025-03-31T20:28:22.654167Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "import sqlite3\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import openai\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n"
   ],
   "id": "cf12b8bc356c9181",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Configuration",
   "id": "99ef339bdfea1fd8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "a764c56d86acecd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Define paths\n",
    "TEXT_FILE = 'PublicLaw_118-42.txt'\n",
    "DB_FILE = 'law_embeddings.data'\n",
    "EMBEDDING_MODEL = 'all-MiniLM-L6-v2'\n",
    "CHUNK_SIZE = 1000\n",
    "CHUNK_OVERLAP = 200\n"
   ],
   "id": "e8ee553da46c407c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Load and Clean Raw Text",
   "id": "842a900a1b80cde5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def load_and_clean_text( filepath ):\n",
    "    with open( filepath, 'r', encoding='utf-8' ) as file:\n",
    "        raw_text = file.read( )\n",
    "\n",
    "    # Basic normalization\n",
    "    text = re.sub( r'\\f+', ' ', raw_text )\n",
    "    text = re.sub( r'\\n+', ' ', text )\n",
    "    text = re.sub( r'\\s{2,}', ' ', text )\n",
    "    return text.strip( )\n"
   ],
   "id": "c202cc689865f32"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Generate Embeddings",
   "id": "ae3ddba5e85404c6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def get_embedding( text, model=OPENAI_MODEL ):\n",
    "    response = openai.Embedding.create( input=text, model=model )\n",
    "    return response[ 'data' [ 0 ][ 'embedding' ] ]\n",
    "\n",
    "\n",
    "def embed_chunks( chunks ):\n",
    "    embeddings = [ ]\n",
    "    for chunk in tqdm( chunks, desc='EmbeddingRequest chunks via OpenAI' ):\n",
    "        try:\n",
    "            embedding = get_embedding( chunk )\n",
    "            embeddings.append( embedding )\n",
    "        except Exception as e:\n",
    "            print( f'Error embedding chunk: {e}' )\n",
    "            embeddings.append( [ 0.0 ] * 1536 )  # Placeholder for failed requests\n",
    "    return embeddings"
   ],
   "id": "8fa7587e7a3094be"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Create SQLite DB\n",
    "\n"
   ],
   "id": "3c985e6d3ecd5e8f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def create_and_populate_db( chunks, embeddings, db_path ):\n",
    "    conn = sqlite3.connect( db_path )\n",
    "    cursor = conn.cursor( )\n",
    "    sql_create = '''\n",
    "    CREATE TABLE IF NOT EXISTS law_embeddings\n",
    "    (\n",
    "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "        chunk TEXT NOT NULL,\n",
    "        embedding BLOB NOT NULL\n",
    "    )\n",
    "    '''\n",
    "\n",
    "    cursor.execute( sql_create )\n",
    "    for chunk, vector in zip( chunks, embeddings ):\n",
    "        blob = pickle.dumps( vector )\n",
    "        sql_insert = 'INSERT INTO law_embeddings ( chunk, embedding ) VALUES ( ?, ? )'\n",
    "        cursor.execute( sql_insert, ( chunk, blob ) )\n",
    "\n",
    "    conn.commit( )\n",
    "    conn.close( )\n"
   ],
   "id": "ea15943e543e0d70"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Script",
   "id": "e82751929d4fd563"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# === MAIN ===\n",
    "def main():\n",
    "    print('Step 1: Load and clean documents')\n",
    "    cleaned_text = load_and_clean_text(TEXT_FILE)\n",
    "\n",
    "    print('Step 2: Chunking documents')\n",
    "    chunks = chunk_text(cleaned_text)\n",
    "    print(f'Total chunks: {len(chunks)}')\n",
    "\n",
    "    print('Step 3: EmbeddingRequest with OpenAI API')\n",
    "    embeddings = embed_chunks(chunks)\n",
    "\n",
    "    print('Step 4: Saving to SQLite')\n",
    "    create_and_populate_db(chunks, embeddings, DB_FILE)\n",
    "\n",
    "    print(f'Pipeline complete. Embeddings stored in: {DB_FILE}')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ],
   "id": "5f2d269aaa88212b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "E",
   "id": "de305bd9c7ccbd96"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# === 1. Load Model ===\n",
    "# You can try other models like 'all-MiniLM-L6-v2', 'all-mpnet-base-v2', or 'multi-qa-MiniLM-L6-cos-v1'\n",
    "model = SentenceTransformer( 'all-MiniLM-L6-v2' )\n",
    "\n",
    "# === 2. Embed Chunks ===\n",
    "def embed_with_sentence_transformers( texts, model ):\n",
    "    return model.encode( texts, show_progress_bar=True, convert_to_numpy=True )\n",
    "\n",
    "local_embeddings = embed_with_sentence_transformers( chunks, model )\n",
    "\n",
    "# === 3. Save in a DataFrame ===\n",
    "df_local = pd.DataFrame({\n",
    "    'chunk': chunks,\n",
    "    'embedding': list( local_embeddings )  # numpy arrays to list for DataFrame compatibility\n",
    "})\n",
    "\n",
    "# === 4. Save to Disk ===\n",
    "df_local.to_parquet( 'public_law_118_32_local_embeddings.parquet', index=False )\n",
    "\n",
    "# === 5. Preview ===\n",
    "df_local.head( 2 )\n",
    "\n"
   ],
   "id": "47df028a12155604"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Fine-Tuning",
   "id": "e1efd28f65f33d39"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import json\n",
    "import openai\n",
    "import os\n",
    "import pandas as pd\n",
    "from pprint import pprint\n"
   ],
   "id": "51f033fc98145091"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "client = openai.OpenAI(\n",
    "    api_key=os.environ.get('OPENAI_API_KEY'),\n",
    "    organization='<org id>',\n",
    "    project='<project id>',\n",
    ")"
   ],
   "id": "b11d00974dbaa480"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Read in the dataset we'll use for this task.\n",
    "# This will be the RecipesNLG dataset, which we've cleaned to only contain documents from www.cookbooks.com\n",
    "recipe_df = pd.read_csv('data/cookbook_recipes_nlg_10k.csv')\n",
    "\n",
    "recipe_df.head()"
   ],
   "id": "aa84f0c255158f4f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "system_message = 'You are a helpful recipe assistant. You are to extract the generic ingredients from each of the recipes provided.'\n",
    "\n",
    "\n",
    "def create_user_message(row):\n",
    "    return f'Title: {row['title']}\\n\\nIngredients: {row['ingredients']}\\n\\nGeneric ingredients: '\n",
    "\n",
    "\n",
    "def prepare_example_conversation(row):\n",
    "    return {\n",
    "        'messages': [\n",
    "            {'role': 'system', 'content': system_message},\n",
    "            {'role': 'user', 'content': create_user_message(row)},\n",
    "            {'role': 'assistant', 'content': row['NER']},\n",
    "        ]\n",
    "    }\n",
    "\n"
   ],
   "id": "d29e9579f6f0e9dd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# use the first 100 rows of the dataset for training\n",
    "training_df = recipe_df.loc[0:100]\n",
    "\n",
    "# apply the prepare_example_conversation function to each row of the training_df\n",
    "training_data = training_df.apply(prepare_example_conversation, axis=1).tolist()\n",
    "\n",
    "for example in training_data[:5]:\n",
    "    print(example)"
   ],
   "id": "23bdc89006166778"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "validation_df = recipe_df.loc[101:200]\n",
    "validation_data = validation_df.apply(\n",
    "    prepare_example_conversation, axis=1).tolist()"
   ],
   "id": "4773228f51c07750"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def write_jsonl(data_list: list, filename: str) -> None:\n",
    "    with open(filename, 'w') as out:\n",
    "        for ddict in data_list:\n",
    "            jout = json.dumps(ddict) + '\\n'\n",
    "            out.write(jout)"
   ],
   "id": "6dcf445d39eee404"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "652e2134d8966009"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "training_file_name = 'tmp_recipe_finetune_training.jsonl'\n",
    "write_jsonl(training_data, training_file_name)\n",
    "\n",
    "validation_file_name = 'tmp_recipe_finetune_validation.jsonl'\n",
    "write_jsonl(validation_data, validation_file_name)"
   ],
   "id": "a492a14231147287"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "170bad89408062ff"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def upload_file(file_name: str, purpose: str) -> str:\n",
    "    with open(file_name, 'rb') as file_fd:\n",
    "        response = client.files.create(file=file_fd, purpose=purpose)\n",
    "    return response.id"
   ],
   "id": "10f941082775954c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "751aceb620d9c7bb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "training_file_id = upload_file(training_file_name, 'fine-tune')\n",
    "validation_file_id = upload_file(validation_file_name, 'fine-tune')"
   ],
   "id": "c8dbd2395ef59d03"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "feadf4f400f8454c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "MODEL = 'gpt-4o-mini-2024-07-18'\n",
    "\n",
    "response = client.fine_tuning.jobs.create(\n",
    "    training_file=training_file_id,\n",
    "    validation_file=validation_file_id,\n",
    "    model=MODEL,\n",
    "    suffix='recipe-ner',\n",
    ")\n",
    "\n",
    "job_id = response.id"
   ],
   "id": "5ca162663d00cad3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
