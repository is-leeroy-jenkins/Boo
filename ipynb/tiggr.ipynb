{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# NLTK\n",
    "___\n"
   ],
   "id": "700c6019c3802579"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 📦 One-Time Setup (NLTK Resources)",
   "id": "17b64a5a35525229"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import io\n",
    "\n",
    "# NLTK needs to download items onces\n",
    "import nltk\n",
    "from nltk.corpus import words\n",
    "from isort.format import remove_whitespace\n",
    "\n",
    "\n",
    "nltk.download( 'punkt' )\n",
    "nltk.download( 'punkt_tab' )\n",
    "nltk.download( 'stopwords' )\n",
    "nltk.download( 'wordnet' )\n",
    "nltk.download( 'omw-1.4' )\n",
    "nltk.download( 'words' )\n"
   ],
   "id": "ea993d341972b4d9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Load Dependencies",
   "id": "3957678e92123d8f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T13:06:33.740012Z",
     "start_time": "2025-05-16T13:06:33.715046Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "import re\n",
    "import os\n",
    "from pathlib import Path\n",
    "import openai as OpenAI\n",
    "import pandas as pd\n",
    "import time\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "import pickle\n",
    "import unicodedata\n",
    "from bs4 import BeautifulSoup\n",
    "from textblob import Word\n",
    "from collections import Counter\n",
    "from typing import Any, List, Tuple, Optional, Union, Dict\n",
    "import ipywidgets as widgets, IPython, platform, ipywidgets, jupyterlab\n",
    "from importlib import reload\n"
   ],
   "id": "9a5c2c52dd74d494",
   "outputs": [],
   "execution_count": 342
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Text Processing",
   "id": "ee23478183ced671"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### ✅ Checklist\n",
    "\n",
    "| Step | Task                     | Function/Library               |\n",
    "|------|--------------------------|--------------------------------|\n",
    "| 1    | Load Text                | `open()`, `pandas.read_csv()` |\n",
    "| 2    | Convert to Lowercase     | `.lower()`                     |\n",
    "| 3    | Remove Punctuation       | `string.punctuation`           |\n",
    "| 4    | Remove Numbers           | `re.sub()`                     |\n",
    "| 5    | Trim Whitespaces         | `' '.join()`                   |\n",
    "| 6    | Tokenization             | `nltk.word_tokenize()`         |\n",
    "| 7    | Remove Stopwords         | `nltk.corpus.stopwords`        |\n",
    "| 8    | Lemmatization/Stemming   | `WordNetLemmatizer`, `PorterStemmer` |\n",
    "| 9    | Reconstruct Clean Text   | `' '.join()`                   |\n",
    "| 10   | (Optional) Spellcheck    | `TextBlob.correct()`           |\n",
    "| 11   | Vectorization for ML     | `TfidfVectorizer`, `CountVectorizer`, Word Embeddings |\n",
    "\n",
    "\n",
    "___"
   ],
   "id": "947b8eb1368fe0c4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1.  Load File",
   "id": "36fbc578426999c8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T14:27:09.611342Z",
     "start_time": "2025-05-14T14:27:09.599830Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === Load Raw Text ===\n",
    "file_path = '<url to file>'\n",
    "_rawtext = ''\n",
    "\n",
    "\n",
    "def load_text( file_path ):\n",
    "\twith open( file_path, 'r', encoding='utf-8' ) as f:\n",
    "\t\t_rawtext = f.read( )\n",
    "\t\treturn _rawtext\n"
   ],
   "id": "e3d5cc42b6dd4e3b",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2.  Clean Whitespace\n",
    "- Consecutive whitespace reduced to a single space\n",
    "- Leading/trailing spaces removed\n",
    "- Blank lines removed"
   ],
   "id": "ea7db4bf069f99e1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T14:22:37.949923Z",
     "start_time": "2025-05-15T14:22:37.939780Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def clean_whitespace( text: str ) -> str:\n",
    "\t\"\"\"\n",
    "\n",
    "\t\tThis function:\n",
    "\t\t_____________\n",
    "        Removes extra spaces and blank tokens from the path pages.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        pages : str\n",
    "            The raw path pages path to be cleaned_lines.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        str\n",
    "            A cleaned_lines pages path with:\n",
    "                - Consecutive whitespace reduced to a single space\n",
    "                - Leading/trailing spaces removed\n",
    "                - Blank tokens removed\n",
    "\n",
    "    \"\"\"\n",
    "\t# Replace multiple spaces or tabs with a single space\n",
    "\ttext = re.sub( r'[ \\t+]', ' ', text )\n",
    "\n",
    "\t# Remove empty tokens\n",
    "\tcleaned_lines = [ line for line in text if line ]\n",
    "\n",
    "\t# Join tokens back into a single string\n",
    "\tcleaned_text = ''.join( cleaned_lines )\n",
    "\n",
    "\treturn cleaned_text"
   ],
   "id": "14c5765f6bf2af38",
   "outputs": [],
   "execution_count": 173
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3. Normalize",
   "id": "63bd713fbba95faa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T12:45:31.090172Z",
     "start_time": "2025-05-15T12:45:31.069758Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def normalize( text: str ) -> str:\n",
    "\t\"\"\"\n",
    "\n",
    "        This function:\n",
    "        Normalizes the path pages path.\n",
    "          - Converts pages to lowercase\n",
    "          - Removes accented characters (e.g., é -> e)\n",
    "          - Removes leading/trailing spaces\n",
    "          - Collapses multiple whitespace characters into a single space\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        pages : str\n",
    "            The raw path pages path to be normalized.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        str\n",
    "            A normalized, cleaned_lines version of the path path.\n",
    "\n",
    "    \"\"\"\n",
    "\treturn unicodedata.normalize( 'NFKD', text ).encode( 'ascii', 'ignore' ).decode( 'utf-8' )"
   ],
   "id": "82515d328d466a74",
   "outputs": [],
   "execution_count": 58
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T14:02:56.974346Z",
     "start_time": "2025-05-15T14:02:56.967036Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### 3. Collapse Space\n",
    "def collapse_space( text: str ) -> str:\n",
    "    \"\"\"\n",
    "\n",
    "    Purpose:\n",
    "    _________\n",
    "    Convert two or more consecutive\n",
    "    spaces into a single space using regular expressions.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text containing multiple spaces.\n",
    "\n",
    "    Returns:\n",
    "        str: The text with multiple spaces collapsed into a single space.\n",
    "    \"\"\"\n",
    "    return re.sub(r' {2,}', ' ', text)"
   ],
   "id": "139e8a9deec449a9",
   "outputs": [],
   "execution_count": 156
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T14:08:52.495241Z",
     "start_time": "2025-05-15T14:08:52.485063Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "6d18d3d035ef6a39",
   "outputs": [],
   "execution_count": 159
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 4. Remove Punctuation",
   "id": "5e0ee21b82c3c43"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T15:11:15.088465Z",
     "start_time": "2025-05-15T15:11:15.081525Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def remove_punctuation( text: str ) -> str:\n",
    "    \"\"\"\n",
    "\n",
    "\t\tRemove non-alphanumeric characters and extra whitespace from text.\n",
    "\n",
    "\t\tArgs:\n",
    "\t\t\ttext (str): The input text string.\n",
    "\n",
    "\t\tReturns:\n",
    "\t\t\tstr: Cleaned text string with only alphanumeric characters and spaces.\n",
    "\n",
    "    \"\"\"\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text"
   ],
   "id": "3ec4704593ac2991",
   "outputs": [],
   "execution_count": 227
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 5. Trim Whitespace\n",
    "\n",
    "- Removes leading and trailing whitespace\n",
    "- Replaces multiple internal spaces with a single space"
   ],
   "id": "5234418835bd42fa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T13:36:50.005438Z",
     "start_time": "2025-05-15T13:36:49.998882Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def trim_whitespace( text: str ) -> str:\n",
    "\t\"\"\"\n",
    "\n",
    "        Trims whitespace from the path pages path.\n",
    "\n",
    "        This function:\n",
    "          - Removes leading and trailing whitespace\n",
    "          - Replaces multiple internal spaces with a single space\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        pages : str\n",
    "            The raw path path with potential extra whitespace.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        str\n",
    "            The cleaned_lines path with trimmed and normalized whitespace.\n",
    "\n",
    "    \"\"\"\n",
    "\t# Replace multiple whitespace characters (spaces, tabs, etc.) with a single space\n",
    "\tcleaned_text = re.sub( r'\\s+', ' ', text )\n",
    "\treturn cleaned_text"
   ],
   "id": "37ade6f5b1d34922",
   "outputs": [],
   "execution_count": 130
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 6. Lemmatize\n",
    "- Reduces words to their base or root form.\n",
    "- Converts text to lowercase\n",
    "- Tokenizes the text into words\n",
    "- Lemmatizes each token using WordNetLemmatizer\n",
    "- Reconstructs the lemmatized tokens into a single string"
   ],
   "id": "5f4db8f82f926ae"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T12:45:43.337314Z",
     "start_time": "2025-05-15T12:45:43.326208Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def lemmatize( text: str ) -> str:\n",
    "\t\"\"\"\n",
    "\n",
    "        Performs lemmatization on the path pages path.\n",
    "\n",
    "        This function:\n",
    "          - Converts pages to lowercase\n",
    "          - Tokenizes the pages into words\n",
    "          - Lemmatizes each token using WordNetLemmatizer\n",
    "          - Reconstructs the lemmatized tokens into a single path\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        pages : str\n",
    "            The path pages path to be lemmatized.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        str\n",
    "            A path with all words lemmatized.\n",
    "\n",
    "    \"\"\"\n",
    "\t# Initialize lemmatizer\n",
    "\tlemmatizer = WordNetLemmatizer( )\n",
    "\n",
    "\tlower_case = text.lower( )\n",
    "\t# Convert to lowercase and tokenize_text\n",
    "\ttokens = word_tokenize( lower_case )\n",
    "\n",
    "\t# Lemmatize each token\n",
    "\tlemmatized_tokens = [ lemmatizer.lemmatize( token ) for token in tokens ]\n",
    "\n",
    "\t# Join tokens back to a path\n",
    "\tlemmatized_text = ' '.join( lemmatized_tokens )\n",
    "\n",
    "\treturn lemmatized_text"
   ],
   "id": "93dcb509d45e16f5",
   "outputs": [],
   "execution_count": 63
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 7. Tokenize\n",
    "\n",
    "- Converts text to lowercase\n",
    "- Uses NLTK's word_tokenize to split the text into words and punctuation tokens"
   ],
   "id": "76b7a3f02f7d210"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T12:45:48.756071Z",
     "start_time": "2025-05-15T12:45:48.744091Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def tokenize( words: List[ str ] ) -> List[ str ]:\n",
    "\t\"\"\"\n",
    "\n",
    "        This function:\n",
    "          - Tokenizes the path pages path into individual word tokens.\n",
    "          - Converts pages to lowercase\n",
    "          - Uses NLTK's word_tokenize to split the pages into words and punctuation tokens\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        words : List[ str ]\n",
    "            A list of strings to be tokenized.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "            A list of token strings (words and punctuation) extracted from the pages.\n",
    "\n",
    "    \"\"\"\n",
    "\t# Convert to lowercase\n",
    "\ttokens = [ ]\n",
    "\tfor w in words:\n",
    "\t\t_token = nltk.word_tokenize( w )\n",
    "\t\ttokens.append( _token )\n",
    "\treturn tokens\n"
   ],
   "id": "3e56143f4b97ab1f",
   "outputs": [],
   "execution_count": 64
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 8. Remove Special Characters\n",
    "- Retains only alphanumeric characters and whitespace\n",
    "- Removes symbols like @, #, $, %, &, etc.\n",
    "- Preserves letters, numbers, and spaces"
   ],
   "id": "1ffdd326d8a48815"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T14:25:56.025342Z",
     "start_time": "2025-05-15T14:25:56.014071Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def remove_special( text: str ) -> str:\n",
    "\t\"\"\"\n",
    "\n",
    "        Remove all special characters\n",
    "        from the input string, keeping only letters, digits, and whitespace.\n",
    "\n",
    "        Args:\n",
    "            text (str): The input string to process.\n",
    "\n",
    "        Returns:\n",
    "            str: The processed string with special characters removed.\n",
    "\n",
    "    \"\"\"\n",
    "\tcleaned = [ ]\n",
    "\tkeepers = [ '(', ')', '$', '.', ';', ':', ' - '  ]\n",
    "\tfor char in text:\n",
    "\t\tif char in keepers:\n",
    "\t\t\tcleaned.append( char )\n",
    "\t\telif char.isalnum( ) or char.isspace( ):\n",
    "\t\t\tcleaned.append( char )\n",
    "\treturn ''.join( cleaned )"
   ],
   "id": "d915bd40a8960596",
   "outputs": [],
   "execution_count": 178
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T13:57:25.016453Z",
     "start_time": "2025-05-15T13:57:24.998646Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def remove_special_characters( text: str, keep_spaces: bool=True ) -> str:\n",
    "\t\"\"\"\n",
    "\n",
    "\t\tPurpose:\n",
    "\t\t_______\n",
    "\t\tRemove special characters from the text.\n",
    "\n",
    "\t\tArgs:\n",
    "\t\t\ttext (str): Input string to clean.\n",
    "\t\t\tkeep_spaces (bool): If True, preserves spaces; otherwise removes all non-alphanumerics.\n",
    "\n",
    "\t\tReturns:\n",
    "\t\t\tstr: Cleaned text with only alphanumeric characters (and optionally spaces).\n",
    "\t\"\"\"\n",
    "\tif keep_spaces:\n",
    "\t\treturn re.sub(r'[^a-zA-Z0-9\\s]', ' ', text)\n",
    "\telse:\n",
    "\t\treturn re.sub(r'[^a-zA-Z0-9]', ' ', text)"
   ],
   "id": "56565e0606ba12",
   "outputs": [],
   "execution_count": 153
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 9. Remove HTML\n",
    "- Parses the text as HTML\n",
    "- Extracts and returns only the visible content without tags"
   ],
   "id": "3e5583a42894f537"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T22:09:00.563550Z",
     "start_time": "2025-05-14T22:09:00.555193Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def remove_html_tags( text: str ) -> str:\n",
    "\t\"\"\"\n",
    "\n",
    "\n",
    "        This function:\n",
    "        Removes HTML tags from the path pages path.\n",
    "          - Parses the pages as HTML\n",
    "          - Extracts and returns only the visible content without tags\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        pages : str\n",
    "            The path pages containing HTML tags.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        str\n",
    "            A cleaned_lines path with all HTML tags removed.\n",
    "\n",
    "    \"\"\"\n",
    "\t# Parse HTML and extract pages\n",
    "\tsoup = BeautifulSoup( text, \"raw_html.parser\" )\n",
    "\tcleaned_text = soup.get_text( separator=' ', strip=True )\n",
    "\n",
    "\treturn cleaned_text"
   ],
   "id": "d5e9b16f93ccfed8",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 10. Chunk Tokens\n",
    "- Tokenizes the text into words\n",
    "- Groups them into consecutive word chunks\n",
    "- Returns a list of strings (each chunk_tokens)"
   ],
   "id": "b9e77cd71f1df62e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T19:57:16.365220Z",
     "start_time": "2025-05-15T19:57:16.352406Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def chunk_tokens( text: List[ str ], chunk_size: int=50 ) -> List[ List[ str ] ]:\n",
    "\t\"\"\"\n",
    "\n",
    "\n",
    "        Breaks a list of cleaned_lines,\n",
    "        tokenized strings into\n",
    "        chunks of a specified num of tokens.\n",
    "\n",
    "        This function:\n",
    "          - Flattens the path get_list of tokenized strings (i.e., get_list of lists)\n",
    "          - Groups tokens into chunks of min `chunk_size`\n",
    "          - Returns a list of lists of tokens\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        pages : get_list of tokenizd words\n",
    "            The path get_list where each element is a get_list of tokens (words).\n",
    "\n",
    "        chunk_size : int, optional (default=50)\n",
    "            Number of tokens per chunk_tokens.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        List[ List[ str ] ]\n",
    "            A list of a list of token chunks. Each chunk is a list of tokens.\n",
    "\n",
    "    \"\"\"\n",
    "\t# Flatten the get_list of token lists into a single get_list\n",
    "\tall_tokens = [ token for sublist in text for token in sublist ]\n",
    "\n",
    "\t# Create chunks of tokens\n",
    "\tchunks = [\n",
    "\t\tall_tokens[ i : i + chunk_size ]\n",
    "\t\tfor i in range( 0, len( all_tokens ), chunk_size ) ]\n",
    "\n",
    "\treturn chunks"
   ],
   "id": "a6edbb0eb8fa5720",
   "outputs": [],
   "execution_count": 307
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 11. Chunk Text\n",
    "\n",
    "- Converts text to lowercase\n",
    "- Tokenizes text using NLTK's word_tokenize\n",
    "- Breaks tokens into chunks of a specified size\n",
    "- Optionally joins tokens into strings (for transformer models)"
   ],
   "id": "e93a4c6067321b6d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T17:06:21.576087Z",
     "start_time": "2025-05-15T17:06:21.560991Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def chunk_text( text: str, chunk_size: int=50, return_as_string: bool=True ) -> List[ str ]:\n",
    "\t\"\"\"\n",
    "\n",
    "        This function:\n",
    "        Tokenizes cleaned_lines pages and breaks it into chunks for downstream vectors.\n",
    "          - Converts pages to lowercase\n",
    "          - Tokenizes pages using NLTK's word_tokenize\n",
    "          - Breaks tokens into chunks of a specified size\n",
    "          - Optionally joins tokens into strings (for transformer models)\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        pages : str\n",
    "            The cleaned_lines path pages to be tokenized and chunked.\n",
    "\n",
    "        chunk_size : int, optional (default=50)\n",
    "            Number of tokens per chunk_tokens.\n",
    "\n",
    "        return_string : bool, optional (default=True)\n",
    "            If True, returns each chunk_tokens as a path; otherwise, returns a get_list of tokens.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        get_list\n",
    "            A get_list of token chunks. Each chunk_tokens is either a get_list of tokens or a path.\n",
    "\n",
    "    \"\"\"\n",
    "\t# Download tokenizer models (only once)\n",
    "\tnltk.download( 'punkt', quiet=True )\n",
    "\ttokens = word_tokenize( text )\n",
    "\ttoken_chunks = [ tokens[ i:i + chunk_size ] for i in range( 0, len( tokens ), chunk_size ) ]\n",
    "\tif return_as_string:\n",
    "\t\treturn [ ' '.join( chunk ) for chunk in token_chunks ]\n",
    "\telse:\n",
    "\t\treturn token_chunks"
   ],
   "id": "f6b1ee045b318fd5",
   "outputs": [],
   "execution_count": 276
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 12. Remove Errors\n",
    "- Converts text to lowercase\n",
    "- Tokenizes the text into words\n",
    "- Filters out words not recognized as valid English using TextBlob\n",
    "- Returns a string with only correctly spelled words"
   ],
   "id": "b1e7c36fc4666c45"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T23:38:24.183644Z",
     "start_time": "2025-05-14T23:38:24.174188Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def remove_errors( text: str ) -> str:\n",
    "    \"\"\"\n",
    "\n",
    "\t\tPurpose:\n",
    "\t\t________\n",
    "\t\tRemove all non-English\n",
    "\t\twords but preserve valid numbers\n",
    "\t\tusing the NLTK English vocabulary.\n",
    "\n",
    "\t\tArgs:\n",
    "\t\t\ttext (str): The input text string.\n",
    "\n",
    "\t\tReturns:\n",
    "\t\t\tstr: The cleaned string with only English words and numbers.\n",
    "\n",
    "    \"\"\"\n",
    "    english_vocab = set( w.lower() for w in words.words() )\n",
    "\n",
    "    # Tokenize: includes words and numbers\n",
    "    tokens = re.findall(r'\\b[\\w.]+\\b', text.lower())\n",
    "\n",
    "    # Keep words in vocab or numbers\n",
    "    def is_valid_token( tok: str ) -> bool:\n",
    "        return tok in english_vocab or tok.replace( '.', '', 1 ).isdigit( )\n",
    "\n",
    "    valid_tokens = [tok for tok in tokens if is_valid_token( tok ) ]\n",
    "    return ' '.join(valid_tokens)"
   ],
   "id": "818357851dd8bf54",
   "outputs": [],
   "execution_count": 51
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 13. Correct Errors\n",
    "- Converts text to lowercase\n",
    "- Tokenizes the text into words\n",
    "- Applies spelling correction using TextBlob\n",
    "- Reconstructs and returns the corrected text"
   ],
   "id": "5c6cdef587829c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T22:10:19.219996Z",
     "start_time": "2025-05-14T22:10:19.208905Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def correct_errors( text: str ) -> str:\n",
    "\t\"\"\"\n",
    "\n",
    "        Corrects misspelled words in the path pages path.\n",
    "\n",
    "        This function:\n",
    "          - Converts pages to lowercase\n",
    "          - Tokenizes the pages into words\n",
    "          - Applies spelling correction using TextBlob\n",
    "          - Reconstructs and returns the corrected pages\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        pages : str\n",
    "            The path pages path with potential spelling mistakes.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        str\n",
    "            A corrected version of the path path with proper English words.\n",
    "\n",
    "    \"\"\"\n",
    "\t# Convert to lowercase and tokenize_text\n",
    "\ttokens = word_tokenize( text.lower( ) )\n",
    "\n",
    "\t# Apply spelling correction to each token\n",
    "\tcorrected_tokens = [ str( Word( word ).correct( ) ) for word in tokens ]\n",
    "\n",
    "\t# Join the corrected words into a single path\n",
    "\tcorrected_text = ' '.join( corrected_tokens )\n",
    "\n",
    "\treturn corrected_text"
   ],
   "id": "c7d8dfb08dfbf717",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 14.  Remove Headers\n",
    "- Assumes repeated lines at the top or bottom (like titles, page numbers)\n",
    "- Removes lines that are common across multiple pages (heuristic)\n",
    "- Returns cleaned text with main body content only"
   ],
   "id": "d8489c8d8d9d0af4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T22:10:22.553751Z",
     "start_time": "2025-05-14T22:10:22.543038Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def remove_headers_footers( text: str ) -> str:\n",
    "\t\"\"\"\n",
    "    Removes common headers and footers from a pages document.\n",
    "\n",
    "    This function:\n",
    "      - Assumes repeated tokens at the top or bottom (like titles, page numbers)\n",
    "      - Removes tokens that are common across multiple pages (heuristic)\n",
    "      - Returns cleaned_lines pages with main body content only\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    pages : str\n",
    "        The path pages potentially containing headers/footers.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    str\n",
    "        The cleaned_lines pages with headers and footers removed.\n",
    "    \"\"\"\n",
    "\t# Split the pages into tokens\n",
    "\tlines = text.splitlines( )\n",
    "\n",
    "\t# Remove empty tokens and trim whitespace\n",
    "\tlines = [ line.strip( ) for line in lines if line.strip( ) ]\n",
    "\n",
    "\t# Count line frequencies to identify repeated headers/footers\n",
    "\tline_counts = Counter( lines )\n",
    "\n",
    "\t# Identify frequent tokens (appear in >1% of total tokens)\n",
    "\tthreshold = max( 1, int( len( lines ) * 0.01 ) )\n",
    "\trepeated_lines = { line for line, count in line_counts.items( ) if count > threshold }\n",
    "\n",
    "\t# Remove tokens that are likely headers or footers\n",
    "\tbody_lines = [ line for line in lines if line not in repeated_lines ]\n",
    "\n",
    "\t# Reconstruct the cleaned_lines pages\n",
    "\tcleaned_text = '\\n'.join( body_lines )\n",
    "\n",
    "\treturn cleaned_text\n"
   ],
   "id": "f9c1ba26ea66c445",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 15.  Remove Formatting\n",
    "- Strips HTML tags\n",
    "- Removes Markdown syntax (e.g., *, #, [], etc.)\n",
    "- Collapses whitespace (newlines, tabs)\n",
    "- Optionally removes special characters for clean unformatted text"
   ],
   "id": "76f3936ac0e24e17"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T22:10:25.179509Z",
     "start_time": "2025-05-14T22:10:25.167582Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def remove_formatting( text: str ) -> str:\n",
    "\t\"\"\"\n",
    "\n",
    "        Removes formatting artifacts (Markdown, HTML, control characters) from pages.\n",
    "\n",
    "        This function:\n",
    "          - Strips HTML tags\n",
    "          - Removes Markdown syntax (e.g., *, #, [], etc.)\n",
    "          - Collapses whitespace (newlines, tabs)\n",
    "          - Optionally removes special characters for clean unformatted pages\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        pages : str\n",
    "            The formatted path pages.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        str\n",
    "            A cleaned_lines version of the pages with formatting removed.\n",
    "\n",
    "    \"\"\"\n",
    "\t# Remove HTML tags\n",
    "\ttext = BeautifulSoup( text, \"raw_html.parser\" ).get_text( separator=' ', strip=True )\n",
    "\n",
    "\t# Remove Markdown syntax\n",
    "\ttext = re.sub( r'\\[.*?\\]\\(.*?\\)', '', text )  # Markdown links\n",
    "\ttext = re.sub( r'[`_*#~>-]', '', text )  # Markdown chars\n",
    "\ttext = re.sub( r'!\\[.*?\\]\\(.*?\\)', '', text )  # Markdown images\n",
    "\n",
    "\t# Remove control characters and normalize whitespace\n",
    "\ttext = re.sub( r'[\\r\\n\\t]+', ' ', text )  # Newlines, tabs\n",
    "\ttext = re.sub( r'\\s+', ' ', text ).strip( )  # Collapse multiple spaces\n",
    "\n",
    "\treturn text"
   ],
   "id": "be4039da12904496",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 16. Remove Stopwords\n",
    "- Tokenizes the input text\n",
    "- Removes common stopwords (e.g., \"the\", \"is\", \"and\", etc.)\n",
    "- Returns the text with only meaningful words"
   ],
   "id": "1c75687f6c08c2c5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T13:22:37.383568Z",
     "start_time": "2025-05-16T13:22:37.373860Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def removestopwords( text: str ) -> str:\n",
    "\t\"\"\"\n",
    "\n",
    "        This function:\n",
    "          - Removes English stopwords from the path pages path.\n",
    "          - Tokenizes the path pages\n",
    "          - Removes common stopwords (e.g., \"the\", \"is\", \"and\", etc.)\n",
    "          - Returns the pages with only meaningful words\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        pages : str\n",
    "            The path pages path.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        str\n",
    "            A string without stopwords.\n",
    "\n",
    "    \"\"\"\n",
    "\t# Define English stopword pairs\n",
    "\tstop_words = set( stopwords.words( 'english' ) )\n",
    "\n",
    "\t# Tokenize and lowercase\n",
    "\ttokens = word_tokenize( text.lower( ) )\n",
    "\n",
    "\t# Remove stopwords\n",
    "\tfiltered_tokens = [ word for word in tokens if word.isalnum( ) and word not in stop_words ]\n",
    "\n",
    "\t# Join tokens back into a path\n",
    "\tcleaned_text = ' '.join( filtered_tokens )\n",
    "\n",
    "\treturn cleaned_text\n"
   ],
   "id": "2e51da785742262a",
   "outputs": [],
   "execution_count": 365
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 17. Split Sentences",
   "id": "66e071eacb3377be"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T13:31:55.493355Z",
     "start_time": "2025-05-15T13:31:55.482613Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def split_sentences( text: str ) -> List[ str] :\n",
    "    \"\"\"\n",
    "\n",
    "\t\tPurpose:\n",
    "\t\t________\n",
    "\t\tSplits the input text string into a list of\n",
    "\t\tindividual sentences using NLTK's Punkt sentence tokenizer.\n",
    "\t\tThis function is useful for preparing text for further linguistic processing,\n",
    "\t\tsuch as tokenization, parsing, or named entity recognition.\n",
    "\n",
    "\t\tParameters\n",
    "\t\t----------\n",
    "\t\ttext : str\n",
    "\t\t\tThe raw text string to be segmented into sentences.\n",
    "\n",
    "\t\tReturns\n",
    "\t\t-------\n",
    "\t\tList[str]\n",
    "\t\t\tA list of sentence strings, each corresponding to a single sentence detected\n",
    "\t\t\tin the input text.\n",
    "\n",
    "    \"\"\"\n",
    "    return sent_tokenize( text )"
   ],
   "id": "9667d83b7478ae0d",
   "outputs": [],
   "execution_count": 123
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 🔍 Usage",
   "id": "f4c860851a941aeb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T13:41:15.308910Z",
     "start_time": "2025-05-15T13:41:15.295469Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def clean_text( text: str ) -> str:\n",
    "\t_first = text.replace( '\\r\\n', '\\n' ).replace( '\\r', '\\n' )\n",
    "\t_second = re.sub( r'\\n\\s*\\d+\\s*\\n', '\\n', _first )\n",
    "\t_third = re.sub( r'(\\w+)-\\n(\\w+)', r'\\1\\2', _second )\n",
    "\t_fouth = re.sub( r'(?<!\\n)\\n(?![\\n])', ' ', _third )\n",
    "\t_retval = re.sub( r'\\s+', ' ', _fouth )\n",
    "\treturn _retval"
   ],
   "id": "6ffc1b387c68d803",
   "outputs": [],
   "execution_count": 136
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 🔍 Pipeline",
   "id": "f9435a341c128b29"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T14:28:06.587921Z",
     "start_time": "2025-05-14T14:28:06.568106Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === Preprocessing Configuration ===\n",
    "EN_STOPWORDS = set( stopwords.words( 'english' ) )\n",
    "LEMMATIZER = WordNetLemmatizer( )\n",
    "STEMMER = SnowballStemmer( 'english' )\n",
    "\n",
    "\n",
    "def preprocess_line( line, lower=True, punctuation=True,\n",
    "                     stopwords=False, lemmatize=True, stem=False ):\n",
    "\t'''\n",
    "    Process a single line of documents with optional steps:\n",
    "    - lower\n",
    "    - punctuation removal\n",
    "    - stopword removal\n",
    "    - lemmatization\n",
    "    - stemming (optional)\n",
    "    Returns the cleaned_lines line as a path.\n",
    "    '''\n",
    "\ttokens = word_tokenize( line )\n",
    "\tprocessed = [ ]\n",
    "\tfor token in tokens:\n",
    "\t\tif lower:\n",
    "\t\t\ttoken = token.lower( )\n",
    "\n",
    "\t\tif punctuation and token in string.punctuation:\n",
    "\t\t\tcontinue\n",
    "\n",
    "\t\tif stopwords and token in EN_STOPWORDS:\n",
    "\t\t\tcontinue\n",
    "\n",
    "\t\tif lemmatize:\n",
    "\t\t\ttoken = LEMMATIZER.lemmatize( token )\n",
    "\n",
    "\t\tif stem:\n",
    "\t\t\ttoken = STEMMER.stem( token )\n",
    "\n",
    "\t\tprocessed.append( token )\n",
    "\n",
    "\treturn ' '.join( processed )\n",
    "\n",
    "\n",
    "def process_file( file_path, **preprocess_kwargs ):\n",
    "\t'''\n",
    "        Read a documents file line-by-line, apply preprocessing pipeline to each line,\n",
    "        and return a get_list of cleaned_lines tokens (original order preserved).\n",
    "        Pass any keyword args supported by `preprocess_line`.\n",
    "    '''\n",
    "\tcleaned_lines = [ ]\n",
    "\twith open( file_path, 'r', encoding='utf-8' ) as file:\n",
    "\t\tfor line in file:\n",
    "\t\t\tcleaned = preprocess_line( line, **preprocess_kwargs )\n",
    "\t\t\tcleaned_lines.append( cleaned )\n",
    "\treturn cleaned_lines\n"
   ],
   "id": "af7f3fd7b2990387",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "91981ee2b140d645"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "c7c6e615a319e113"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "\n",
   "id": "d901568b8ef44067"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "c6aca1d3fff8a8ac"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a0478bcf04eb5543"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#  Vectorization (Preparation for ML Models)\n",
    "___\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "ba0f01ba423430f4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Load Dependencies",
   "id": "d3e02f4dbedcc22e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T22:56:53.926365Z",
     "start_time": "2025-05-14T22:56:53.918962Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch"
   ],
   "id": "6ea810a20c6020f2",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T22:56:58.455941Z",
     "start_time": "2025-05-14T22:56:58.449512Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === Load Raw Text ===\n",
    "file_path = '<url to file>'\n",
    "_rawtext = ''\n",
    "\n",
    "\n",
    "def load_text( file_path ):\n",
    "\twith open( file_path, 'r', encoding='utf-8' ) as f:\n",
    "\t\t_rawtext = f.read( )\n",
    "\t\treturn _rawtext"
   ],
   "id": "f930499beb91e45f",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "4cd82d1886e5243"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 🧮 1. Bag of Words (BoW) using CountVectorizer",
   "id": "93554b0db878266a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T22:57:25.676359Z",
     "start_time": "2025-05-14T22:57:25.657084Z"
    }
   },
   "cell_type": "code",
   "source": [
    "corpus = [ 'Bro loves clean code.', 'Code is life.' ]\n",
    "vectorizer = CountVectorizer( )\n",
    "X = vectorizer.fit_transform( corpus )\n",
    "\n",
    "print( vectorizer.get_feature_names_out( ) )\n",
    "print( X.toarray( ) )\n"
   ],
   "id": "df2a01ee327d57d6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bro' 'clean' 'code' 'is' 'life' 'loves']\n",
      "[[1 1 1 0 0 1]\n",
      " [0 0 1 1 1 0]]\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 📊 2. TF-IDF using TfidfVectorizer",
   "id": "606ca62c6f083d04"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T22:57:28.084090Z",
     "start_time": "2025-05-14T22:57:28.068021Z"
    }
   },
   "cell_type": "code",
   "source": [
    "corpus = [ 'Bro writes awesome code.', 'Code must be clean and clear.' ]\n",
    "vectorizer = TfidfVectorizer( )\n",
    "X = vectorizer.fit_transform( corpus )\n",
    "\n",
    "print( vectorizer.get_feature_names_out( ) )\n",
    "print( X.toarray( ) )\n"
   ],
   "id": "58a4cf4ec025e261",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and' 'awesome' 'be' 'bro' 'clean' 'clear' 'code' 'must' 'writes']\n",
      "[[0.         0.53404633 0.         0.53404633 0.         0.\n",
      "  0.37997836 0.         0.53404633]\n",
      " [0.4261596  0.         0.4261596  0.         0.4261596  0.4261596\n",
      "  0.30321606 0.4261596  0.        ]]\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 🧠 3. Word2Vec using gensim",
   "id": "cd392a973450c93a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T22:57:30.828776Z",
     "start_time": "2025-05-14T22:57:30.765259Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sentences = [ [ 'bro', 'loves', 'python' ], [ 'clean', 'code', 'rocks' ] ]\n",
    "model = Word2Vec( sentences, vector_size=100, window=5, min_count=1, workers=4 )\n",
    "\n",
    "# Vector for the word 'bro'\n",
    "vector = model.wv[ 'bro' ]\n",
    "print( vector )\n"
   ],
   "id": "a20f65018fd9c119",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-8.7274825e-03  2.1301615e-03 -8.7354420e-04 -9.3190884e-03\n",
      " -9.4281426e-03 -1.4107180e-03  4.4324086e-03  3.7040710e-03\n",
      " -6.4986930e-03 -6.8730675e-03 -4.9994122e-03 -2.2868442e-03\n",
      " -7.2502876e-03 -9.6033178e-03 -2.7436293e-03 -8.3628409e-03\n",
      " -6.0388758e-03 -5.6709289e-03 -2.3441375e-03 -1.7069972e-03\n",
      " -8.9569986e-03 -7.3519943e-04  8.1525063e-03  7.6904297e-03\n",
      " -7.2061159e-03 -3.6668312e-03  3.1185520e-03 -9.5707225e-03\n",
      "  1.4764392e-03  6.5244664e-03  5.7464195e-03 -8.7630618e-03\n",
      " -4.5171441e-03 -8.1401607e-03  4.5956374e-05  9.2636338e-03\n",
      "  5.9733056e-03  5.0673080e-03  5.0610625e-03 -3.2429171e-03\n",
      "  9.5521836e-03 -7.3564244e-03 -7.2703874e-03 -2.2653891e-03\n",
      " -7.7856064e-04 -3.2161034e-03 -5.9258583e-04  7.4888230e-03\n",
      " -6.9751858e-04 -1.6249407e-03  2.7443992e-03 -8.3591007e-03\n",
      "  7.8558037e-03  8.5361041e-03 -9.5840869e-03  2.4462664e-03\n",
      "  9.9049713e-03 -7.6658037e-03 -6.9669187e-03 -7.7365171e-03\n",
      "  8.3959233e-03 -6.8133592e-04  9.1444086e-03 -8.1582209e-03\n",
      "  3.7430846e-03  2.6350426e-03  7.4271322e-04  2.3276759e-03\n",
      " -7.4690939e-03 -9.3583735e-03  2.3545765e-03  6.1484552e-03\n",
      "  7.9856887e-03  5.7358947e-03 -7.7733636e-04  8.3061643e-03\n",
      " -9.3363142e-03  3.4061326e-03  2.6675343e-04  3.8572443e-03\n",
      "  7.3857834e-03 -6.7251669e-03  5.5844807e-03 -9.5222248e-03\n",
      " -8.0445886e-04 -8.6887367e-03 -5.0986730e-03  9.2892265e-03\n",
      " -1.8582619e-03  2.9144264e-03  9.0712793e-03  8.9381328e-03\n",
      " -8.2084350e-03 -3.0123137e-03  9.8866057e-03  5.1044310e-03\n",
      " -1.5880871e-03 -8.6920215e-03  2.9615164e-03 -6.6758976e-03]\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 🌍 4. GloVe using gensim (with pre-trained vectors)\n",
   "id": "a920cc11594a37ac"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load GloVe vec (convert .txt to .word2vec format beforehand if needed)\n",
    "glove_file = r'C:\\Users\\terry\\source\\llm\\glove\\glove.6B.100d.txt'\n",
    "model = KeyedVectors.load_word2vec_format( glove_file, unicode_errors='ignore' )\n",
    "\n",
    "# Vector for the word 'code'\n",
    "vector = model[ 'code' ]\n",
    "print( vector )\n"
   ],
   "id": "95c58a17aeefed99",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 🤖 5. BERT / Transformer-based Embeddings using transformers + torch\n",
   "id": "8117a17e891dc4c0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T22:58:22.666037Z",
     "start_time": "2025-05-14T22:58:21.482437Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer = BertTokenizer.from_pretrained( 'bert-base-uncased' )\n",
    "model = BertModel.from_pretrained( 'bert-base-uncased' )\n",
    "\n",
    "sentence = \"Bro's code always works.\"\n",
    "inputs = tokenizer( sentence, return_tensors='pt' )\n",
    "outputs = model( **inputs )\n",
    "\n",
    "# Get the vector for [CLS] token (sentence embedding)\n",
    "sentence_embedding = outputs.last_hidden_state[ :, 0, : ]\n",
    "print( sentence_embedding.shape )\n"
   ],
   "id": "8a9d362295bf0536",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 768])\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Clean Document",
   "id": "5e61d59191d25e5c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T22:59:11.827016Z",
     "start_time": "2025-05-14T22:59:11.820001Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def clean_text( text: str ) -> str:\n",
    "\ttext = text.replace( '\\r\\n', '\\n' ).replace( '\\r', '\\n' )\n",
    "\ttext = re.sub( r'\\n\\s*\\d+\\s*\\n', '\\n', text )\n",
    "\ttext = re.sub( r'(\\w+)-\\n(\\w+)', r'\\1\\2', text )\n",
    "\ttext = re.sub( r'(?<!\\n)\\n(?![\\n])', ' ', text )\n",
    "\ttext = re.sub( r'\\s+', ' ', text )\n",
    "\treturn text.strip( )\n"
   ],
   "id": "42c34418a1218398",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "fc9d08f36dde47ba"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "#  OpenAI Embedding\n",
    "___"
   ],
   "id": "9f097164e0f1b335"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### API key",
   "id": "10f8b0039c619d8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Create client\n",
    "client = OpenAI( )\n",
    "client.api_key = os.getenv( 'OPENAI_API_KEY' )"
   ],
   "id": "6f7c70b192899b4b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 1. Define embedding function",
   "id": "e1c76c3548b771af"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T14:30:51.638210Z",
     "start_time": "2025-05-14T14:30:51.622433Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def embed_texts( texts, model='text-embedding-3-small', batch_size=10, sleep=1 ):\n",
    "\tembeddings = [ ]\n",
    "\tfor i in range( 0, len( texts ), batch_size ):\n",
    "\t\tbatch = texts[ i:i + batch_size ]\n",
    "\t\ttry:\n",
    "\t\t\tresponse = openai.embeddings.create( input=batch, model=model )\n",
    "\t\t\tbatch_embeddings = [ e.embedding for e in response.data ]\n",
    "\t\t\tembeddings.extend( batch_embeddings )\n",
    "\t\texcept Exception as e:\n",
    "\t\t\tprint( f'Error at batch {i}: {e}' )\n",
    "\t\t\t# Retry or sleep to avoid rate limits\n",
    "\t\t\ttime.sleep( sleep )\n",
    "\t\t\tcontinue\n",
    "\n",
    "\treturn embeddings\n"
   ],
   "id": "3d19dce1b72312ca",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 2. Embed chunks",
   "id": "7a1cbc3787a81ed2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 2. Embed chunks\n",
    "embeddings = embed_texts( chunks )"
   ],
   "id": "2e8dd3b9c7453b0c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 3.  Create DataFrame",
   "id": "7b29c98e70fdc973"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 3. Create DataFrame\n",
    "df_embeddings = pd.DataFrame( { chunks, embeddings } )\n"
   ],
   "id": "998cc4a4eb1c122f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 3. Save\n",
   "id": "ca13ccc289f545b4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 3. Save\n",
    "df_embeddings.to_parquet( 'public_law_118_32_embeddings.parquet', index=False )\n"
   ],
   "id": "eac78df8b7b91593"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 4. Preview",
   "id": "ae06c79ed3506313"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 4. Preview\n",
    "df_embeddings.head( 2 )\n"
   ],
   "id": "e9935e9a89b6aac5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3. Generate Embeddings\n",
    "- Use a language model (e.g., OpenAI, HuggingFace) to create vector representations of each chunk_tokens."
   ],
   "id": "f62aab776049a508"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model = SentenceTransformer( 'all-MiniLM-L6-v2' )\n",
    "embeddings = model.encode( chunks, show_progress_bar=True )\n"
   ],
   "id": "a67fd85d25aafd1c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 4. Create SQLite Database",
   "id": "d76b8831817cd4d9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- Design a table that links text chunks to their embeddings.",
   "id": "f06349ccd853f770"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "conn = sqlite3.connect( 'vectors.values' )\n",
    "cursor = conn.cursor( )\n",
    "sql_create = '''\n",
    "CREATE TABLE IF NOT EXISTS Law_Embeddings\n",
    "(\n",
    "    Id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    Chunk_Tokens TEXT NOT NULL,\n",
    "    Embedding BLOB NOT NULL\n",
    ")\n",
    "'''\n",
    "\n",
    "cursor.execute( sql_create )\n",
    "\n",
    "for chunk, vector in zip( chunks, embeddings ):\n",
    "\tblob = pickle.dumps( vector )\n",
    "\tcursor.execute( 'INSERT INTO Law_Embeddings ( Chunk_Tokens, Embedding ) VALUES (?, ?)',\n",
    "\t\t(chunk, blob) )\n",
    "\n",
    "conn.commit( )\n",
    "conn.close( )\n"
   ],
   "id": "28ca2f108b874b20"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "###  Retrieval (Vector Search in SQLite)",
   "id": "ca5ad81f911f7250"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- You can perform semantic search by encoding a query and comparing via cosine similarity\n",
   "id": "12113e4d3566bd2f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def cosine_similarity( a, b ):\n",
    "\treturn np.dot( a, b ) / (np.linalg.norm( a ) * np.linalg.norm( b ))"
   ],
   "id": "70665fe56a643b3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "query = 'Appropriations for Department of Defense'\n",
    "query_vec = model.encode( [ query ] )[ 0 ]\n",
    "\n",
    "conn = sqlite3.connect( 'vectors.values' )\n",
    "cursor = conn.cursor( )\n",
    "cursor.execute( 'SELECT Id, Chunk_Tokens, Embedding FROM Law_Embeddings' )\n",
    "\n",
    "results = [ ]\n",
    "for row in cursor.fetchall( ):\n",
    "\tchunk_id, chunk_text, blob = row\n",
    "\tstored_vec = pickle.loads( blob )\n",
    "\tsim = cosine_similarity( query_vec, stored_vec )\n",
    "\tresults.append( (sim, chunk_text) )\n",
    "\n",
    "# Sort and get top N\n",
    "top_matches = sorted( results, key=lambda x: x[ 0 ], reverse=True )[ :5 ]\n"
   ],
   "id": "675d18dac5bd124d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#   Embedding-Pipeline Script\n",
    "___"
   ],
   "id": "3a7f735556cd0d3f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Load Dependencies\n",
    "\n"
   ],
   "id": "e593513dcb937f5f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T12:08:03.310828Z",
     "start_time": "2025-05-14T12:08:03.304763Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "import sqlite3\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import openai\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n"
   ],
   "id": "cf12b8bc356c9181",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Configuration",
   "id": "99ef339bdfea1fd8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "a764c56d86acecd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Define paths\n",
    "TEXT_FILE = 'PublicLaw_118-42.txt'\n",
    "DB_FILE = 'law_embeddings.values'\n",
    "EMBEDDING_MODEL = 'all-MiniLM-L6-v2'\n",
    "CHUNK_SIZE = 1000\n",
    "CHUNK_OVERLAP = 200\n"
   ],
   "id": "e8ee553da46c407c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Load and Clean Raw Text",
   "id": "842a900a1b80cde5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def load_and_clean_text( filepath ):\n",
    "\twith open( filepath, 'r', encoding='utf-8', errors='ignore' ) as file:\n",
    "\t\traw_text = file.read( )\n",
    "\n",
    "\t# Basic normalization\n",
    "\ttext = re.sub( r'\\f+', ' ', raw_text )\n",
    "\ttext = re.sub( r'\\n+', ' ', text )\n",
    "\ttext = re.sub( r'\\s{2,}', ' ', text )\n",
    "\treturn text.strip( )\n"
   ],
   "id": "c202cc689865f32"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Generate Embeddings",
   "id": "ae3ddba5e85404c6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def get_embedding( text, model=OPENAI_MODEL ):\n",
    "\tresponse = openai.Embedding.generate_text( input=text, model=model )\n",
    "\treturn response[ 'values'[ 0 ][ 'embedding' ] ]\n",
    "\n",
    "\n",
    "def embed_chunks( chunks ):\n",
    "\tembeddings = [ ]\n",
    "\tfor chunk in tqdm( chunks, desc='EmbeddingRequest chunks via OpenAI' ):\n",
    "\t\ttry:\n",
    "\t\t\tembedding = get_embedding( chunk )\n",
    "\t\t\tembeddings.append( embedding )\n",
    "\t\texcept Exception as e:\n",
    "\t\t\tprint( f'Error embedding chunk_tokens: {e}' )\n",
    "\t\t\tembeddings.append( [ 0.0 ] * 1536 )  # Placeholder for failed requests\n",
    "\treturn embeddings\n"
   ],
   "id": "8fa7587e7a3094be"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Create SQLite DB\n",
    "\n"
   ],
   "id": "3c985e6d3ecd5e8f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def create_and_populate_db( chunks, embeddings, db_path ):\n",
    "\tconn = sqlite3.connect( db_path )\n",
    "\tcursor = conn.cursor( )\n",
    "\tsql_create = '''\n",
    "    CREATE TABLE IF NOT EXISTS Law_Embeddings\n",
    "    (\n",
    "        Id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "        Chunk_Tokens TEXT NOT NULL,\n",
    "        Embedding BLOB NOT NULL\n",
    "    )\n",
    "    '''\n",
    "\n",
    "\tcursor.execute( sql_create )\n",
    "\tfor chunk, vector in zip( chunks, embeddings ):\n",
    "\t\tblob = pickle.dumps( vector )\n",
    "\t\tsql_insert = 'INSERT INTO Law_Embeddings ( Chunk_Tokens, Embedding ) VALUES ( ?, ? )'\n",
    "\t\tcursor.execute( sql_insert, (chunk, blob) )\n",
    "\n",
    "\tconn.commit( )\n",
    "\tconn.close( )\n"
   ],
   "id": "ea15943e543e0d70"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Script",
   "id": "e82751929d4fd563"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# === MAIN ===\n",
    "def main( ):\n",
    "\tprint( 'Step 1: Load and clean documents' )\n",
    "\tcleaned_text = load_and_clean_text( TEXT_FILE )\n",
    "\n",
    "\tprint( 'Step 2: Chunking documents' )\n",
    "\tchunks = chunk_text( cleaned_text )\n",
    "\tprint( f'Total chunks: {len( chunks )}' )\n",
    "\n",
    "\tprint( 'Step 3: EmbeddingRequest with OpenAI API' )\n",
    "\tembeddings = embed_chunks( chunks )\n",
    "\n",
    "\tprint( 'Step 4: Saving to SQLite' )\n",
    "\tcreate_and_populate_db( chunks, embeddings, DB_FILE )\n",
    "\n",
    "\tprint( f'Pipeline complete. Embeddings stored in: {DB_FILE}' )\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\tmain( )"
   ],
   "id": "5f2d269aaa88212b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Embeddings",
   "id": "de305bd9c7ccbd96"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# === 1. Load Model ===\n",
    "# You can try other models like 'all-MiniLM-L6-v2', 'all-mpnet-base-v2', or 'multi-qa-MiniLM-L6-cos-v1'\n",
    "model = SentenceTransformer( 'all-MiniLM-L6-v2' )\n",
    "\n",
    "\n",
    "# === 2. Embed Chunks ===\n",
    "def embed_with_sentence_transformers( texts, model ):\n",
    "\treturn model.encode( texts, show_progress_bar=True, convert_to_numpy=True )\n",
    "\n",
    "\n",
    "local_embeddings = embed_with_sentence_transformers( chunks, model )\n",
    "\n",
    "# === 3. Save in a DataFrame ===\n",
    "df_local = pd.DataFrame(\n",
    "{\n",
    "\t'chunk_tokens': chunks,\n",
    "\t'embedding': list( local_embeddings )  # numpy arrays to a list for DataFrame compatibility\n",
    "} )\n",
    "\n",
    "# === 4. Save to Disk ===\n",
    "df_local.to_parquet( 'public_law_118_32_local_embeddings.parquet', index=False )\n",
    "\n",
    "# === 5. Preview ===\n",
    "df_local.head( 2 )\n",
    "\n"
   ],
   "id": "47df028a12155604",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Fine-Tuning",
   "id": "e1efd28f65f33d39"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import json\n",
    "import openai\n",
    "import os\n",
    "import pandas as pd\n",
    "from pprint import pprint\n"
   ],
   "id": "51f033fc98145091"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "client = openai.OpenAI(\n",
    "\tapi_key=os.environ.get( 'OPENAI_API_KEY' ),\n",
    "\torganization='<org id>',\n",
    "\tproject='<project id>',\n",
    ")"
   ],
   "id": "b11d00974dbaa480"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T23:14:01.451194Z",
     "start_time": "2025-05-14T23:14:01.270150Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Read in the dataset we'll use for this task.\n",
    "# This will be the RecipesNLG dataset, which we've cleaned_lines to only contain documents from www.cookbooks.com\n",
    "recipe_df = pd.read_csv( r'C:\\Users\\terry\\Desktop\\cookbook_recipes_nlg_10k.csv' )\n",
    "recipe_df.head( )"
   ],
   "id": "aa84f0c255158f4f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                   title                                        ingredients  \\\n",
       "0    No-Bake Nut Cookies  [\"1 c. firmly packed brown sugar\", \"1/2 c. eva...   \n",
       "1  Jewell Ball'S Chicken  [\"1 small jar chipped beef, cut up\", \"4 boned ...   \n",
       "2            Creamy Corn  [\"2 (16 oz.) pkg. frozen corn\", \"1 (8 oz.) pkg...   \n",
       "3          Chicken Funny  [\"1 large whole chicken\", \"2 (10 1/2 oz.) cans...   \n",
       "4   Reeses Cups(Candy)    [\"1 c. peanut butter\", \"3/4 c. graham cracker ...   \n",
       "\n",
       "                                          directions  \\\n",
       "0  [\"In a heavy 2-quart saucepan, mix brown sugar...   \n",
       "1  [\"Place chipped beef on bottom of baking dish....   \n",
       "2  [\"In a slow cooker, combine all ingredients. C...   \n",
       "3  [\"Boil and debone chicken.\", \"Put bite size pi...   \n",
       "4  [\"Combine first four ingredients and press in ...   \n",
       "\n",
       "                                              link             source  \\\n",
       "0   www.cookbooks.com/Recipe-Details.aspx?id=44874  www.cookbooks.com   \n",
       "1  www.cookbooks.com/Recipe-Details.aspx?id=699419  www.cookbooks.com   \n",
       "2   www.cookbooks.com/Recipe-Details.aspx?id=10570  www.cookbooks.com   \n",
       "3  www.cookbooks.com/Recipe-Details.aspx?id=897570  www.cookbooks.com   \n",
       "4  www.cookbooks.com/Recipe-Details.aspx?id=659239  www.cookbooks.com   \n",
       "\n",
       "                                                 NER  \n",
       "0  [\"brown sugar\", \"milk\", \"vanilla\", \"nuts\", \"bu...  \n",
       "1  [\"beef\", \"chicken breasts\", \"cream of mushroom...  \n",
       "2  [\"frozen corn\", \"cream cheese\", \"butter\", \"gar...  \n",
       "3  [\"chicken\", \"chicken gravy\", \"cream of mushroo...  \n",
       "4  [\"peanut butter\", \"graham cracker crumbs\", \"bu...  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>ingredients</th>\n",
       "      <th>directions</th>\n",
       "      <th>link</th>\n",
       "      <th>source</th>\n",
       "      <th>NER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>No-Bake Nut Cookies</td>\n",
       "      <td>[\"1 c. firmly packed brown sugar\", \"1/2 c. eva...</td>\n",
       "      <td>[\"In a heavy 2-quart saucepan, mix brown sugar...</td>\n",
       "      <td>www.cookbooks.com/Recipe-Details.aspx?id=44874</td>\n",
       "      <td>www.cookbooks.com</td>\n",
       "      <td>[\"brown sugar\", \"milk\", \"vanilla\", \"nuts\", \"bu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jewell Ball'S Chicken</td>\n",
       "      <td>[\"1 small jar chipped beef, cut up\", \"4 boned ...</td>\n",
       "      <td>[\"Place chipped beef on bottom of baking dish....</td>\n",
       "      <td>www.cookbooks.com/Recipe-Details.aspx?id=699419</td>\n",
       "      <td>www.cookbooks.com</td>\n",
       "      <td>[\"beef\", \"chicken breasts\", \"cream of mushroom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Creamy Corn</td>\n",
       "      <td>[\"2 (16 oz.) pkg. frozen corn\", \"1 (8 oz.) pkg...</td>\n",
       "      <td>[\"In a slow cooker, combine all ingredients. C...</td>\n",
       "      <td>www.cookbooks.com/Recipe-Details.aspx?id=10570</td>\n",
       "      <td>www.cookbooks.com</td>\n",
       "      <td>[\"frozen corn\", \"cream cheese\", \"butter\", \"gar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Chicken Funny</td>\n",
       "      <td>[\"1 large whole chicken\", \"2 (10 1/2 oz.) cans...</td>\n",
       "      <td>[\"Boil and debone chicken.\", \"Put bite size pi...</td>\n",
       "      <td>www.cookbooks.com/Recipe-Details.aspx?id=897570</td>\n",
       "      <td>www.cookbooks.com</td>\n",
       "      <td>[\"chicken\", \"chicken gravy\", \"cream of mushroo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Reeses Cups(Candy)</td>\n",
       "      <td>[\"1 c. peanut butter\", \"3/4 c. graham cracker ...</td>\n",
       "      <td>[\"Combine first four ingredients and press in ...</td>\n",
       "      <td>www.cookbooks.com/Recipe-Details.aspx?id=659239</td>\n",
       "      <td>www.cookbooks.com</td>\n",
       "      <td>[\"peanut butter\", \"graham cracker crumbs\", \"bu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "system_message = 'You are a helpful recipe assistant. You are to extract the generic ingredients from each of the recipes provided.'\n",
    "\n",
    "def create_user_message( row ):\n",
    "\treturn f'Title: {row[ 'title' ]}\\n\\nIngredients: {row[ 'ingredients' ]}\\n\\nGeneric ingredients: '\n",
    "\n",
    "\n",
    "def prepare_example_conversation( row ):\n",
    "\treturn \\\n",
    "\t{\n",
    "\t\t'messages': [\n",
    "\t\t{\n",
    "\t\t\t'role': 'system',\n",
    "\t\t\t'content': system_message\n",
    "\t\t},\n",
    "\t\t{\n",
    "\t\t\t'role': 'user',\n",
    "\t\t\t'content': create_user_message( row )\n",
    "\t\t},\n",
    "\t\t{\n",
    "\t\t\t'role': 'assistant',\n",
    "\t\t\t'content': row[ 'NER' ]\n",
    "\t\t}, ]\n",
    "\t}\n",
    "\n"
   ],
   "id": "d29e9579f6f0e9dd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# use the first 100 rows of the dataset for training\n",
    "training_df = recipe_df.loc[ 0:100 ]\n",
    "\n",
    "# apply the prepare_example_conversation function to each row of the training_df\n",
    "training_data = training_df.apply( prepare_example_conversation, axis=1 ).tolist( )\n",
    "\n",
    "for example in training_data[ :5 ]:\n",
    "\tprint( example )"
   ],
   "id": "23bdc89006166778"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "validation_df = recipe_df.loc[ 101:200 ]\n",
    "validation_data = validation_df.apply(\n",
    "\tprepare_example_conversation, axis=1 ).tolist( )"
   ],
   "id": "4773228f51c07750"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "3896c43cec6862e4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def write_jsonl( data: List[ Dict ], filename: str ) -> None:\n",
    "\twith open( filename, 'w' ) as out:\n",
    "\t\tfor kvp in data:\n",
    "\t\t\tjout = json.dumps( kvp ) + '\\n'\n",
    "\t\t\tout.write( jout )"
   ],
   "id": "6dcf445d39eee404"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "652e2134d8966009"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "training_file_name = 'tmp_recipe_finetune_training.jsonl'\n",
    "write_jsonl( training_data, training_file_name )\n",
    "\n",
    "validation_file_name = 'tmp_recipe_finetune_validation.jsonl'\n",
    "write_jsonl( validation_data, validation_file_name )"
   ],
   "id": "a492a14231147287"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "170bad89408062ff"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def upload_file( file_name: str, purpose: str ) -> str:\n",
    "\twith open( file_name, 'rb' ) as file_fd:\n",
    "\t\tresponse = client.files.create( file=file_fd, purpose=purpose )\n",
    "\treturn response.id"
   ],
   "id": "10f941082775954c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "751aceb620d9c7bb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "training_file_id = upload_file( training_file_name, 'fine-tune' )\n",
    "validation_file_id = upload_file( validation_file_name, 'fine-tune' )"
   ],
   "id": "c8dbd2395ef59d03"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "MODEL = 'openai-4o-mini-2024-07-18'\n",
    "\n",
    "response = client.fine_tuning.jobs.create(\n",
    "\ttraining_file=training_file_id,\n",
    "\tvalidation_file=validation_file_id,\n",
    "\tmodel=MODEL,\n",
    "\tsuffix='recipe-ner',\n",
    ")\n",
    "\n",
    "job_id = response.id"
   ],
   "id": "5ca162663d00cad3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Text Cleaning",
   "id": "dcabaa53b3a578c0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T13:20:47.926324Z",
     "start_time": "2025-05-16T13:20:47.901991Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tigrr as tr\n",
    "from tigrr import *\n",
    "reload( tr )"
   ],
   "id": "1dd53ce7908a5042",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'textwrap' from 'C:\\\\Users\\\\terry\\\\source\\\\compilers\\\\py\\\\anaconda\\\\Lib\\\\textwrap.py'>"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 360
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Preprocessing",
   "id": "f8d915c3f2d4b28f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T13:14:21.978950Z",
     "start_time": "2025-05-16T13:14:21.974059Z"
    }
   },
   "cell_type": "code",
   "source": [
    "source = r'C:\\Users\\terry\\Desktop\\Budget\\Appropriations\\Cleaned'\n",
    "destination = r'C:\\Users\\terry\\Desktop\\Budget\\Appropriations\\Jsonl'"
   ],
   "id": "a59ab34bb40814",
   "outputs": [],
   "execution_count": 352
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T20:11:08.239263Z",
     "start_time": "2025-05-14T20:11:08.227221Z"
    }
   },
   "cell_type": "code",
   "source": [
    "docs = [ ]\n",
    "files = os.listdir( source )\n",
    "for i in files:\n",
    "\tdocs = [ source + f'\\\\{i}' for i in files ]"
   ],
   "id": "1195a17b56c1132a",
   "outputs": [],
   "execution_count": 205
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T14:31:30.342263Z",
     "start_time": "2025-05-14T14:31:30.332790Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for d in docs:\n",
    "\tprint( d )"
   ],
   "id": "af2bd8b593c87d8b",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T20:10:36.888281Z",
     "start_time": "2025-05-15T20:10:34.568591Z"
    }
   },
   "cell_type": "code",
   "source": [
    "file = r'C:\\Users\\terry\\Desktop\\Text\\Cleaned\\Senate Report 109-293.txt'\n",
    "cleaned_lines = [ ]\n",
    "filename = os.path.basename( file )\n",
    "text = open( file, 'rt' ).read( )\n",
    "stops = remove_stopwords( text )\n",
    "chunks = chunk_text( stops )\n",
    "tokens = tokenize( chunks )\n",
    "lines = chunk_tokens( tokens )"
   ],
   "id": "bc7745c00f12cd65",
   "outputs": [],
   "execution_count": 319
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for i in range( 10 ):\n",
    "\tprint( lines[ i ] )"
   ],
   "id": "471b42f7fd2f8492",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T20:40:56.796230Z",
     "start_time": "2025-05-15T20:40:56.769797Z"
    }
   },
   "cell_type": "code",
   "source": [
    "new = r'C:\\Users\\terry\\Desktop\\Text\\Chunked'  + '\\\\' + filename\n",
    "folder = open( new, 'wt+' )\n",
    "processed = [ ]\n",
    "for i, c in enumerate( lines ):\n",
    "\tpart = ' '.join( c )\n",
    "\tline = '{ ' + f'\"{i}\"' + ' : ' + '\"' + part + '\"' + ' },' + '\\r'\n",
    "\tprocessed.append( line )\n",
    "\n",
    "for line in processed:\n",
    "\tfolder.write( line )\n",
    "\n",
    "folder.close( )"
   ],
   "id": "feb3b69467ab7c9c",
   "outputs": [],
   "execution_count": 331
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T13:21:41.764964Z",
     "start_time": "2025-05-16T13:21:41.663029Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tx = Text( )\n",
    "tx.convert_jsonl( source, destination )"
   ],
   "id": "7e63e87a30d13ac0",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'remove_stopwords' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[364]\u001B[39m\u001B[32m, line 2\u001B[39m\n\u001B[32m      1\u001B[39m tx = Text( )\n\u001B[32m----> \u001B[39m\u001B[32m2\u001B[39m \u001B[43mtx\u001B[49m\u001B[43m.\u001B[49m\u001B[43mconvert_jsonl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m \u001B[49m\u001B[43msource\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdestination\u001B[49m\u001B[43m \u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\source\\repos\\Boo\\src\\tigrr.py:263\u001B[39m, in \u001B[36mText.convert_jsonl\u001B[39m\u001B[34m(self, src, dest)\u001B[39m\n\u001B[32m    261\u001B[39m source_path = source + \u001B[33m'\u001B[39m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[33m'\u001B[39m + basename\n\u001B[32m    262\u001B[39m text = \u001B[38;5;28mopen\u001B[39m( source_path, \u001B[33m'\u001B[39m\u001B[33mrt\u001B[39m\u001B[33m'\u001B[39m, encoding=\u001B[33m'\u001B[39m\u001B[33mutf-8\u001B[39m\u001B[33m'\u001B[39m, errors=\u001B[33m'\u001B[39m\u001B[33mignore\u001B[39m\u001B[33m'\u001B[39m ).read( )\n\u001B[32m--> \u001B[39m\u001B[32m263\u001B[39m stops = \u001B[43mremove_stopwords\u001B[49m( text )\n\u001B[32m    264\u001B[39m chunks = chunk_text( stops )\n\u001B[32m    265\u001B[39m tokens = tokenize( chunks )\n",
      "\u001B[31mNameError\u001B[39m: name 'remove_stopwords' is not defined"
     ]
    }
   ],
   "execution_count": 364
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
