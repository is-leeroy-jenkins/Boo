{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# NLTK\n",
    "___\n"
   ],
   "id": "700c6019c3802579"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 📦 One-Time Setup (NLTK Resources)",
   "id": "17b64a5a35525229"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# NLTK needs to download items onces\n",
    "import nltk\n",
    "nltk.download( 'punkt' )\n",
    "nltk.download( 'stopwords' )\n",
    "nltk.download( 'wordnet' )\n",
    "nltk.download( 'omw-1.4' )"
   ],
   "id": "ea993d341972b4d9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Load Dependencies",
   "id": "3957678e92123d8f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T14:49:57.644682Z",
     "start_time": "2025-04-04T14:49:57.620171Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "import re\n",
    "import os\n",
    "from pathlib import Path\n",
    "import openai\n",
    "import pandas as pd\n",
    "import time\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import sentence_transformers\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "import pickle\n",
    "import unicodedata\n",
    "from bs4 import BeautifulSoup\n",
    "from textblob import Word\n",
    "from collections import Counter\n"
   ],
   "id": "9a5c2c52dd74d494",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Text Processing",
   "id": "ee23478183ced671"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### ✅ Checklist\n",
    "\n",
    "| Step | Task                     | Function/Library               |\n",
    "|------|--------------------------|--------------------------------|\n",
    "| 1    | Load Text                | `open()`, `pandas.read_csv()` |\n",
    "| 2    | Convert to Lowercase     | `.lower()`                     |\n",
    "| 3    | Remove Punctuation       | `string.punctuation`           |\n",
    "| 4    | Remove Numbers           | `re.sub()`                     |\n",
    "| 5    | Trim Whitespaces         | `' '.join()`                   |\n",
    "| 6    | Tokenization             | `nltk.word_tokenize()`         |\n",
    "| 7    | Remove Stopwords         | `nltk.corpus.stopwords`        |\n",
    "| 8    | Lemmatization/Stemming   | `WordNetLemmatizer`, `PorterStemmer` |\n",
    "| 9    | Reconstruct Clean Text   | `' '.join()`                   |\n",
    "| 10   | (Optional) Spellcheck    | `TextBlob.correct()`           |\n",
    "| 11   | Vectorization for ML     | `TfidfVectorizer`, `CountVectorizer`, Word Embeddings |\n",
    "\n",
    "\n",
    "___"
   ],
   "id": "947b8eb1368fe0c4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1.  Load File",
   "id": "36fbc578426999c8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# === Load Raw Text ===\n",
    "file_path = '<url to file>'\n",
    "_rawtext = ''\n",
    "\n",
    "\n",
    "def load_text( file_path ):\n",
    "\twith open( file_path, 'r', encoding='utf-8' ) as f:\n",
    "\t\t_rawtext = f.read( )\n",
    "\t\treturn _rawtext\n"
   ],
   "id": "e3d5cc42b6dd4e3b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2.  Clean Whitespace\n",
    "- Consecutive whitespace reduced to a single space\n",
    "- Leading/trailing spaces removed\n",
    "- Blank lines removed"
   ],
   "id": "ea7db4bf069f99e1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def clean_whitespace( text: str ) -> str:\n",
    "    \"\"\"\n",
    "\n",
    "        Removes extra spaces and blank lines from the input text.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        text : str\n",
    "            The raw input text string to be cleaned.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        str\n",
    "            A cleaned text string with:\n",
    "                - Consecutive whitespace reduced to a single space\n",
    "                - Leading/trailing spaces removed\n",
    "                - Blank lines removed\n",
    "\n",
    "    \"\"\"\n",
    "    # Replace multiple spaces or tabs with a single space\n",
    "    text = re.sub(r'[ \\t]+', ' ', text)\n",
    "\n",
    "    # Remove leading/trailing spaces from each line\n",
    "    lines = [line.strip() for line in text.splitlines()]\n",
    "\n",
    "    # Remove empty lines\n",
    "    cleaned_lines = [line for line in lines if line]\n",
    "\n",
    "    # Join lines back into a single string\n",
    "    cleaned_text = '\\n'.join(cleaned_lines)\n",
    "\n",
    "    return cleaned_text"
   ],
   "id": "14c5765f6bf2af38"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3. Normalize",
   "id": "63bd713fbba95faa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def normalize( text: str ) -> str:\n",
    "    \"\"\"\n",
    "\n",
    "        Normalizes the input text string.\n",
    "\n",
    "        This function:\n",
    "          - Converts text to lowercase\n",
    "          - Removes accented characters (e.g., é -> e)\n",
    "          - Removes leading/trailing spaces\n",
    "          - Collapses multiple whitespace characters into a single space\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        text : str\n",
    "            The raw input text string to be normalized.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        str\n",
    "            A normalized, cleaned version of the input string.\n",
    "\n",
    "    \"\"\"\n",
    "    # Convert to lowercase\n",
    "    _lower = text.lower( )\n",
    "\n",
    "    # Remove accented characters using Unicode normalization\n",
    "    _unicode = unicodedata.normalize( 'NFKD', _lower ).encode( 'ascii', 'ignore' ).decode( 'utf-8' )\n",
    "\n",
    "    # Trim leading/trailing spaces and collapse_whitespace internal whitespace\n",
    "    _normalized = re.sub( r'\\s+', ' ', _unicode ).strip( )\n",
    "\n",
    "    return _normalized"
   ],
   "id": "82515d328d466a74"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 4. Remove Punctuation",
   "id": "5e0ee21b82c3c43"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def remove_punctuation( text: str ) -> str:\n",
    "    \"\"\"\n",
    "\n",
    "        Removes all punctuation characters from the input text string.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        text : str\n",
    "            The input text string to be cleaned.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        str\n",
    "            The text string with all punctuation removed.\n",
    "\n",
    "    \"\"\"\n",
    "    # Create a translation table that maps punctuation to None\n",
    "    translator = str.maketrans( '', '', string.punctuation )\n",
    "\n",
    "    # Apply the translation to the text\n",
    "    cleaned_text = text.translate( translator )\n",
    "\n",
    "    return cleaned_text"
   ],
   "id": "3ec4704593ac2991"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 5. Trim Whitespace\n",
    "\n",
    "- Removes leading and trailing whitespace\n",
    "- Replaces multiple internal spaces with a single space"
   ],
   "id": "5234418835bd42fa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def trim_whitespace( text: str ) -> str:\n",
    "    \"\"\"\n",
    "\n",
    "        Trims whitespace from the input text string.\n",
    "\n",
    "        This function:\n",
    "          - Removes leading and trailing whitespace\n",
    "          - Replaces multiple internal spaces with a single space\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        text : str\n",
    "            The raw input string with potential extra whitespace.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        str\n",
    "            The cleaned string with trimmed and normalized whitespace.\n",
    "\n",
    "    \"\"\"\n",
    "    # Strip leading and trailing whitespace\n",
    "    text = text.strip()\n",
    "\n",
    "    # Replace multiple whitespace characters (spaces, tabs, etc.) with a single space\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "    return cleaned_text"
   ],
   "id": "37ade6f5b1d34922"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 6. Lemmatize\n",
    "- Reduces words to their base or root form.\n",
    "- Converts text to lowercase\n",
    "- Tokenizes the text into words\n",
    "- Lemmatizes each token using WordNetLemmatizer\n",
    "- Reconstructs the lemmatized tokens into a single string"
   ],
   "id": "5f4db8f82f926ae"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def lemmatize(text: str) -> str:\n",
    "    \"\"\"\n",
    "\n",
    "        Performs lemmatization on the input text string.\n",
    "\n",
    "        This function:\n",
    "          - Converts text to lowercase\n",
    "          - Tokenizes the text into words\n",
    "          - Lemmatizes each token using WordNetLemmatizer\n",
    "          - Reconstructs the lemmatized tokens into a single string\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        text : str\n",
    "            The input text string to be lemmatized.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        str\n",
    "            A string with all words lemmatized.\n",
    "\n",
    "    \"\"\"\n",
    "    # Initialize lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    lower_case = text.lower( )\n",
    "    # Convert to lowercase and tokenize\n",
    "    tokens = word_tokenize( lower_case )\n",
    "\n",
    "    # Lemmatize each token\n",
    "    lemmatized_tokens = [ lemmatizer.lemmatize( token ) for token in tokens ]\n",
    "\n",
    "    # Join tokens back to a string\n",
    "    lemmatized_text = ' '.join( lemmatized_tokens )\n",
    "\n",
    "    return lemmatized_text"
   ],
   "id": "93dcb509d45e16f5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 7. Tokenize\n",
    "\n",
    "- Converts text to lowercase\n",
    "- Uses NLTK's word_tokenize to split the text into words and punctuation tokens"
   ],
   "id": "76b7a3f02f7d210"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def tokenize( text: str ) -> list:\n",
    "    \"\"\"\n",
    "\n",
    "        Tokenizes the input text string into individual word tokens.\n",
    "\n",
    "        This function:\n",
    "          - Converts text to lowercase\n",
    "          - Uses NLTK's word_tokenize to split the text into words and punctuation tokens\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        text : str\n",
    "            The raw input text string to be tokenized.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        list\n",
    "            A list of tokens (words and punctuation) extracted from the text.\n",
    "\n",
    "    \"\"\"\n",
    "    # Convert to lowercase\n",
    "    _lower = text.lower( )\n",
    "\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize( _lower )\n",
    "\n",
    "    return tokens\n"
   ],
   "id": "3e56143f4b97ab1f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 8. Remove Special Characters\n",
    "- Retains only alphanumeric characters and whitespace\n",
    "- Removes symbols like @, #, $, %, &, etc.\n",
    "- Preserves letters, numbers, and spaces"
   ],
   "id": "1ffdd326d8a48815"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def remove_special( text: str ) -> str:\n",
    "    \"\"\"\n",
    "\n",
    "        Removes special characters from the input text string.\n",
    "\n",
    "        This function:\n",
    "          - Retains only alphanumeric characters and whitespace\n",
    "          - Removes symbols like @, #, $, %, &, etc.\n",
    "          - Preserves letters, numbers, and spaces\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        text : str\n",
    "            The raw input text string potentially containing special characters.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        str\n",
    "            A cleaned string containing only letters, numbers, and spaces.\n",
    "\n",
    "    \"\"\"\n",
    "    # Use regex to replace all non-alphanumeric characters (excluding spaces) with empty string\n",
    "    cleaned_text = re.sub(r'[^A-Za-z0-9\\s]', '', text)\n",
    "\n",
    "    return cleaned_text"
   ],
   "id": "d915bd40a8960596"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 9. Remove HTML\n",
    "- Parses the text as HTML\n",
    "- Extracts and returns only the visible content without tags"
   ],
   "id": "3e5583a42894f537"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def remove_html_tags( text: str ) -> str:\n",
    "    \"\"\"\n",
    "\n",
    "        Removes HTML tags from the input text string.\n",
    "\n",
    "        This function:\n",
    "          - Parses the text as HTML\n",
    "          - Extracts and returns only the visible content without tags\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        text : str\n",
    "            The input text containing HTML tags.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        str\n",
    "            A cleaned string with all HTML tags removed.\n",
    "\n",
    "    \"\"\"\n",
    "    # Parse HTML and extract text\n",
    "    soup = BeautifulSoup(text, \"raw_html.parser\")\n",
    "    cleaned_text = soup.get_text(separator=' ', strip=True)\n",
    "\n",
    "    return cleaned_text"
   ],
   "id": "d5e9b16f93ccfed8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 10. Chunk Tokens\n",
    "- Tokenizes the text into words\n",
    "- Groups them into consecutive word chunks\n",
    "- Returns a list of strings (each chunk)"
   ],
   "id": "b9e77cd71f1df62e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def chunk_tokens( text: list, chunk_size: int = 50) -> list:\n",
    "    \"\"\"\n",
    "\n",
    "        Breaks a list of cleaned, tokenized strings into chunks of a specified number of tokens.\n",
    "\n",
    "        This function:\n",
    "          - Flattens the input list of tokenized strings (i.e., list of lists)\n",
    "          - Groups tokens into chunks of length `chunk_size`\n",
    "          - Returns a list of token chunks, each as a list of tokens\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        text : list of tokenizd words\n",
    "            The input list where each element is a list of tokens (words).\n",
    "\n",
    "        chunk_size : int, optional (default=50)\n",
    "            Number of tokens per chunk.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        list\n",
    "            A list of token chunks. Each chunk is a list of tokens.\n",
    "\n",
    "    \"\"\"\n",
    "    # Flatten the list of token lists into a single list\n",
    "    all_tokens = [ token for sublist in text for token in sublist ]\n",
    "\n",
    "    # Create chunks of tokens\n",
    "    chunks = [\n",
    "        all_tokens[i:i + chunk_size]\n",
    "        for i in range(0, len(all_tokens), chunk_size)\n",
    "    ]\n",
    "\n",
    "    return chunks"
   ],
   "id": "a6edbb0eb8fa5720"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 11. Chunk Text\n",
    "\n",
    "- Converts text to lowercase\n",
    "- Tokenizes text using NLTK's word_tokenize\n",
    "- Breaks tokens into chunks of a specified size\n",
    "- Optionally joins tokens into strings (for transformer models)"
   ],
   "id": "e93a4c6067321b6d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def chunk_text( text: str, chunk_size: int = 50, return_as_string: bool = True ) -> list:\n",
    "    \"\"\"\n",
    "\n",
    "        Tokenizes cleaned text and breaks it into chunks for downstream embeddings.\n",
    "\n",
    "        This function:\n",
    "          - Converts text to lowercase\n",
    "          - Tokenizes text using NLTK's word_tokenize\n",
    "          - Breaks tokens into chunks of a specified size\n",
    "          - Optionally joins tokens into strings (for transformer models)\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        text : str\n",
    "            The cleaned input text to be tokenized and chunked.\n",
    "\n",
    "        chunk_size : int, optional (default=50)\n",
    "            Number of tokens per chunk.\n",
    "\n",
    "        return_string : bool, optional (default=True)\n",
    "            If True, returns each chunk as a string; otherwise, returns a list of tokens.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        list\n",
    "            A list of token chunks. Each chunk is either a list of tokens or a string.\n",
    "\n",
    "    \"\"\"\n",
    "    # Download tokenizer models (only once)\n",
    "    nltk.download('punkt', quiet=True)\n",
    "\n",
    "    # Tokenize the text into words\n",
    "    tokens = word_tokenize( text.lower( ) )\n",
    "\n",
    "    # Create chunks of specified token length\n",
    "    token_chunks = [\n",
    "        tokens[i:i + chunk_size]\n",
    "        for i in range(0, len(tokens), chunk_size)\n",
    "    ]\n",
    "\n",
    "    # Optionally join tokens into strings\n",
    "    if return_as_string:\n",
    "        return [' '.join(chunk) for chunk in token_chunks]\n",
    "    else:\n",
    "        return token_chunks"
   ],
   "id": "f6b1ee045b318fd5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 12. Remove Errors\n",
    "- Converts text to lowercase\n",
    "- Tokenizes the text into words\n",
    "- Filters out words not recognized as valid English using TextBlob\n",
    "- Returns a string with only correctly spelled words"
   ],
   "id": "b1e7c36fc4666c45"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def remove_errors( text: str ) -> str:\n",
    "    \"\"\"\n",
    "\n",
    "        Removes misspelled or non-English words from the input text.\n",
    "\n",
    "        This function:\n",
    "          - Converts text to lowercase\n",
    "          - Tokenizes the text into words\n",
    "          - Filters out words not recognized as valid English using TextBlob\n",
    "          - Returns a string with only correctly spelled words\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        text : str\n",
    "            The input text to clean.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        str\n",
    "            A cleaned string containing only valid English words.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Download NLTK resources (only once)\n",
    "    nltk.download('punkt', quiet=True)\n",
    "\n",
    "    # Convert to lowercase and tokenize\n",
    "    tokens = word_tokenize(text.lower())\n",
    "\n",
    "    # Keep only correctly spelled words (as per Word dictionary in TextBlob)\n",
    "    cleaned_tokens = [word for word in tokens if Word(word).spellcheck()[0][1] > 0.9]\n",
    "\n",
    "    # Return cleaned string\n",
    "    return ' '.join(cleaned_tokens)"
   ],
   "id": "818357851dd8bf54"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 13. Correct Errors\n",
    "- Converts text to lowercase\n",
    "- Tokenizes the text into words\n",
    "- Applies spelling correction using TextBlob\n",
    "- Reconstructs and returns the corrected text"
   ],
   "id": "5c6cdef587829c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def correct_errors(text: str) -> str:\n",
    "    \"\"\"\n",
    "\n",
    "        Corrects misspelled words in the input text string.\n",
    "\n",
    "        This function:\n",
    "          - Converts text to lowercase\n",
    "          - Tokenizes the text into words\n",
    "          - Applies spelling correction using TextBlob\n",
    "          - Reconstructs and returns the corrected text\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        text : str\n",
    "            The input text string with potential spelling mistakes.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        str\n",
    "            A corrected version of the input string with proper English words.\n",
    "\n",
    "    \"\"\"\n",
    "    # Download tokenizer values (only once)\n",
    "    nltk.download('punkt', quiet=True)\n",
    "\n",
    "    # Convert to lowercase and tokenize\n",
    "    tokens = word_tokenize(text.lower())\n",
    "\n",
    "    # Apply spelling correction to each token\n",
    "    corrected_tokens = [str(Word(word).correct()) for word in tokens]\n",
    "\n",
    "    # Join the corrected words into a single string\n",
    "    corrected_text = ' '.join(corrected_tokens)\n",
    "\n",
    "    return corrected_text"
   ],
   "id": "c7d8dfb08dfbf717"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 14.  Remove Headers\n",
    "- Assumes repeated lines at the top or bottom (like titles, page numbers)\n",
    "- Removes lines that are common across multiple pages (heuristic)\n",
    "- Returns cleaned text with main body content only"
   ],
   "id": "d8489c8d8d9d0af4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def remove_headers_footers(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Removes common headers and footers from a text document.\n",
    "\n",
    "    This function:\n",
    "      - Assumes repeated lines at the top or bottom (like titles, page numbers)\n",
    "      - Removes lines that are common across multiple pages (heuristic)\n",
    "      - Returns cleaned text with main body content only\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    text : str\n",
    "        The input text potentially containing headers/footers.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    str\n",
    "        The cleaned text with headers and footers removed.\n",
    "    \"\"\"\n",
    "    # Split the text into lines\n",
    "    lines = text.splitlines()\n",
    "\n",
    "    # Remove empty lines and trim whitespace\n",
    "    lines = [line.strip() for line in lines if line.strip()]\n",
    "\n",
    "    # Count line frequencies to identify repeated headers/footers\n",
    "    line_counts = Counter(lines)\n",
    "\n",
    "    # Identify frequent lines (appear in >1% of total lines)\n",
    "    threshold = max(1, int(len(lines) * 0.01))\n",
    "    repeated_lines = {line for line, count in line_counts.items() if count > threshold}\n",
    "\n",
    "    # Remove lines that are likely headers or footers\n",
    "    body_lines = [line for line in lines if line not in repeated_lines]\n",
    "\n",
    "    # Reconstruct the cleaned text\n",
    "    cleaned_text = '\\n'.join(body_lines)\n",
    "\n",
    "    return cleaned_text\n"
   ],
   "id": "f9c1ba26ea66c445"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 15.  Remove Formatting\n",
    "- Strips HTML tags\n",
    "- Removes Markdown syntax (e.g., *, #, [], etc.)\n",
    "- Collapses whitespace (newlines, tabs)\n",
    "- Optionally removes special characters for clean unformatted text"
   ],
   "id": "76f3936ac0e24e17"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def remove_formatting( text: str ) -> str:\n",
    "    \"\"\"\n",
    "\n",
    "        Removes formatting artifacts (Markdown, HTML, control characters) from text.\n",
    "\n",
    "        This function:\n",
    "          - Strips HTML tags\n",
    "          - Removes Markdown syntax (e.g., *, #, [], etc.)\n",
    "          - Collapses whitespace (newlines, tabs)\n",
    "          - Optionally removes special characters for clean unformatted text\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        text : str\n",
    "            The formatted input text.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        str\n",
    "            A cleaned version of the text with formatting removed.\n",
    "\n",
    "    \"\"\"\n",
    "    # Remove HTML tags\n",
    "    text = BeautifulSoup(text, \"raw_html.parser\").get_text(separator=' ', strip=True)\n",
    "\n",
    "    # Remove Markdown syntax\n",
    "    text = re.sub(r'\\[.*?\\]\\(.*?\\)', '', text)           # Markdown links\n",
    "    text = re.sub(r'[`_*#~>-]', '', text)                # Markdown chars\n",
    "    text = re.sub(r'!\\[.*?\\]\\(.*?\\)', '', text)          # Markdown images\n",
    "\n",
    "    # Remove control characters and normalize whitespace\n",
    "    text = re.sub(r'[\\r\\n\\t]+', ' ', text)               # Newlines, tabs\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()             # Collapse multiple spaces\n",
    "\n",
    "    return text"
   ],
   "id": "be4039da12904496"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 16. Remove Stopwords\n",
    "- Tokenizes the input text\n",
    "- Removes common stopwords (e.g., \"the\", \"is\", \"and\", etc.)\n",
    "- Returns the text with only meaningful words"
   ],
   "id": "1c75687f6c08c2c5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def remove_stopwords(text: str) -> str:\n",
    "    \"\"\"\n",
    "\n",
    "        Removes English stopwords from the input text string.\n",
    "\n",
    "        This function:\n",
    "          - Tokenizes the input text\n",
    "          - Removes common stopwords (e.g., \"the\", \"is\", \"and\", etc.)\n",
    "          - Returns the text with only meaningful words\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        text : str\n",
    "            The input text string.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        str\n",
    "            A cleaned version of the input text without stopwords.\n",
    "\n",
    "    \"\"\"\n",
    "    # Download required NLTK resources (only once)\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "\n",
    "    # Define English stopword set\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    # Tokenize and lowercase\n",
    "    tokens = word_tokenize(text.lower())\n",
    "\n",
    "    # Remove stopwords\n",
    "    filtered_tokens = [word for word in tokens if word.isalnum() and word not in stop_words]\n",
    "\n",
    "    # Join tokens back into a string\n",
    "    cleaned_text = ' '.join(filtered_tokens)\n",
    "\n",
    "    return cleaned_text\n"
   ],
   "id": "2e51da785742262a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 🔍 Usage",
   "id": "f4c860851a941aeb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def clean_text( text: str ) -> str:\n",
    "    # Step 1: Normalize normalized\n",
    "    text = text.replace('\\r\\n', '\\n').replace('\\r', '\\n')\n",
    "\n",
    "    # Step 2: Remove page headers and footers (Public Law-specific)\n",
    "    text = re.sub(r'PUBLIC LAW 118–32.*?\\n', '', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\n\\s*\\d+\\s*\\n', '\\n', text)  # Remove page numbers between lines\n",
    "\n",
    "    # Step 3: Remove hyphenation at line breaks (e.g., 'appropria-\\ntion')\n",
    "    text = re.sub(r'(\\w+)-\\n(\\w+)', r'\\1\\2', text)\n",
    "\n",
    "    # Step 4: Merge broken lines where sentence continues\n",
    "    text = re.sub(r'(?<!\\n)\\n(?![\\n])', ' ', text)\n",
    "\n",
    "    # Step 5: Collapse excessive whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "    return text.strip()"
   ],
   "id": "6ffc1b387c68d803"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 🔍 Pipeline",
   "id": "f9435a341c128b29"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# === Preprocessing Configuration ===\n",
    "EN_STOPWORDS = set( stopwords.words( 'english' ) )\n",
    "LEMMATIZER = WordNetLemmatizer( )\n",
    "STEMMER = SnowballStemmer( 'english' )\n",
    "\n",
    "def preprocess_line( line, lower=True, punctuation=True,\n",
    "                     stopwords=False, lemmatize=True, stem=False ):\n",
    "    '''\n",
    "    Process a single line of documents with optional steps:\n",
    "    - lower\n",
    "    - punctuation removal\n",
    "    - stopword removal\n",
    "    - lemmatization\n",
    "    - stemming (optional)\n",
    "    Returns the cleaned line as a string.\n",
    "    '''\n",
    "    tokens = word_tokenize( line )\n",
    "    processed = [ ]\n",
    "    for token in tokens:\n",
    "        if lower:\n",
    "            token = token.lower( )\n",
    "\n",
    "        if punctuation and token in string.punctuation:\n",
    "            continue\n",
    "\n",
    "        if stopwords and token in EN_STOPWORDS:\n",
    "            continue\n",
    "\n",
    "        if lemmatize:\n",
    "            token = LEMMATIZER.lemmatize( token )\n",
    "\n",
    "        if stem:\n",
    "            token = STEMMER.stem( token )\n",
    "\n",
    "        processed.append( token )\n",
    "\n",
    "    return ' '.join( processed )\n",
    "\n",
    "\n",
    "def process_file( file_path, **preprocess_kwargs ):\n",
    "    '''\n",
    "        Read a documents file line-by-line, apply preprocessing pipeline to each line,\n",
    "        and return a list of cleaned lines (original order preserved).\n",
    "        Pass any keyword args supported by `preprocess_line`.\n",
    "    '''\n",
    "    cleaned_lines = []\n",
    "    with open( file_path, 'r', encoding='utf-8' ) as file:\n",
    "        for line in file:\n",
    "            cleaned = preprocess_line( line, **preprocess_kwargs )\n",
    "            cleaned_lines.append( cleaned )\n",
    "    return cleaned_lines\n"
   ],
   "id": "af7f3fd7b2990387"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "91981ee2b140d645"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "c7c6e615a319e113"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "\n",
   "id": "d901568b8ef44067"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "c6aca1d3fff8a8ac"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a0478bcf04eb5543"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#  Vectorization (Preparation for ML Models)\n",
    "___\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "ba0f01ba423430f4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Load Dependencies",
   "id": "d3e02f4dbedcc22e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch"
   ],
   "id": "6ea810a20c6020f2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# === Load Raw Text ===\n",
    "file_path = '<url to file>'\n",
    "_rawtext = ''\n",
    "\n",
    "def load_text( file_path ):\n",
    "\twith open( file_path, 'r', encoding='utf-8' ) as f:\n",
    "\t\t_rawtext = f.read( )\n",
    "\t\treturn _rawtext"
   ],
   "id": "f930499beb91e45f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "4cd82d1886e5243"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 🧮 1. Bag of Words (BoW) using CountVectorizer",
   "id": "93554b0db878266a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "\n",
    "corpus = ['Bro loves clean code.', 'Code is life.']\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "print( vectorizer.get_feature_names_out( ) )\n",
    "print( X.toarray() )\n"
   ],
   "id": "df2a01ee327d57d6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 📊 2. TF-IDF using TfidfVectorizer",
   "id": "606ca62c6f083d04"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "\n",
    "corpus = [ 'Bro writes awesome code.', 'Code must be clean and clear.' ]\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print(X.toarray())\n"
   ],
   "id": "58a4cf4ec025e261"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 🧠 3. Word2Vec using gensim",
   "id": "cd392a973450c93a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "\n",
    "sentences = [ [ 'bro', 'loves', 'python'], ['clean', 'code', 'rocks' ] ]\n",
    "model = Word2Vec( sentences, vector_size=100, window=5, min_count=1, workers=4 )\n",
    "\n",
    "# Vector for the word 'bro'\n",
    "vector = model.wv[ 'bro' ]\n",
    "print(vector)\n"
   ],
   "id": "a20f65018fd9c119"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 🌍 4. GloVe using gensim (with pre-trained vectors)\n",
   "id": "a920cc11594a37ac"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "\n",
    "# Load GloVe vectors (convert .txt to .word2vec format beforehand if needed)\n",
    "glove_file = 'glove.6B.100d.word2vec.txt'\n",
    "model = KeyedVectors.load_word2vec_format(glove_file, binary=False)\n",
    "\n",
    "# Vector for the word 'code'\n",
    "vector = model['code']\n",
    "print(vector)\n"
   ],
   "id": "95c58a17aeefed99"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 🤖 5. BERT / Transformer-based Embeddings using transformers + torch\n",
   "id": "8117a17e891dc4c0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "sentence = \"Bro's code always works.\"\n",
    "inputs = tokenizer(sentence, return_tensors='pt')\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# Get the vector for [CLS] token (sentence embedding)\n",
    "sentence_embedding = outputs.last_hidden_state[:, 0, :]\n",
    "print(sentence_embedding.shape)\n"
   ],
   "id": "8a9d362295bf0536"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Clean Document",
   "id": "5e61d59191d25e5c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def clean_text( text: str ) -> str:\n",
    "    # Step 1: Normalize normalized\n",
    "    text = text.replace('\\r\\n', '\\n').replace('\\r', '\\n')\n",
    "\n",
    "    # Step 2: Remove page headers and footers (Public Law-specific)\n",
    "    text = re.sub(r'PUBLIC LAW 118–32.*?\\n', '', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\n\\s*\\d+\\s*\\n', '\\n', text)  # Remove page numbers between lines\n",
    "\n",
    "    # Step 3: Remove hyphenation at line breaks (e.g., 'appropria-\\ntion')\n",
    "    text = re.sub(r'(\\w+)-\\n(\\w+)', r'\\1\\2', text)\n",
    "\n",
    "    # Step 4: Merge broken lines where sentence continues\n",
    "    text = re.sub(r'(?<!\\n)\\n(?![\\n])', ' ', text)\n",
    "\n",
    "    # Step 5: Collapse excessive whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "    return text.strip()\n"
   ],
   "id": "42c34418a1218398"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 3. Chunk File",
   "id": "9bf93b3631a66b6a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Simple chunking by words assuming ~1.3 words per token\n",
    "def chunk_text( text, max_tokens=512 ):\n",
    "\twords = text.split( )\n",
    "\tchunk_size = int( max_tokens * 1.3 )\n",
    "\tchunks = [ ' '.join( words[ i:i + chunk_size ] ) for i in range( 0, len( words ), chunk_size ) ]\n",
    "\treturn chunks\n"
   ],
   "id": "bdf2dd9223fede76"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "fc9d08f36dde47ba"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "#  OpenAI Embedding\n",
    "___"
   ],
   "id": "9f097164e0f1b335"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### API key",
   "id": "10f8b0039c619d8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create client\n",
    "client = OpenAI( )\n",
    "client.api_key = os.getenv( 'OPENAI_API_KEY' )"
   ],
   "id": "6f7c70b192899b4b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 1. Define embedding function",
   "id": "e1c76c3548b771af"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def embed_texts( texts, model='documents-embedding-3-small', batch_size=10, sleep=1 ):\n",
    "    embeddings = []\n",
    "    for i in range( 0, len( texts ), batch_size ):\n",
    "        batch = texts[ i:i+batch_size ]\n",
    "        try:\n",
    "            response = openai.embeddings.create( input=batch, model=model )\n",
    "            batch_embeddings = [ e.embedding for e in response.data ]\n",
    "            embeddings.extend( batch_embeddings )\n",
    "        except Exception as e:\n",
    "            print( f'Error at batch {i}: {e}' )\n",
    "            # Retry or sleep to avoid rate limits\n",
    "            time.sleep( sleep )\n",
    "            continue\n",
    "\n",
    "    return embeddings\n"
   ],
   "id": "3d19dce1b72312ca"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 2. Embed chunks",
   "id": "7a1cbc3787a81ed2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 2. Embed chunks\n",
    "embeddings = embed_texts( chunks )"
   ],
   "id": "2e8dd3b9c7453b0c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 3.  Create DataFrame",
   "id": "7b29c98e70fdc973"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 3. Create DataFrame\n",
    "df_embeddings = pd.DataFrame( { chunks, embeddings } )\n"
   ],
   "id": "998cc4a4eb1c122f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 3. Save\n",
   "id": "ca13ccc289f545b4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 3. Save\n",
    "df_embeddings.to_parquet( 'public_law_118_32_embeddings.parquet', index=False )\n"
   ],
   "id": "eac78df8b7b91593"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 4. Preview",
   "id": "ae06c79ed3506313"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 4. Preview\n",
    "df_embeddings.head(2)\n"
   ],
   "id": "e9935e9a89b6aac5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3. Generate Embeddings\n",
    "- Use a language model (e.g., OpenAI, HuggingFace) to create vector representations of each chunk."
   ],
   "id": "f62aab776049a508"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "\n",
    "model = SentenceTransformer( 'all-MiniLM-L6-v2' )\n",
    "embeddings = model.encode( chunks, show_progress_bar=True )\n"
   ],
   "id": "a67fd85d25aafd1c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 4. Create SQLite Database",
   "id": "d76b8831817cd4d9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- Design a table that links text chunks to their embeddings.",
   "id": "f06349ccd853f770"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "conn = sqlite3.connect( 'embeddings.values' )\n",
    "cursor = conn.cursor( )\n",
    "sql_create = '''\n",
    "CREATE TABLE IF NOT EXISTS law_embeddings\n",
    "(\n",
    "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    chunk TEXT NOT NULL,\n",
    "    embedding BLOB NOT NULL\n",
    ")\n",
    "'''\n",
    "\n",
    "cursor.execute( sql_create )\n",
    "\n",
    "for chunk, vector in zip( chunks, embeddings ):\n",
    "    blob = pickle.dumps( vector )\n",
    "    cursor.execute( 'INSERT INTO law_embeddings ( chunk, embedding ) VALUES (?, ?)', ( chunk, blob ) )\n",
    "\n",
    "conn.commit( )\n",
    "conn.close( )\n"
   ],
   "id": "28ca2f108b874b20"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "###  Retrieval (Vector Search in SQLite)",
   "id": "ca5ad81f911f7250"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- You can perform semantic search by encoding a query and comparing via cosine similarity\n",
   "id": "12113e4d3566bd2f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def cosine_similarity( a, b ):\n",
    "    return np.dot( a, b ) / ( np.linalg.norm( a ) * np.linalg.norm( b ) )"
   ],
   "id": "70665fe56a643b3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "query = 'Appropriations for Department of Defense'\n",
    "query_vec = model.encode( [ query ] )[ 0 ]\n",
    "\n",
    "conn = sqlite3.connect( 'embeddings.values' )\n",
    "cursor = conn.cursor( )\n",
    "cursor.execute( 'SELECT id, chunk, embedding FROM law_embeddings' )\n",
    "\n",
    "results = []\n",
    "for row in cursor.fetchall( ):\n",
    "    chunk_id, chunk_text, blob = row\n",
    "    stored_vec = pickle.loads( blob )\n",
    "    sim = cosine_similarity( query_vec, stored_vec )\n",
    "    results.append( ( sim, chunk_text ) )\n",
    "\n",
    "# Sort and get top N\n",
    "top_matches = sorted( results, key=lambda x: x[ 0 ], reverse=True )[ :5 ]\n"
   ],
   "id": "675d18dac5bd124d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#   Embedding-Pipeline Script\n",
    "___"
   ],
   "id": "3a7f735556cd0d3f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Load Dependencies\n",
    "\n"
   ],
   "id": "e593513dcb937f5f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-31T20:28:22.663413Z",
     "start_time": "2025-03-31T20:28:22.654167Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "import sqlite3\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import openai\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n"
   ],
   "id": "cf12b8bc356c9181",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Configuration",
   "id": "99ef339bdfea1fd8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "a764c56d86acecd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Define paths\n",
    "TEXT_FILE = 'PublicLaw_118-42.txt'\n",
    "DB_FILE = 'law_embeddings.values'\n",
    "EMBEDDING_MODEL = 'all-MiniLM-L6-v2'\n",
    "CHUNK_SIZE = 1000\n",
    "CHUNK_OVERLAP = 200\n"
   ],
   "id": "e8ee553da46c407c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Load and Clean Raw Text",
   "id": "842a900a1b80cde5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def load_and_clean_text( filepath ):\n",
    "    with open( filepath, 'r', encoding='utf-8' ) as file:\n",
    "        raw_text = file.read( )\n",
    "\n",
    "    # Basic normalization\n",
    "    text = re.sub( r'\\f+', ' ', raw_text )\n",
    "    text = re.sub( r'\\n+', ' ', text )\n",
    "    text = re.sub( r'\\s{2,}', ' ', text )\n",
    "    return text.strip( )\n"
   ],
   "id": "c202cc689865f32"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Generate Embeddings",
   "id": "ae3ddba5e85404c6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def get_embedding( text, model=OPENAI_MODEL ):\n",
    "    response = openai.Embedding.generate_text( input=text, model=model )\n",
    "    return response[ 'values' [ 0 ][ 'embedding' ] ]\n",
    "\n",
    "\n",
    "def embed_chunks( chunks ):\n",
    "    embeddings = [ ]\n",
    "    for chunk in tqdm( chunks, desc='EmbeddingRequest chunks via OpenAI' ):\n",
    "        try:\n",
    "            embedding = get_embedding( chunk )\n",
    "            embeddings.append( embedding )\n",
    "        except Exception as e:\n",
    "            print( f'Error embedding chunk: {e}' )\n",
    "            embeddings.append( [ 0.0 ] * 1536 )  # Placeholder for failed requests\n",
    "    return embeddings\n"
   ],
   "id": "8fa7587e7a3094be"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Create SQLite DB\n",
    "\n"
   ],
   "id": "3c985e6d3ecd5e8f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def create_and_populate_db( chunks, embeddings, db_path ):\n",
    "    conn = sqlite3.connect( db_path )\n",
    "    cursor = conn.cursor( )\n",
    "    sql_create = '''\n",
    "    CREATE TABLE IF NOT EXISTS law_embeddings\n",
    "    (\n",
    "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "        chunk TEXT NOT NULL,\n",
    "        embedding BLOB NOT NULL\n",
    "    )\n",
    "    '''\n",
    "\n",
    "    cursor.execute( sql_create )\n",
    "    for chunk, vector in zip( chunks, embeddings ):\n",
    "        blob = pickle.dumps( vector )\n",
    "        sql_insert = 'INSERT INTO law_embeddings ( chunk, embedding ) VALUES ( ?, ? )'\n",
    "        cursor.execute( sql_insert, ( chunk, blob ) )\n",
    "\n",
    "    conn.commit( )\n",
    "    conn.close( )\n"
   ],
   "id": "ea15943e543e0d70"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Script",
   "id": "e82751929d4fd563"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# === MAIN ===\n",
    "def main():\n",
    "    print('Step 1: Load and clean documents')\n",
    "    cleaned_text = load_and_clean_text(TEXT_FILE)\n",
    "\n",
    "    print('Step 2: Chunking documents')\n",
    "    chunks = chunk_text(cleaned_text)\n",
    "    print(f'Total chunks: {len(chunks)}')\n",
    "\n",
    "    print('Step 3: EmbeddingRequest with OpenAI API')\n",
    "    embeddings = embed_chunks(chunks)\n",
    "\n",
    "    print('Step 4: Saving to SQLite')\n",
    "    create_and_populate_db(chunks, embeddings, DB_FILE)\n",
    "\n",
    "    print(f'Pipeline complete. Embeddings stored in: {DB_FILE}')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ],
   "id": "5f2d269aaa88212b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "E",
   "id": "de305bd9c7ccbd96"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# === 1. Load Model ===\n",
    "# You can try other models like 'all-MiniLM-L6-v2', 'all-mpnet-base-v2', or 'multi-qa-MiniLM-L6-cos-v1'\n",
    "model = SentenceTransformer( 'all-MiniLM-L6-v2' )\n",
    "\n",
    "# === 2. Embed Chunks ===\n",
    "def embed_with_sentence_transformers( texts, model ):\n",
    "    return model.encode( texts, show_progress_bar=True, convert_to_numpy=True )\n",
    "\n",
    "local_embeddings = embed_with_sentence_transformers( chunks, model )\n",
    "\n",
    "# === 3. Save in a DataFrame ===\n",
    "df_local = pd.DataFrame({\n",
    "    'chunk': chunks,\n",
    "    'embedding': list( local_embeddings )  # numpy arrays to list for DataFrame compatibility\n",
    "})\n",
    "\n",
    "# === 4. Save to Disk ===\n",
    "df_local.to_parquet( 'public_law_118_32_local_embeddings.parquet', index=False )\n",
    "\n",
    "# === 5. Preview ===\n",
    "df_local.head( 2 )\n",
    "\n"
   ],
   "id": "47df028a12155604"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Fine-Tuning",
   "id": "e1efd28f65f33d39"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import json\n",
    "import openai\n",
    "import os\n",
    "import pandas as pd\n",
    "from pprint import pprint\n"
   ],
   "id": "51f033fc98145091"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "client = openai.OpenAI(\n",
    "    api_key=os.environ.get('OPENAI_API_KEY'),\n",
    "    organization='<org id>',\n",
    "    project='<project id>',\n",
    ")"
   ],
   "id": "b11d00974dbaa480"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Read in the dataset we'll use for this task.\n",
    "# This will be the RecipesNLG dataset, which we've cleaned to only contain documents from www.cookbooks.com\n",
    "recipe_df = pd.read_csv('data/cookbook_recipes_nlg_10k.csv')\n",
    "\n",
    "recipe_df.head()"
   ],
   "id": "aa84f0c255158f4f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "system_message = 'You are a helpful recipe assistant. You are to extract the generic ingredients from each of the recipes provided.'\n",
    "\n",
    "\n",
    "def create_user_message(row):\n",
    "    return f'Title: {row['title']}\\n\\nIngredients: {row['ingredients']}\\n\\nGeneric ingredients: '\n",
    "\n",
    "\n",
    "def prepare_example_conversation(row):\n",
    "    return {\n",
    "        'messages': [\n",
    "            {'role': 'system', 'content': system_message},\n",
    "            {'role': 'user', 'content': create_user_message(row)},\n",
    "            {'role': 'assistant', 'content': row['NER']},\n",
    "        ]\n",
    "    }\n",
    "\n"
   ],
   "id": "d29e9579f6f0e9dd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# use the first 100 rows of the dataset for training\n",
    "training_df = recipe_df.loc[0:100]\n",
    "\n",
    "# apply the prepare_example_conversation function to each row of the training_df\n",
    "training_data = training_df.apply(prepare_example_conversation, axis=1).tolist()\n",
    "\n",
    "for example in training_data[:5]:\n",
    "    print(example)"
   ],
   "id": "23bdc89006166778"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "validation_df = recipe_df.loc[101:200]\n",
    "validation_data = validation_df.apply(\n",
    "    prepare_example_conversation, axis=1).tolist()"
   ],
   "id": "4773228f51c07750"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def write_jsonl(data_list: list, filename: str) -> None:\n",
    "    with open(filename, 'w') as out:\n",
    "        for ddict in data_list:\n",
    "            jout = json.dumps(ddict) + '\\n'\n",
    "            out.write(jout)"
   ],
   "id": "6dcf445d39eee404"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "652e2134d8966009"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "training_file_name = 'tmp_recipe_finetune_training.jsonl'\n",
    "write_jsonl(training_data, training_file_name)\n",
    "\n",
    "validation_file_name = 'tmp_recipe_finetune_validation.jsonl'\n",
    "write_jsonl(validation_data, validation_file_name)"
   ],
   "id": "a492a14231147287"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "170bad89408062ff"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def upload_file(file_name: str, purpose: str) -> str:\n",
    "    with open(file_name, 'rb') as file_fd:\n",
    "        response = client.files.create(file=file_fd, purpose=purpose)\n",
    "    return response.id"
   ],
   "id": "10f941082775954c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "751aceb620d9c7bb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "training_file_id = upload_file(training_file_name, 'fine-tune')\n",
    "validation_file_id = upload_file(validation_file_name, 'fine-tune')"
   ],
   "id": "c8dbd2395ef59d03"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "feadf4f400f8454c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "MODEL = 'gpt-4o-mini-2024-07-18'\n",
    "\n",
    "response = client.fine_tuning.jobs.create(\n",
    "    training_file=training_file_id,\n",
    "    validation_file=validation_file_id,\n",
    "    model=MODEL,\n",
    "    suffix='recipe-ner',\n",
    ")\n",
    "\n",
    "job_id = response.id"
   ],
   "id": "5ca162663d00cad3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
