{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# NLTK\n",
    "___\n"
   ],
   "id": "700c6019c3802579"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### ðŸ“¦ One-Time Setup (NLTK Resources)",
   "id": "17b64a5a35525229"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import io\n",
    "import nltk\n",
    "from nltk.corpus import words\n",
    "from isort.format import remove_whitespace\n",
    "\n",
    "nltk.download( 'punkt' )\n",
    "nltk.download( 'punkt_tab' )\n",
    "nltk.download( 'stopwords' )\n",
    "nltk.download( 'wordnet' )\n",
    "nltk.download( 'omw-1.4' )\n",
    "nltk.download( 'words' )\n"
   ],
   "id": "ea993d341972b4d9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Load Dependencies",
   "id": "3957678e92123d8f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-10T14:51:19.345698Z",
     "start_time": "2025-06-10T14:51:19.298372Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "import re\n",
    "import os\n",
    "from pathlib import Path\n",
    "import openai as OpenAI\n",
    "import pandas as pd\n",
    "import time\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "import pickle\n",
    "import unicodedata\n",
    "from bs4 import BeautifulSoup\n",
    "from textblob import Word\n",
    "from collections import Counter\n",
    "from typing import Any, List, Tuple, Optional, Union, Dict\n",
    "import ipywidgets as widgets, IPython, platform, ipywidgets, jupyterlab\n",
    "from importlib import reload\n"
   ],
   "id": "9a5c2c52dd74d494",
   "outputs": [],
   "execution_count": 170
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-10T14:51:22.419464Z",
     "start_time": "2025-06-10T14:51:22.394396Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors"
   ],
   "id": "4c4a3094edae0ea",
   "outputs": [],
   "execution_count": 171
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Text Processing",
   "id": "ee23478183ced671"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### âœ… Checklist\n",
    "\n",
    "| Step | Task                     | Function/Library               |\n",
    "|------|--------------------------|--------------------------------|\n",
    "| 1    | Load Text                | `open()`, `pandas.read_csv()` |\n",
    "| 2    | Convert to Lowercase     | `.lower()`                     |\n",
    "| 3    | Remove Punctuation       | `string.punctuation`           |\n",
    "| 4    | Remove Numbers           | `re.sub()`                     |\n",
    "| 5    | Trim Whitespaces         | `' '.join()`                   |\n",
    "| 6    | Tokenization             | `nltk.word_tokenize()`         |\n",
    "| 7    | Remove Stopwords         | `nltk.corpus.stopwords`        |\n",
    "| 8    | Lemmatization/Stemming   | `WordNetLemmatizer`, `PorterStemmer` |\n",
    "| 9    | Reconstruct Clean Text   | `' '.join()`                   |\n",
    "| 10   | (Optional) Spellcheck    | `TextBlob.correct()`           |\n",
    "| 11   | Vectorization for ML     | `TfidfVectorizer`, `CountVectorizer`, Word Embeddings |\n",
    "\n",
    "\n",
    "___"
   ],
   "id": "947b8eb1368fe0c4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1.  Load File",
   "id": "36fbc578426999c8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-10T14:51:28.628431Z",
     "start_time": "2025-06-10T14:51:28.619434Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === Load Raw Text ===\n",
    "file_path = '<url to file>'\n",
    "_rawtext = ''\n",
    "\n",
    "\n",
    "def load_text( file_path ):\n",
    "\twith open( file_path, 'r', encoding='utf-8' ) as f:\n",
    "\t\t_rawtext = f.read( )\n",
    "\t\treturn _rawtext\n"
   ],
   "id": "e3d5cc42b6dd4e3b",
   "outputs": [],
   "execution_count": 172
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2.  Clean Space\n",
    "- Consecutive whitespace reduced to a single space\n",
    "- Leading/trailing spaces removed\n",
    "- Blank lines removed"
   ],
   "id": "ea7db4bf069f99e1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-10T18:21:16.917067Z",
     "start_time": "2025-06-10T18:21:16.909263Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def clean_space( text: str ) -> str:\n",
    "\t\"\"\"\n",
    "\n",
    "\t\tThis function:\n",
    "\t\t_____________\n",
    "        Removes extra spaces and blank tokens from the path pages.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        pages : str\n",
    "            The raw path pages path to be cleaned_lines.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        str\n",
    "            A cleaned_lines pages path with:\n",
    "                - Consecutive whitespace reduced to a single space\n",
    "                - Leading/trailing spaces removed\n",
    "                - Blank tokens removed\n",
    "\n",
    "    \"\"\"\n",
    "\t# Replace multiple spaces or tabs with a single space\n",
    "\ttext = re.sub( r'\\t+', ' ', text )\n",
    "\ttext = re.sub( r'\\s+', ' ', text )\n",
    "\treturn text"
   ],
   "id": "14c5765f6bf2af38",
   "outputs": [],
   "execution_count": 422
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3. Normalize",
   "id": "63bd713fbba95faa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-10T14:51:36.917877Z",
     "start_time": "2025-06-10T14:51:36.903495Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def normalize( text: str ) -> str:\n",
    "\t\"\"\"\n",
    "\n",
    "        This function: Normalizes the path pages path.\n",
    "          - Converts pages to lowercase\n",
    "          - Removes accented characters (e.g., Ã© -> e)\n",
    "          - Removes leading/trailing spaces\n",
    "          - Collapses multiple whitespace characters into a single space\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        pages : str\n",
    "            The raw path pages path to be normalized.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        str\n",
    "            A normalized, cleaned_lines version of the path path.\n",
    "\n",
    "    \"\"\"\n",
    "\treturn unicodedata.normalize( 'NFKD', text ).encode( 'ascii', 'ignore' ).decode( 'utf-8' )"
   ],
   "id": "82515d328d466a74",
   "outputs": [],
   "execution_count": 174
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T14:08:52.495241Z",
     "start_time": "2025-05-15T14:08:52.485063Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "6d18d3d035ef6a39",
   "outputs": [],
   "execution_count": 159
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 4. Remove Punctuation",
   "id": "5e0ee21b82c3c43"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-10T16:18:44.780674Z",
     "start_time": "2025-06-10T16:18:44.772827Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def remove_punctuation( text: str ) -> str:\n",
    "    \"\"\"\n",
    "\n",
    "\t\tRemove non-alphanumeric characters\n",
    "\t\tand extra whitespace from text.\n",
    "\n",
    "\t\tArgs:\n",
    "\t\t\ttext (str): The text text string.\n",
    "\n",
    "\t\tReturns:\n",
    "\t\t\tstr: Cleaned text string with only alphanumeric characters and spaces.\n",
    "\n",
    "    \"\"\"\n",
    "    text = re.sub( r'[^a-zA-Z0-9\\s]', ' ', text )\n",
    "    text = re.sub( r'\\s+', ' ', text ).strip( )\n",
    "    return text"
   ],
   "id": "3ec4704593ac2991",
   "outputs": [],
   "execution_count": 301
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 5. Trim Whitespace\n",
    "\n",
    "- Removes leading and trailing whitespace\n",
    "- Replaces multiple internal spaces with a single space"
   ],
   "id": "5234418835bd42fa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-10T18:09:59.775054Z",
     "start_time": "2025-06-10T18:09:59.768520Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def trim_whitespace( text: str ) -> str:\n",
    "\t\"\"\"\n",
    "\n",
    "\t\tPurpose:\n",
    "\t\t---------\n",
    "        This function:\n",
    "          - Removes leading and trailing whitespace\n",
    "          - Replaces multiple internal spaces with a single space\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        pages : str\n",
    "            The raw path path with potential extra whitespace.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        str\n",
    "            The cleaned_lines path with trimmed and normalized whitespace.\n",
    "\n",
    "    \"\"\"\n",
    "\t# Replace multiple whitespace characters (spaces, tabs, etc.) with a single space\n",
    "\tcleaned_text = re.sub( r'\\s+', ' ', text )\n",
    "\treturn cleaned_text"
   ],
   "id": "37ade6f5b1d34922",
   "outputs": [],
   "execution_count": 408
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 6. Lemmatize\n",
    "- Reduces words to their base or root form.\n",
    "- Converts text to lowercase\n",
    "- Tokenizes the text into words\n",
    "- Lemmatizes each token using WordNetLemmatizer\n",
    "- Reconstructs the lemmatized tokens into a single string"
   ],
   "id": "5f4db8f82f926ae"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-10T14:51:45.082786Z",
     "start_time": "2025-06-10T14:51:45.066010Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def lemmatize( text: str ) -> str:\n",
    "\t\"\"\"\n",
    "\n",
    "        Performs lemmatization on the path pages path.\n",
    "\n",
    "        This function:\n",
    "          - Converts pages to lowercase\n",
    "          - Tokenizes the pages into words\n",
    "          - Lemmatizes each token using WordNetLemmatizer\n",
    "          - Reconstructs the lemmatized tokens into a single path\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        pages : str\n",
    "            The path pages path to be lemmatized.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        str\n",
    "            A path with all words lemmatized.\n",
    "\n",
    "    \"\"\"\n",
    "\t# Initialize lemmatizer\n",
    "\tlemmatizer = WordNetLemmatizer( )\n",
    "\n",
    "\tlower_case = text.lower( )\n",
    "\t# Convert to lowercase and tokenize_text\n",
    "\ttokens = word_tokenize( lower_case )\n",
    "\n",
    "\t# Lemmatize each token\n",
    "\tlemmatized_tokens = [ lemmatizer.lemmatize( token ) for token in tokens ]\n",
    "\n",
    "\t# Join tokens back to a path\n",
    "\tlemmatized_text = ' '.join( lemmatized_tokens )\n",
    "\n",
    "\treturn lemmatized_text"
   ],
   "id": "93dcb509d45e16f5",
   "outputs": [],
   "execution_count": 177
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 7. Tokenize\n",
    "\n",
    "- Converts text to lowercase\n",
    "- Uses NLTK's word_tokenize to split the text into words and punctuation tokens"
   ],
   "id": "76b7a3f02f7d210"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-10T14:51:48.669989Z",
     "start_time": "2025-06-10T14:51:48.658024Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def tokenize( words: List[ str ] ) -> List[ str ]:\n",
    "\t\"\"\"\n",
    "\n",
    "        This function:\n",
    "          - Tokenizes the path pages path into individual word tokens.\n",
    "          - Converts pages to lowercase\n",
    "          - Uses NLTK's word_tokenize to split the pages into words and punctuation tokens\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        words : List[ str ]\n",
    "            A list of strings to be tokenized.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "            A list of token strings (words and punctuation) extracted from the pages.\n",
    "\n",
    "    \"\"\"\n",
    "\t# Convert to lowercase\n",
    "\ttokens = [ ]\n",
    "\tfor w in words:\n",
    "\t\t_token = nltk.word_tokenize( w )\n",
    "\t\ttokens.append( _token )\n",
    "\treturn tokens\n"
   ],
   "id": "3e56143f4b97ab1f",
   "outputs": [],
   "execution_count": 178
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 8. Remove Special\n",
    "- Retains only alphanumeric characters and whitespace\n",
    "- Removes symbols like @, #, $, %, &, etc.\n",
    "- Preserves letters, numbers, and spaces"
   ],
   "id": "1ffdd326d8a48815"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-10T18:41:10.387887Z",
     "start_time": "2025-06-10T18:41:10.377652Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def remove_special( text: str ) -> str:\n",
    "\t\"\"\"\n",
    "\n",
    "        Remove all special characters\n",
    "        from the text string, keeping only letters, digits, and whitespace.\n",
    "\n",
    "        Args:\n",
    "            text (str): The text string to process.\n",
    "\n",
    "        Returns:\n",
    "            str: The processed string with special characters removed.\n",
    "\n",
    "    \"\"\"\n",
    "\tcleaned = [ ]\n",
    "\tkeepers = [ '$', '. ', '; ', ': ' ]\n",
    "\tfor char in text:\n",
    "\t\tif char in keepers:\n",
    "\t\t\tcleaned.append( char )\n",
    "\t\telif char == '--':\n",
    "\t\t\tcleaned.append( ' ' )\n",
    "\t\telif char == '-':\n",
    "\t\t\tcleaned.append( ' ' )\n",
    "\t\telif char.isalnum( ) or char == ' ':\n",
    "\t\t\tcleaned.append( char )\n",
    "\n",
    "\treturn ''.join( cleaned )"
   ],
   "id": "d915bd40a8960596",
   "outputs": [],
   "execution_count": 441
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-10T17:16:48.647354Z",
     "start_time": "2025-06-10T17:16:48.638653Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def remove_numerals( chunks: List[ str ] ) -> str:\n",
    "\t\"\"\"\n",
    "\n",
    "        Remove all special characters\n",
    "        from the text string, keeping only letters, digits, and whitespace.\n",
    "\n",
    "        Args:\n",
    "            text (str): The text string to process.\n",
    "\n",
    "        Returns:\n",
    "            str: The processed string with special characters removed.\n",
    "\n",
    "    \"\"\"\n",
    "\tcleaned = [ ]\n",
    "\tkeepers = [  ' i',  'ii', 'iii', 'iv', ' v', 'vi', 'vii', 'viii', 'ix' ]\n",
    "\tfor text in chunks:\n",
    "\t\tif text.lower() in keepers:\n",
    "\t\t\tcleaned.append( ' ' )\n",
    "\t\telse:\n",
    "\t\t\tcleaned.append( text.lower( ) )\n",
    "\n",
    "\treturn ''.join( cleaned )"
   ],
   "id": "1b1f126c114550db",
   "outputs": [],
   "execution_count": 346
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-10T14:51:54.260036Z",
     "start_time": "2025-06-10T14:51:54.248100Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def remove_special_characters( text: str, keep_spaces: bool=True ) -> str:\n",
    "\t\"\"\"\n",
    "\n",
    "\t\tPurpose:\n",
    "\t\t_______\n",
    "\t\tRemove special characters from the text.\n",
    "\n",
    "\t\tArgs:\n",
    "\t\t\ttext (str): Input string to clean.\n",
    "\t\t\tkeep_spaces (bool): If True, preserves spaces; otherwise removes all non-alphanumerics.\n",
    "\n",
    "\t\tReturns:\n",
    "\t\t\tstr: Cleaned text with only alphanumeric characters (and optionally spaces).\n",
    "\t\"\"\"\n",
    "\tif keep_spaces:\n",
    "\t\treturn re.sub( r'[^a-zA-Z0-9\\s]', ' ', text )\n",
    "\telse:\n",
    "\t\treturn re.sub( r'[^a-zA-Z0-9]', ' ', text )"
   ],
   "id": "56565e0606ba12",
   "outputs": [],
   "execution_count": 180
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 9. Remove HTML\n",
    "- Parses the text as HTML\n",
    "- Extracts and returns only the visible content without tags"
   ],
   "id": "3e5583a42894f537"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-10T14:51:56.483614Z",
     "start_time": "2025-06-10T14:51:56.470179Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def remove_html_tags( text: str ) -> str:\n",
    "\t\"\"\"\n",
    "\n",
    "\n",
    "        This function:\n",
    "        Removes HTML tags from the path pages path.\n",
    "          - Parses the pages as HTML\n",
    "          - Extracts and returns only the visible content without tags\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        pages : str\n",
    "            The path pages containing HTML tags.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        str\n",
    "            A cleaned_lines path with all HTML tags removed.\n",
    "\n",
    "    \"\"\"\n",
    "\t# Parse HTML and extract pages\n",
    "\tsoup = BeautifulSoup( text, \"raw_html.parser\" )\n",
    "\tcleaned_text = soup.get_text( separator=' ', strip=True )\n",
    "\n",
    "\treturn cleaned_text"
   ],
   "id": "d5e9b16f93ccfed8",
   "outputs": [],
   "execution_count": 181
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 10. Chunk Words\n",
    "- Tokenizes the text into words\n",
    "- Groups them into consecutive word chunks\n",
    "- Returns a list of strings (each chunk_words)"
   ],
   "id": "b9e77cd71f1df62e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-10T14:51:59.596626Z",
     "start_time": "2025-06-10T14:51:59.582880Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def chunk_words( text: List[ str ], chunk_size: int=50 ) -> List[ List[ str ] ]:\n",
    "\t\"\"\"\n",
    "\n",
    "        Breaks a list of cleaned_lines,\n",
    "        tokenized strings into\n",
    "        chunks of a specified num of tokens.\n",
    "\n",
    "        This function:\n",
    "          - Flattens the path get_list of tokenized strings (i.e., get_list of lists)\n",
    "          - Groups tokens into chunks of min `chunk_size`\n",
    "          - Returns a list of lists of tokens\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        pages : get_list of tokenizd words\n",
    "            The path get_list where each element is a get_list of tokens (words).\n",
    "\n",
    "        chunk_size : int, optional (default=250)\n",
    "            Number of tokens per chunk_words.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        List[ List[ str ] ]\n",
    "            A list of a list of token chunks. Each chunk is a list of tokens.\n",
    "\n",
    "    \"\"\"\n",
    "\t# Flatten the get_list of token lists into a single list\n",
    "\tall_tokens = [ token for sublist in text for token in sublist ]\n",
    "\n",
    "\t# Create chunks of tokens\n",
    "\tchunks = [ all_tokens[ i : i + chunk_size ] for i in range( 0, len( all_tokens ), chunk_size ) ]\n",
    "\n",
    "\treturn chunks"
   ],
   "id": "a6edbb0eb8fa5720",
   "outputs": [],
   "execution_count": 182
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 10. Chunk Pages",
   "id": "c454ec12bc8bd6e3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-10T18:24:04.005482Z",
     "start_time": "2025-06-10T18:24:03.996255Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def chunk_pages( text: List[ str ], chunk_size: int=250 ) -> List[ str ]:\n",
    "\t\"\"\"\n",
    "\n",
    "        Breaks a list of cleaned_lines,\n",
    "        tokenized strings into\n",
    "        chunks of a specified num of tokens.\n",
    "\n",
    "        This function:\n",
    "          - Flattens the path get_list of tokenized strings (i.e., get_list of lists)\n",
    "          - Groups tokens into chunks of min `chunk_size`\n",
    "          - Returns a list of lists of tokens\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        pages : get_list of tokenizd words\n",
    "            The path get_list where each element is a get_list of tokens (words).\n",
    "\n",
    "        chunk_size : int, optional (default=250)\n",
    "            Number of tokens per chunk_words.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        List[ List[ str ] ]\n",
    "            A list of a list of token chunks. Each chunk is a list of tokens.\n",
    "\n",
    "    \"\"\"\n",
    "\t# Flatten the get_list of token lists into a single list\n",
    "\tall_tokens = [ token for sublist in text for token in sublist ]\n",
    "\n",
    "\t# Create chunks of tokens\n",
    "\ttoken_chunks = [ all_tokens[ i : i + chunk_size ] for i in range( 0, len( all_tokens ), chunk_size ) ]\n",
    "\n",
    "\treturn [ ''.join( chunk ) for chunk in token_chunks ]"
   ],
   "id": "449f84f48132bc08",
   "outputs": [],
   "execution_count": 424
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 11. Chunk Text\n",
    "\n",
    "- Converts text to lowercase\n",
    "- Tokenizes text using NLTK's word_tokenize\n",
    "- Breaks tokens into chunks of a specified size\n",
    "- Optionally joins tokens into strings (for transformer models)"
   ],
   "id": "e93a4c6067321b6d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-10T14:52:03.777346Z",
     "start_time": "2025-06-10T14:52:03.749988Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def chunk_text( text: str, chunk_size: int=50, return_as_string: bool=True ) -> List[ str ]:\n",
    "\t\"\"\"\n",
    "\n",
    "        This function:\n",
    "        Tokenizes cleaned_lines pages and breaks it into chunks for downstream vectors.\n",
    "          - Converts pages to lowercase\n",
    "          - Tokenizes pages using NLTK's word_tokenize\n",
    "          - Breaks tokens into chunks of a specified size\n",
    "          - Optionally joins tokens into strings (for transformer models)\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        pages : str\n",
    "            The cleaned_lines path pages to be tokenized and chunked.\n",
    "\n",
    "        chunk_size : int, optional (default=50)\n",
    "            Number of tokens per chunk_words.\n",
    "\n",
    "        return_string : bool, optional (default=True)\n",
    "            If True, returns each chunk_words as a path; otherwise, returns a get_list of tokens.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        get_list\n",
    "            A get_list of token chunks. Each chunk_words is either a get_list of tokens or a path.\n",
    "\n",
    "    \"\"\"\n",
    "\t# Download tokenizer models (only once)\n",
    "\ttokens = nltk.word_tokenize( text )\n",
    "\ttoken_chunks = [ tokens[ i:i + chunk_size ] for i in range( 0, len( tokens ), chunk_size ) ]\n",
    "\treturn [ ' '.join( chunk ) for chunk in token_chunks ]"
   ],
   "id": "f6b1ee045b318fd5",
   "outputs": [],
   "execution_count": 184
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 12. Remove Errors\n",
    "- Converts text to lowercase\n",
    "- Tokenizes the text into words\n",
    "- Filters out words not recognized as valid English using TextBlob\n",
    "- Returns a string with only correctly spelled words"
   ],
   "id": "b1e7c36fc4666c45"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-10T14:52:07.627722Z",
     "start_time": "2025-06-10T14:52:07.615700Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def remove_errors( text: str ) -> str:\n",
    "    \"\"\"\n",
    "\n",
    "\t\tPurpose:\n",
    "\t\t________\n",
    "\t\tRemove all non-English\n",
    "\t\twords but preserve valid numbers\n",
    "\t\tusing the NLTK English vocabulary.\n",
    "\n",
    "\t\tArgs:\n",
    "\t\t\ttext (str): The text text string.\n",
    "\n",
    "\t\tReturns:\n",
    "\t\t\tstr: The cleaned string with only English words and numbers.\n",
    "\n",
    "    \"\"\"\n",
    "    vocabulary = set( w.lower() for w in words.words() )\n",
    "    keepers = [ '(', ')', '$', '.', ';', ':', ' - '  ]\n",
    "\n",
    "    # Tokenize: includes words and numbers\n",
    "    tokens = re.findall(r'\\b[\\w.]+\\b', text.lower( ))\n",
    "\n",
    "    # Keep words in vocab or numbers\n",
    "    def is_valid_token( tok: str ) -> bool:\n",
    "        return ( tok in vocabulary\n",
    "                or tok.isdigit( )\n",
    "                or tok in keepers )\n",
    "\n",
    "    valid_tokens = [ tok for tok in tokens if is_valid_token( tok ) ]\n",
    "    return ' '.join( valid_tokens )"
   ],
   "id": "818357851dd8bf54",
   "outputs": [],
   "execution_count": 185
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 13. Correct Errors\n",
    "- Converts text to lowercase\n",
    "- Tokenizes the text into words\n",
    "- Applies spelling correction using TextBlob\n",
    "- Reconstructs and returns the corrected text"
   ],
   "id": "5c6cdef587829c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-10T14:52:11.571575Z",
     "start_time": "2025-06-10T14:52:11.561838Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def correct_errors( text: str ) -> str:\n",
    "\t\"\"\"\n",
    "\n",
    "        Corrects misspelled words in the path pages path.\n",
    "\n",
    "        This function:\n",
    "          - Converts pages to lowercase\n",
    "          - Tokenizes the pages into words\n",
    "          - Applies spelling correction using TextBlob\n",
    "          - Reconstructs and returns the corrected pages\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        pages : str\n",
    "            The path pages path with potential spelling mistakes.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        str\n",
    "            A corrected version of the path path with proper English words.\n",
    "\n",
    "    \"\"\"\n",
    "\t# Convert to lowercase and tokenize_text\n",
    "\ttokens = word_tokenize( text.lower( ) )\n",
    "\n",
    "\t# Apply spelling correction to each token\n",
    "\tcorrected_tokens = [ str( Word( word ).correct( ) ) for word in tokens ]\n",
    "\n",
    "\t# Join the corrected words into a single path\n",
    "\tcorrected_text = ' '.join( corrected_tokens )\n",
    "\n",
    "\treturn corrected_text"
   ],
   "id": "c7d8dfb08dfbf717",
   "outputs": [],
   "execution_count": 186
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 14.  Remove Headers\n",
    "- Assumes repeated lines at the top or bottom (like titles, page numbers)\n",
    "- Removes lines that are common across multiple pages (heuristic)\n",
    "- Returns cleaned text with main body content only"
   ],
   "id": "d8489c8d8d9d0af4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-10T14:52:15.302700Z",
     "start_time": "2025-06-10T14:52:15.288574Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def remove_headers_footers( text: str ) -> str:\n",
    "\t\"\"\"\n",
    "    Removes common headers and footers from a pages document.\n",
    "\n",
    "    This function:\n",
    "      - Assumes repeated tokens at the top or bottom (like titles, page numbers)\n",
    "      - Removes tokens that are common across multiple pages (heuristic)\n",
    "      - Returns cleaned_lines pages with main body content only\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    pages : str\n",
    "        The path pages potentially containing headers/footers.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    str\n",
    "        The cleaned_lines pages with headers and footers removed.\n",
    "    \"\"\"\n",
    "\t# Split the pages into tokens\n",
    "\tlines = text.splitlines( )\n",
    "\n",
    "\t# Remove empty tokens and trim whitespace\n",
    "\tlines = [ line.strip( ) for line in lines if line.strip( ) ]\n",
    "\n",
    "\t# Count line frequencies to identify repeated headers/footers\n",
    "\tline_counts = Counter( lines )\n",
    "\n",
    "\t# Identify frequent tokens (appear in >1% of total tokens)\n",
    "\tthreshold = max( 1, int( len( lines ) * 0.01 ) )\n",
    "\trepeated_lines = { line for line, count in line_counts.items( ) if count > threshold }\n",
    "\n",
    "\t# Remove tokens that are likely headers or footers\n",
    "\tbody_lines = [ line for line in lines if line not in repeated_lines ]\n",
    "\n",
    "\t# Reconstruct the cleaned_lines pages\n",
    "\tcleaned_text = '\\n'.join( body_lines )\n",
    "\n",
    "\treturn cleaned_text\n"
   ],
   "id": "f9c1ba26ea66c445",
   "outputs": [],
   "execution_count": 187
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 15.  Remove Formatting\n",
    "- Strips HTML tags\n",
    "- Removes Markdown syntax (e.g., *, #, [], etc.)\n",
    "- Collapses whitespace (newlines, tabs)\n",
    "- Optionally removes special characters for clean unformatted text"
   ],
   "id": "76f3936ac0e24e17"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-10T14:52:18.774444Z",
     "start_time": "2025-06-10T14:52:18.761173Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def remove_formatting( text: str ) -> str:\n",
    "\t\"\"\"\n",
    "\n",
    "        Removes formatting artifacts (Markdown, HTML, control characters) from pages.\n",
    "\n",
    "        This function:\n",
    "          - Strips HTML tags\n",
    "          - Removes Markdown syntax (e.g., *, #, [], etc.)\n",
    "          - Collapses whitespace (newlines, tabs)\n",
    "          - Optionally removes special characters for clean unformatted pages\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        pages : str\n",
    "            The formatted path pages.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        str\n",
    "            A cleaned_lines version of the pages with formatting removed.\n",
    "\n",
    "    \"\"\"\n",
    "\t# Remove HTML tags\n",
    "\ttext = BeautifulSoup( text, \"raw_html.parser\" ).get_text( separator=' ', strip=True )\n",
    "\n",
    "\t# Remove Markdown syntax\n",
    "\ttext = re.sub( r'\\[.*?\\]\\(.*?\\)', '', text )  # Markdown links\n",
    "\ttext = re.sub( r'[`_*#~>-]', '', text )  # Markdown chars\n",
    "\ttext = re.sub( r'!\\[.*?\\]\\(.*?\\)', '', text )  # Markdown images\n",
    "\n",
    "\t# Remove and normalize whitespace\n",
    "\ttext = re.sub( r'[\\r\\n\\t]+', ' ', text )  # Newlines, tabs\n",
    "\ttext = re.sub( r'\\s+', ' ', text ).strip( )  # Collapse multiple spaces\n",
    "\n",
    "\treturn text"
   ],
   "id": "be4039da12904496",
   "outputs": [],
   "execution_count": 188
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 16. Remove Stopwords\n",
    "- Tokenizes the input text\n",
    "- Removes common stopwords (e.g., \"the\", \"is\", \"and\", etc.)\n",
    "- Returns the text with only meaningful words"
   ],
   "id": "1c75687f6c08c2c5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 17. Split Sentences",
   "id": "66e071eacb3377be"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-10T14:52:22.701043Z",
     "start_time": "2025-06-10T14:52:22.688613Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def split_sentences( text: str ) -> List[ str] :\n",
    "    \"\"\"\n",
    "\n",
    "\t\tPurpose:\n",
    "\t\t________\n",
    "\t\tSplits the text text string into a list of\n",
    "\t\tindividual sentences using NLTK's Punkt sentence tokenizer.\n",
    "\t\tThis function is useful for preparing text for further linguistic processing,\n",
    "\t\tsuch as tokenization, parsing, or named entity recognition.\n",
    "\n",
    "\t\tParameters\n",
    "\t\t----------\n",
    "\t\ttext : str\n",
    "\t\t\tThe raw text string to be segmented into sentences.\n",
    "\n",
    "\t\tReturns\n",
    "\t\t-------\n",
    "\t\tList[str]\n",
    "\t\t\tA list of sentence strings, each corresponding to a single sentence detected\n",
    "\t\t\tin the text text.\n",
    "\n",
    "    \"\"\"\n",
    "    return nltk.sent_tokenize( text )"
   ],
   "id": "9667d83b7478ae0d",
   "outputs": [],
   "execution_count": 189
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### ðŸ” Usage",
   "id": "f4c860851a941aeb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-10T14:52:26.540115Z",
     "start_time": "2025-06-10T14:52:26.522231Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def clean_text( text: str ) -> str:\n",
    "\t_first = text.replace( '\\r\\n', '\\n' ).replace( '\\r', '\\n' )\n",
    "\t_second = re.sub( r'\\n\\s*\\d+\\s*\\n', '\\n', _first )\n",
    "\t_third = re.sub( r'(\\w+)-\\n(\\w+)', r'\\1\\2', _second )\n",
    "\t_fouth = re.sub( r'(?<!\\n)\\n(?![\\n])', ' ', _third )\n",
    "\t_retval = re.sub( r'\\s+', ' ', _fouth )\n",
    "\treturn _retval"
   ],
   "id": "6ffc1b387c68d803",
   "outputs": [],
   "execution_count": 190
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### ðŸ” Pipeline",
   "id": "f9435a341c128b29"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T14:28:06.587921Z",
     "start_time": "2025-05-14T14:28:06.568106Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === Preprocessing Configuration ===\n",
    "EN_STOPWORDS = set( stopwords.words( 'english' ) )\n",
    "LEMMATIZER = WordNetLemmatizer( )\n",
    "STEMMER = SnowballStemmer( 'english' )\n",
    "\n",
    "\n",
    "def preprocess_line( line, lower=True, punctuation=True,\n",
    "                     stopwords=False, lemmatize=True, stem=False ):\n",
    "\t'''\n",
    "\t\tProcess a single line of documents with optional steps:\n",
    "\t\t- lower\n",
    "\t\t- punctuation removal\n",
    "\t\t- stopword removal\n",
    "\t\t- lemmatization\n",
    "\t\t- stemming (optional)\n",
    "\t\tReturns the cleaned_lines line as a path.\n",
    "    '''\n",
    "\ttokens = word_tokenize( line )\n",
    "\tprocessed = [ ]\n",
    "\tfor token in tokens:\n",
    "\t\tif lower:\n",
    "\t\t\ttoken = token.lower( )\n",
    "\n",
    "\t\tif punctuation and token in string.punctuation:\n",
    "\t\t\tcontinue\n",
    "\n",
    "\t\tif stopwords and token in EN_STOPWORDS:\n",
    "\t\t\tcontinue\n",
    "\n",
    "\t\tif lemmatize:\n",
    "\t\t\ttoken = LEMMATIZER.lemmatize( token )\n",
    "\n",
    "\t\tif stem:\n",
    "\t\t\ttoken = STEMMER.stem( token )\n",
    "\n",
    "\t\tprocessed.append( token )\n",
    "\n",
    "\treturn ' '.join( processed )\n",
    "\n",
    "\n",
    "def process_file( file_path, **preprocess_kwargs ):\n",
    "\t'''\n",
    "        Read a documents file line-by-line, apply preprocessing pipeline to each line,\n",
    "        and return a get_list of cleaned_lines tokens (original order preserved).\n",
    "        Pass any keyword args supported by `preprocess_line`.\n",
    "    '''\n",
    "\tcleaned_lines = [ ]\n",
    "\twith open( file_path, 'r', encoding='utf-8' ) as file:\n",
    "\t\tfor line in file:\n",
    "\t\t\tcleaned = preprocess_line( line, **preprocess_kwargs )\n",
    "\t\t\tcleaned_lines.append( cleaned )\n",
    "\treturn cleaned_lines\n"
   ],
   "id": "af7f3fd7b2990387",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "c7c6e615a319e113"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a0478bcf04eb5543"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#  Vectorization (Preparation for ML Models)\n",
    "___\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "ba0f01ba423430f4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Load Dependencies",
   "id": "d3e02f4dbedcc22e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T22:56:53.926365Z",
     "start_time": "2025-05-14T22:56:53.918962Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "6ea810a20c6020f2",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T22:56:58.455941Z",
     "start_time": "2025-05-14T22:56:58.449512Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === Load Raw Text ===\n",
    "file_path = '<url to file>'\n",
    "_rawtext = ''\n",
    "\n",
    "\n",
    "def load_text( file_path ):\n",
    "\twith open( file_path, 'r', encoding='utf-8' ) as f:\n",
    "\t\t_rawtext = f.read( )\n",
    "\t\treturn _rawtext"
   ],
   "id": "f930499beb91e45f",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "4cd82d1886e5243"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### ðŸ§® 1. Bag of Words (BoW) using CountVectorizer",
   "id": "93554b0db878266a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T22:57:25.676359Z",
     "start_time": "2025-05-14T22:57:25.657084Z"
    }
   },
   "cell_type": "code",
   "source": [
    "corpus = [ 'Bro loves clean code.', 'Code is life.' ]\n",
    "vectorizer = CountVectorizer( )\n",
    "X = vectorizer.fit_transform( corpus )\n",
    "\n",
    "print( vectorizer.get_feature_names_out( ) )\n",
    "print( X.toarray( ) )\n"
   ],
   "id": "df2a01ee327d57d6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bro' 'clean' 'code' 'is' 'life' 'loves']\n",
      "[[1 1 1 0 0 1]\n",
      " [0 0 1 1 1 0]]\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### ðŸ“Š 2. TF-IDF using TfidfVectorizer",
   "id": "606ca62c6f083d04"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T22:57:28.084090Z",
     "start_time": "2025-05-14T22:57:28.068021Z"
    }
   },
   "cell_type": "code",
   "source": [
    "corpus = [ 'Bro writes awesome code.', 'Code must be clean and clear.' ]\n",
    "vectorizer = TfidfVectorizer( )\n",
    "X = vectorizer.fit_transform( corpus )\n",
    "\n",
    "print( vectorizer.get_feature_names_out( ) )\n",
    "print( X.toarray( ) )\n"
   ],
   "id": "58a4cf4ec025e261",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and' 'awesome' 'be' 'bro' 'clean' 'clear' 'code' 'must' 'writes']\n",
      "[[0.         0.53404633 0.         0.53404633 0.         0.\n",
      "  0.37997836 0.         0.53404633]\n",
      " [0.4261596  0.         0.4261596  0.         0.4261596  0.4261596\n",
      "  0.30321606 0.4261596  0.        ]]\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### ðŸ§  3. Word2Vec using gensim",
   "id": "cd392a973450c93a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T22:57:30.828776Z",
     "start_time": "2025-05-14T22:57:30.765259Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sentences = [ [ 'bro', 'loves', 'python' ], [ 'clean', 'code', 'rocks' ] ]\n",
    "model = Word2Vec( sentences, vector_size=100, window=5, min_count=1, workers=4 )\n",
    "\n",
    "# VectorStore for the word 'bro'\n",
    "vector = model.wv[ 'bro' ]\n",
    "print( vector )\n"
   ],
   "id": "a20f65018fd9c119",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-8.7274825e-03  2.1301615e-03 -8.7354420e-04 -9.3190884e-03\n",
      " -9.4281426e-03 -1.4107180e-03  4.4324086e-03  3.7040710e-03\n",
      " -6.4986930e-03 -6.8730675e-03 -4.9994122e-03 -2.2868442e-03\n",
      " -7.2502876e-03 -9.6033178e-03 -2.7436293e-03 -8.3628409e-03\n",
      " -6.0388758e-03 -5.6709289e-03 -2.3441375e-03 -1.7069972e-03\n",
      " -8.9569986e-03 -7.3519943e-04  8.1525063e-03  7.6904297e-03\n",
      " -7.2061159e-03 -3.6668312e-03  3.1185520e-03 -9.5707225e-03\n",
      "  1.4764392e-03  6.5244664e-03  5.7464195e-03 -8.7630618e-03\n",
      " -4.5171441e-03 -8.1401607e-03  4.5956374e-05  9.2636338e-03\n",
      "  5.9733056e-03  5.0673080e-03  5.0610625e-03 -3.2429171e-03\n",
      "  9.5521836e-03 -7.3564244e-03 -7.2703874e-03 -2.2653891e-03\n",
      " -7.7856064e-04 -3.2161034e-03 -5.9258583e-04  7.4888230e-03\n",
      " -6.9751858e-04 -1.6249407e-03  2.7443992e-03 -8.3591007e-03\n",
      "  7.8558037e-03  8.5361041e-03 -9.5840869e-03  2.4462664e-03\n",
      "  9.9049713e-03 -7.6658037e-03 -6.9669187e-03 -7.7365171e-03\n",
      "  8.3959233e-03 -6.8133592e-04  9.1444086e-03 -8.1582209e-03\n",
      "  3.7430846e-03  2.6350426e-03  7.4271322e-04  2.3276759e-03\n",
      " -7.4690939e-03 -9.3583735e-03  2.3545765e-03  6.1484552e-03\n",
      "  7.9856887e-03  5.7358947e-03 -7.7733636e-04  8.3061643e-03\n",
      " -9.3363142e-03  3.4061326e-03  2.6675343e-04  3.8572443e-03\n",
      "  7.3857834e-03 -6.7251669e-03  5.5844807e-03 -9.5222248e-03\n",
      " -8.0445886e-04 -8.6887367e-03 -5.0986730e-03  9.2892265e-03\n",
      " -1.8582619e-03  2.9144264e-03  9.0712793e-03  8.9381328e-03\n",
      " -8.2084350e-03 -3.0123137e-03  9.8866057e-03  5.1044310e-03\n",
      " -1.5880871e-03 -8.6920215e-03  2.9615164e-03 -6.6758976e-03]\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### ðŸŒ 4. GloVe using gensim (with pre-trained vectors)\n",
   "id": "a920cc11594a37ac"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load GloVe vec (convert .txt to .word2vec format beforehand if needed)\n",
    "glove_file = r'C:\\Users\\terry\\source\\llm\\glove\\glove.6B.100d.txt'\n",
    "model = KeyedVectors.load_word2vec_format( glove_file, unicode_errors='ignore' )\n",
    "\n",
    "# VectorStore for the word 'code'\n",
    "vector = model[ 'code' ]\n",
    "print( vector )\n"
   ],
   "id": "95c58a17aeefed99",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### ðŸ¤– 5. BERT / Transformer-based Embeddings using transformers + torch\n",
   "id": "8117a17e891dc4c0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T22:58:22.666037Z",
     "start_time": "2025-05-14T22:58:21.482437Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer = BertTokenizer.from_pretrained( 'bert-base-uncased' )\n",
    "model = BertModel.from_pretrained( 'bert-base-uncased' )\n",
    "\n",
    "sentence = \"Bro's code always works.\"\n",
    "inputs = tokenizer( sentence, return_tensors='pt' )\n",
    "outputs = model( **inputs )\n",
    "\n",
    "# Get the vector for [CLS] token (sentence embedding)\n",
    "sentence_embedding = outputs.last_hidden_state[ :, 0, : ]\n",
    "print( sentence_embedding.shape )\n"
   ],
   "id": "8a9d362295bf0536",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 768])\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Clean Document",
   "id": "5e61d59191d25e5c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T22:59:11.827016Z",
     "start_time": "2025-05-14T22:59:11.820001Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def clean_text( text: str ) -> str:\n",
    "\ttext = text.replace( '\\r\\n', '\\n' ).replace( '\\r', '\\n' )\n",
    "\ttext = re.sub( r'\\n\\s*\\d+\\s*\\n', '\\n', text )\n",
    "\ttext = re.sub( r'(\\w+)-\\n(\\w+)', r'\\1\\2', text )\n",
    "\ttext = re.sub( r'(?<!\\n)\\n(?![\\n])', ' ', text )\n",
    "\ttext = re.sub( r'\\s+', ' ', text )\n",
    "\treturn text.strip( )\n"
   ],
   "id": "42c34418a1218398",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "fc9d08f36dde47ba"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "#  OpenAI Embedding\n",
    "___"
   ],
   "id": "9f097164e0f1b335"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### API key",
   "id": "10f8b0039c619d8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Create client\n",
    "client = OpenAI( )\n",
    "client.api_key = os.getenv( 'OPENAI_API_KEY' )"
   ],
   "id": "6f7c70b192899b4b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 1. Define embedding function",
   "id": "e1c76c3548b771af"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T14:30:51.638210Z",
     "start_time": "2025-05-14T14:30:51.622433Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def embed_texts( texts, model='text-embedding-3-small', batch_size=10, sleep=1 ):\n",
    "\tembeddings = [ ]\n",
    "\tfor i in range( 0, len( texts ), batch_size ):\n",
    "\t\tbatch = texts[ i:i + batch_size ]\n",
    "\t\ttry:\n",
    "\t\t\tresponse = openai.embeddings.create( input=batch, model=model )\n",
    "\t\t\tbatch_embeddings = [ e.embedding for e in response.data ]\n",
    "\t\t\tembeddings.extend( batch_embeddings )\n",
    "\t\texcept Exception as e:\n",
    "\t\t\tprint( f'Error at batch {i}: {e}' )\n",
    "\t\t\t# Retry or sleep to avoid rate limits\n",
    "\t\t\ttime.sleep( sleep )\n",
    "\t\t\tcontinue\n",
    "\n",
    "\treturn embeddings\n"
   ],
   "id": "3d19dce1b72312ca",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 2. Embed chunks",
   "id": "7a1cbc3787a81ed2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 2. Embed chunks\n",
    "embeddings = embed_texts( chunks )"
   ],
   "id": "2e8dd3b9c7453b0c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 3.  Create DataFrame",
   "id": "7b29c98e70fdc973"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 3. Create DataFrame\n",
    "df_embeddings = pd.DataFrame( { chunks, embeddings } )\n"
   ],
   "id": "998cc4a4eb1c122f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 3. Save\n",
   "id": "ca13ccc289f545b4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 3. Save\n",
    "df_embeddings.to_parquet( 'public_law_118_32_embeddings.parquet', index=False )\n"
   ],
   "id": "eac78df8b7b91593"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 4. Preview",
   "id": "ae06c79ed3506313"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 4. Preview\n",
    "df_embeddings.head( 2 )\n"
   ],
   "id": "e9935e9a89b6aac5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3. Generate Embeddings\n",
    "- Use a language model (e.g., OpenAI, HuggingFace) to create vector representations of each chunk_words."
   ],
   "id": "f62aab776049a508"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model = SentenceTransformer( 'all-MiniLM-L6-v2' )\n",
    "embeddings = model.encode( chunks, show_progress_bar=True )\n"
   ],
   "id": "a67fd85d25aafd1c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 4. Create SQLite Database",
   "id": "d76b8831817cd4d9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- Design a table that links text chunks to their embeddings.",
   "id": "f06349ccd853f770"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "conn = sqlite3.connect( 'vectors.target_values' )\n",
    "cursor = conn.cursor( )\n",
    "sql_create = '''\n",
    "CREATE TABLE IF NOT EXISTS Law_Embeddings\n",
    "(\n",
    "    Id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    Chunk_Tokens TEXT NOT NULL,\n",
    "    Embedding BLOB NOT NULL\n",
    ")\n",
    "'''\n",
    "\n",
    "cursor.execute( sql_create )\n",
    "\n",
    "for chunk, vector in zip( chunks, embeddings ):\n",
    "\tblob = pickle.dumps( vector )\n",
    "\tcursor.execute( 'INSERT INTO Law_Embeddings ( Chunk_Tokens, Embedding ) VALUES (?, ?)',\n",
    "\t\t(chunk, blob) )\n",
    "\n",
    "conn.commit( )\n",
    "conn.close( )\n"
   ],
   "id": "28ca2f108b874b20"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "###  Retrieval (Vector Search in SQLite)",
   "id": "ca5ad81f911f7250"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- You can perform semantic search by encoding a query and comparing via cosine similarity\n",
   "id": "12113e4d3566bd2f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def cosine_similarity( a, b ):\n",
    "\treturn np.dot( a, b ) / (np.linalg.norm( a ) * np.linalg.norm( b ))"
   ],
   "id": "70665fe56a643b3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "query = 'Appropriations for Department of Defense'\n",
    "query_vec = model.encode( [ query ] )[ 0 ]\n",
    "\n",
    "conn = sqlite3.connect( 'vectors.target_values' )\n",
    "cursor = conn.cursor( )\n",
    "cursor.execute( 'SELECT Id, Chunk_Tokens, Embedding FROM Law_Embeddings' )\n",
    "\n",
    "results = [ ]\n",
    "for row in cursor.fetchall( ):\n",
    "\tchunk_id, chunk_text, blob = row\n",
    "\tstored_vec = pickle.loads( blob )\n",
    "\tsim = cosine_similarity( query_vec, stored_vec )\n",
    "\tresults.append( (sim, chunk_text) )\n",
    "\n",
    "# Sort and get top N\n",
    "top_matches = sorted( results, key=lambda x: x[ 0 ], reverse=True )[ :5 ]\n"
   ],
   "id": "675d18dac5bd124d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#   Embedding-Pipeline Script\n",
    "___"
   ],
   "id": "3a7f735556cd0d3f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Load Dependencies\n",
    "\n"
   ],
   "id": "e593513dcb937f5f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T12:08:03.310828Z",
     "start_time": "2025-05-14T12:08:03.304763Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import openai\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n"
   ],
   "id": "cf12b8bc356c9181",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Configuration",
   "id": "99ef339bdfea1fd8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "a764c56d86acecd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Define paths\n",
    "TEXT_FILE = 'PublicLaw_118-42.txt'\n",
    "DB_FILE = 'law_embeddings.target_values'\n",
    "EMBEDDING_MODEL = 'all-MiniLM-L6-v2'\n",
    "CHUNK_SIZE = 1000\n",
    "CHUNK_OVERLAP = 200\n"
   ],
   "id": "e8ee553da46c407c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Load and Clean Raw Text",
   "id": "842a900a1b80cde5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def load_and_clean_text( filepath ):\n",
    "\twith open( filepath, 'r', encoding='utf-8', errors='ignore' ) as file:\n",
    "\t\traw_text = file.read( )\n",
    "\n",
    "\t# Basic normalization\n",
    "\ttext = re.sub( r'\\f+', ' ', raw_text )\n",
    "\ttext = re.sub( r'\\n+', ' ', text )\n",
    "\ttext = re.sub( r'\\s{2,}', ' ', text )\n",
    "\treturn text.strip( )\n"
   ],
   "id": "c202cc689865f32"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Generate Embeddings",
   "id": "ae3ddba5e85404c6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def get_embedding( text, model=OPENAI_MODEL ):\n",
    "\tresponse = openai.Embedding.generate_text( input=text, model=model )\n",
    "\treturn response[ 'target_values'[ 0 ][ 'embedding' ] ]\n",
    "\n",
    "\n",
    "def embed_chunks( chunks ):\n",
    "\tembeddings = [ ]\n",
    "\tfor chunk in tqdm( chunks, desc='EmbeddingRequest chunks via OpenAI' ):\n",
    "\t\ttry:\n",
    "\t\t\tembedding = get_embedding( chunk )\n",
    "\t\t\tembeddings.append( embedding )\n",
    "\t\texcept Exception as e:\n",
    "\t\t\tprint( f'Error embedding chunk_words: {e}' )\n",
    "\t\t\tembeddings.append( [ 0.0 ] * 1536 )  # Placeholder for failed requests\n",
    "\treturn embeddings\n"
   ],
   "id": "8fa7587e7a3094be"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Create SQLite DB\n",
    "\n"
   ],
   "id": "3c985e6d3ecd5e8f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def create_and_populate_db( chunks, embeddings, db_path ):\n",
    "\tconn = sqlite3.connect( db_path )\n",
    "\tcursor = conn.cursor( )\n",
    "\tsql_create = '''\n",
    "    CREATE TABLE IF NOT EXISTS Law_Embeddings\n",
    "    (\n",
    "        Id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "        Chunk_Tokens TEXT NOT NULL,\n",
    "        Embedding BLOB NOT NULL\n",
    "    )\n",
    "    '''\n",
    "\n",
    "\tcursor.execute( sql_create )\n",
    "\tfor chunk, vector in zip( chunks, embeddings ):\n",
    "\t\tblob = pickle.dumps( vector )\n",
    "\t\tsql_insert = 'INSERT INTO Law_Embeddings ( Chunk_Tokens, Embedding ) VALUES ( ?, ? )'\n",
    "\t\tcursor.execute( sql_insert, (chunk, blob) )\n",
    "\n",
    "\tconn.commit( )\n",
    "\tconn.close( )\n"
   ],
   "id": "ea15943e543e0d70"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Script",
   "id": "e82751929d4fd563"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# === MAIN ===\n",
    "def main( ):\n",
    "\tprint( 'Step 1: Load and clean documents' )\n",
    "\tcleaned_text = load_and_clean_text( TEXT_FILE )\n",
    "\n",
    "\tprint( 'Step 2: Chunking documents' )\n",
    "\tchunks = chunk_text( cleaned_text )\n",
    "\tprint( f'Total chunks: {len( chunks )}' )\n",
    "\n",
    "\tprint( 'Step 3: EmbeddingRequest with OpenAI API' )\n",
    "\tembeddings = embed_chunks( chunks )\n",
    "\n",
    "\tprint( 'Step 4: Saving to SQLite' )\n",
    "\tcreate_and_populate_db( chunks, embeddings, DB_FILE )\n",
    "\n",
    "\tprint( f'Pipeline complete. Embeddings stored in: {DB_FILE}' )\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\tmain( )"
   ],
   "id": "5f2d269aaa88212b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Embeddings",
   "id": "de305bd9c7ccbd96"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-10T14:54:19.035921Z",
     "start_time": "2025-06-10T14:53:52.331419Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# === 1. Load Model ===\n",
    "# You can try other models like 'all-MiniLM-L6-v2', 'all-mpnet-base-v2', or 'multi-qa-MiniLM-L6-cos-v1'\n",
    "model = SentenceTransformer( 'all-MiniLM-L6-v2' )\n",
    "\n",
    "\n",
    "# === 2. Embed Chunks ===\n",
    "def embed_with_sentence_transformers( texts, model ):\n",
    "\treturn model.encode( texts, show_progress_bar=True, convert_to_numpy=True )\n",
    "\n",
    "\n",
    "local_embeddings = embed_with_sentence_transformers( chunks, model )\n",
    "\n",
    "# === 3. Save in a DataFrame ===\n",
    "df_local = pd.DataFrame(\n",
    "{\n",
    "\t'chunk_words': chunks,\n",
    "\t'embedding': list( local_embeddings )  # numpy arrays to a list for DataFrame compatibility\n",
    "} )\n",
    "\n",
    "# === 4. Save to Disk ===\n",
    "df_local.to_parquet( 'public_law_118_32_local_embeddings.parquet', index=False )\n",
    "\n",
    "# === 5. Preview ===\n",
    "df_local.head( 2 )\n",
    "\n"
   ],
   "id": "47df028a12155604",
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "File \u001B[1;32m<frozen importlib._bootstrap>:1331\u001B[0m, in \u001B[0;36m_find_and_load_unlocked\u001B[1;34m(name, import_)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap>:921\u001B[0m, in \u001B[0;36m_load_unlocked\u001B[1;34m(spec)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap>:813\u001B[0m, in \u001B[0;36mmodule_from_spec\u001B[1;34m(spec)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap_external>:1289\u001B[0m, in \u001B[0;36mcreate_module\u001B[1;34m(self, spec)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap>:488\u001B[0m, in \u001B[0;36m_call_with_frames_removed\u001B[1;34m(f, *args, **kwds)\u001B[0m\n",
      "\u001B[1;31mImportError\u001B[0m: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[194], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01msentence_transformers\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m SentenceTransformer\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mnumpy\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mnp\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mpandas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mpd\u001B[39;00m\n",
      "File \u001B[1;32m~\\source\\compilers\\py\\anaconda\\Lib\\site-packages\\sentence_transformers\\__init__.py:14\u001B[0m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mos\u001B[39;00m\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01msentence_transformers\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbackend\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m     10\u001B[0m     export_dynamic_quantized_onnx_model,\n\u001B[0;32m     11\u001B[0m     export_optimized_onnx_model,\n\u001B[0;32m     12\u001B[0m     export_static_quantized_openvino_model,\n\u001B[0;32m     13\u001B[0m )\n\u001B[1;32m---> 14\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01msentence_transformers\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcross_encoder\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m     15\u001B[0m     CrossEncoder,\n\u001B[0;32m     16\u001B[0m     CrossEncoderModelCardData,\n\u001B[0;32m     17\u001B[0m     CrossEncoderTrainer,\n\u001B[0;32m     18\u001B[0m     CrossEncoderTrainingArguments,\n\u001B[0;32m     19\u001B[0m )\n\u001B[0;32m     20\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01msentence_transformers\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdatasets\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m ParallelSentencesDataset, SentencesDataset\n\u001B[0;32m     21\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01msentence_transformers\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mLoggingHandler\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m LoggingHandler\n",
      "File \u001B[1;32m~\\source\\compilers\\py\\anaconda\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\__init__.py:3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01m__future__\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m annotations\n\u001B[1;32m----> 3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mCrossEncoder\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m CrossEncoder\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodel_card\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m CrossEncoderModelCardData\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtrainer\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m CrossEncoderTrainer\n",
      "File \u001B[1;32m~\\source\\compilers\\py\\anaconda\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:15\u001B[0m\n\u001B[0;32m     13\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m nn\n\u001B[0;32m     14\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtqdm\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mautonotebook\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m trange\n\u001B[1;32m---> 15\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtransformers\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m     16\u001B[0m     AutoConfig,\n\u001B[0;32m     17\u001B[0m     AutoModelForSequenceClassification,\n\u001B[0;32m     18\u001B[0m     AutoTokenizer,\n\u001B[0;32m     19\u001B[0m     PretrainedConfig,\n\u001B[0;32m     20\u001B[0m     PreTrainedModel,\n\u001B[0;32m     21\u001B[0m )\n\u001B[0;32m     22\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtransformers\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m PushToHubMixin\n\u001B[0;32m     23\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtyping_extensions\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m deprecated\n",
      "File \u001B[1;32m<frozen importlib._bootstrap>:1412\u001B[0m, in \u001B[0;36m_handle_fromlist\u001B[1;34m(module, fromlist, import_, recursive)\u001B[0m\n",
      "File \u001B[1;32m~\\source\\compilers\\py\\anaconda\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1956\u001B[0m, in \u001B[0;36m_LazyModule.__getattr__\u001B[1;34m(self, name)\u001B[0m\n\u001B[0;32m   1954\u001B[0m     value \u001B[38;5;241m=\u001B[39m Placeholder\n\u001B[0;32m   1955\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m name \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_class_to_module\u001B[38;5;241m.\u001B[39mkeys():\n\u001B[1;32m-> 1956\u001B[0m     module \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_module\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_class_to_module\u001B[49m\u001B[43m[\u001B[49m\u001B[43mname\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1957\u001B[0m     value \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(module, name)\n\u001B[0;32m   1958\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m name \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_modules:\n",
      "File \u001B[1;32m~\\source\\compilers\\py\\anaconda\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1968\u001B[0m, in \u001B[0;36m_LazyModule._get_module\u001B[1;34m(self, module_name)\u001B[0m\n\u001B[0;32m   1966\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m_get_module\u001B[39m(\u001B[38;5;28mself\u001B[39m, module_name: \u001B[38;5;28mstr\u001B[39m):\n\u001B[0;32m   1967\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 1968\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mimportlib\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mimport_module\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m.\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mmodule_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;18;43m__name__\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1969\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m   1970\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[0;32m   1971\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFailed to import \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodule_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m because of the following error (look up to see its\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1972\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m traceback):\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00me\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1973\u001B[0m         ) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01me\u001B[39;00m\n",
      "File \u001B[1;32m~\\source\\compilers\\py\\anaconda\\Lib\\importlib\\__init__.py:90\u001B[0m, in \u001B[0;36mimport_module\u001B[1;34m(name, package)\u001B[0m\n\u001B[0;32m     88\u001B[0m             \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[0;32m     89\u001B[0m         level \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m---> 90\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_bootstrap\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_gcd_import\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m[\u001B[49m\u001B[43mlevel\u001B[49m\u001B[43m:\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpackage\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlevel\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\source\\compilers\\py\\anaconda\\Lib\\site-packages\\transformers\\modeling_utils.py:61\u001B[0m\n\u001B[0;32m     56\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mintegrations\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msdpa_attention\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m sdpa_attention_forward\n\u001B[0;32m     57\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mintegrations\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtensor_parallel\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m     58\u001B[0m     SUPPORTED_TP_STYLES,\n\u001B[0;32m     59\u001B[0m     shard_and_distribute_module,\n\u001B[0;32m     60\u001B[0m )\n\u001B[1;32m---> 61\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mloss\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mloss_utils\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m LOSS_MAPPING\n\u001B[0;32m     62\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpytorch_utils\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m (  \u001B[38;5;66;03m# noqa: F401\u001B[39;00m\n\u001B[0;32m     63\u001B[0m     Conv1D,\n\u001B[0;32m     64\u001B[0m     apply_chunking_to_forward,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     69\u001B[0m     prune_linear_layer,\n\u001B[0;32m     70\u001B[0m )\n\u001B[0;32m     71\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mquantizers\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m AutoHfQuantizer, HfQuantizer\n",
      "File \u001B[1;32m~\\source\\compilers\\py\\anaconda\\Lib\\site-packages\\transformers\\loss\\loss_utils.py:19\u001B[0m\n\u001B[0;32m     16\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mnn\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mnn\u001B[39;00m\n\u001B[0;32m     17\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mnn\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m BCEWithLogitsLoss, MSELoss\n\u001B[1;32m---> 19\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mloss_deformable_detr\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m DeformableDetrForObjectDetectionLoss, DeformableDetrForSegmentationLoss\n\u001B[0;32m     20\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mloss_for_object_detection\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m ForObjectDetectionLoss, ForSegmentationLoss\n\u001B[0;32m     21\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mloss_grounding_dino\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m GroundingDinoForObjectDetectionLoss\n",
      "File \u001B[1;32m~\\source\\compilers\\py\\anaconda\\Lib\\site-packages\\transformers\\loss\\loss_deformable_detr.py:4\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtorch\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mnn\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mnn\u001B[39;00m\n\u001B[1;32m----> 4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mimage_transforms\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m center_to_corners_format\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m is_scipy_available\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mloss_for_object_detection\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m      7\u001B[0m     HungarianMatcher,\n\u001B[0;32m      8\u001B[0m     ImageLoss,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     11\u001B[0m     sigmoid_focal_loss,\n\u001B[0;32m     12\u001B[0m )\n",
      "File \u001B[1;32m~\\source\\compilers\\py\\anaconda\\Lib\\site-packages\\transformers\\image_transforms.py:48\u001B[0m\n\u001B[0;32m     45\u001B[0m     \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtorch\u001B[39;00m\n\u001B[0;32m     47\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_tf_available():\n\u001B[1;32m---> 48\u001B[0m     \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtf\u001B[39;00m\n\u001B[0;32m     50\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_flax_available():\n\u001B[0;32m     51\u001B[0m     \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mjax\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mnumpy\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mjnp\u001B[39;00m\n",
      "File \u001B[1;32m~\\source\\compilers\\py\\anaconda\\Lib\\site-packages\\tensorflow\\__init__.py:40\u001B[0m\n\u001B[0;32m     37\u001B[0m _os\u001B[38;5;241m.\u001B[39menviron\u001B[38;5;241m.\u001B[39msetdefault(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mENABLE_RUNTIME_UPTIME_TELEMETRY\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m1\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     39\u001B[0m \u001B[38;5;66;03m# Do not remove this line; See https://github.com/tensorflow/tensorflow/issues/42596\u001B[39;00m\n\u001B[1;32m---> 40\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m pywrap_tensorflow \u001B[38;5;28;01mas\u001B[39;00m _pywrap_tensorflow  \u001B[38;5;66;03m# pylint: disable=unused-import\u001B[39;00m\n\u001B[0;32m     41\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtools\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m module_util \u001B[38;5;28;01mas\u001B[39;00m _module_util\n\u001B[0;32m     42\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutil\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlazy_loader\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m KerasLazyLoader \u001B[38;5;28;01mas\u001B[39;00m _KerasLazyLoader\n",
      "File \u001B[1;32m~\\source\\compilers\\py\\anaconda\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py:73\u001B[0m\n\u001B[0;32m     71\u001B[0m \u001B[38;5;66;03m# pylint: disable=wildcard-import,g-import-not-at-top,line-too-long,undefined-variable\u001B[39;00m\n\u001B[0;32m     72\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m---> 73\u001B[0m   \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_pywrap_tensorflow_internal\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n\u001B[0;32m     74\u001B[0m \u001B[38;5;66;03m# This try catch logic is because there is no bazel equivalent for py_extension.\u001B[39;00m\n\u001B[0;32m     75\u001B[0m \u001B[38;5;66;03m# Externally in opensource we must enable exceptions to load the shared object\u001B[39;00m\n\u001B[0;32m     76\u001B[0m \u001B[38;5;66;03m# by exposing the PyInit symbols with pybind. This error will only be\u001B[39;00m\n\u001B[0;32m     77\u001B[0m \u001B[38;5;66;03m# caught internally or if someone changes the name of the target _pywrap_tensorflow_internal.\u001B[39;00m\n\u001B[0;32m     78\u001B[0m \n\u001B[0;32m     79\u001B[0m \u001B[38;5;66;03m# This logic is used in other internal projects using py_extension.\u001B[39;00m\n\u001B[0;32m     80\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mModuleNotFoundError\u001B[39;00m:\n",
      "File \u001B[1;32m<frozen importlib._bootstrap>:1360\u001B[0m, in \u001B[0;36m_find_and_load\u001B[1;34m(name, import_)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap>:1334\u001B[0m, in \u001B[0;36m_find_and_load_unlocked\u001B[1;34m(name, import_)\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 194
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Fine-Tuning",
   "id": "e1efd28f65f33d39"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-10T14:53:49.417638Z",
     "start_time": "2025-06-10T14:53:49.411148Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "import openai\n",
    "import os\n",
    "import pandas as pd\n",
    "from pprint import pprint\n"
   ],
   "id": "51f033fc98145091",
   "outputs": [],
   "execution_count": 193
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "client = openai.OpenAI(\n",
    "\tapi_key=os.environ.get( 'OPENAI_API_KEY' ),\n",
    "\torganization='<org id>',\n",
    "\tproject='<project id>',\n",
    ")"
   ],
   "id": "b11d00974dbaa480"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T23:14:01.451194Z",
     "start_time": "2025-05-14T23:14:01.270150Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Read in the dataset we'll use for this task.\n",
    "# This will be the RecipesNLG dataset, which we've cleaned_lines to only contain documents from www.cookbooks.com\n",
    "recipe_df = pd.read_csv( r'C:\\Users\\terry\\Desktop\\cookbook_recipes_nlg_10k.csv' )\n",
    "recipe_df.head( )"
   ],
   "id": "aa84f0c255158f4f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                   title                                        ingredients  \\\n",
       "0    No-Bake Nut Cookies  [\"1 c. firmly packed brown sugar\", \"1/2 c. eva...   \n",
       "1  Jewell Ball'S Chicken  [\"1 small jar chipped beef, cut up\", \"4 boned ...   \n",
       "2            Creamy Corn  [\"2 (16 oz.) pkg. frozen corn\", \"1 (8 oz.) pkg...   \n",
       "3          Chicken Funny  [\"1 large whole chicken\", \"2 (10 1/2 oz.) cans...   \n",
       "4   Reeses Cups(Candy)    [\"1 c. peanut butter\", \"3/4 c. graham cracker ...   \n",
       "\n",
       "                                          directions  \\\n",
       "0  [\"In a heavy 2-quart saucepan, mix brown sugar...   \n",
       "1  [\"Place chipped beef on bottom of baking dish....   \n",
       "2  [\"In a slow cooker, combine all ingredients. C...   \n",
       "3  [\"Boil and debone chicken.\", \"Put bite size pi...   \n",
       "4  [\"Combine first four ingredients and press in ...   \n",
       "\n",
       "                                              link             source  \\\n",
       "0   www.cookbooks.com/Recipe-Details.aspx?id=44874  www.cookbooks.com   \n",
       "1  www.cookbooks.com/Recipe-Details.aspx?id=699419  www.cookbooks.com   \n",
       "2   www.cookbooks.com/Recipe-Details.aspx?id=10570  www.cookbooks.com   \n",
       "3  www.cookbooks.com/Recipe-Details.aspx?id=897570  www.cookbooks.com   \n",
       "4  www.cookbooks.com/Recipe-Details.aspx?id=659239  www.cookbooks.com   \n",
       "\n",
       "                                                 NER  \n",
       "0  [\"brown sugar\", \"milk\", \"vanilla\", \"nuts\", \"bu...  \n",
       "1  [\"beef\", \"chicken breasts\", \"cream of mushroom...  \n",
       "2  [\"frozen corn\", \"cream cheese\", \"butter\", \"gar...  \n",
       "3  [\"chicken\", \"chicken gravy\", \"cream of mushroo...  \n",
       "4  [\"peanut butter\", \"graham cracker crumbs\", \"bu...  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>ingredients</th>\n",
       "      <th>directions</th>\n",
       "      <th>link</th>\n",
       "      <th>source</th>\n",
       "      <th>NER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>No-Bake Nut Cookies</td>\n",
       "      <td>[\"1 c. firmly packed brown sugar\", \"1/2 c. eva...</td>\n",
       "      <td>[\"In a heavy 2-quart saucepan, mix brown sugar...</td>\n",
       "      <td>www.cookbooks.com/Recipe-Details.aspx?id=44874</td>\n",
       "      <td>www.cookbooks.com</td>\n",
       "      <td>[\"brown sugar\", \"milk\", \"vanilla\", \"nuts\", \"bu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jewell Ball'S Chicken</td>\n",
       "      <td>[\"1 small jar chipped beef, cut up\", \"4 boned ...</td>\n",
       "      <td>[\"Place chipped beef on bottom of baking dish....</td>\n",
       "      <td>www.cookbooks.com/Recipe-Details.aspx?id=699419</td>\n",
       "      <td>www.cookbooks.com</td>\n",
       "      <td>[\"beef\", \"chicken breasts\", \"cream of mushroom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Creamy Corn</td>\n",
       "      <td>[\"2 (16 oz.) pkg. frozen corn\", \"1 (8 oz.) pkg...</td>\n",
       "      <td>[\"In a slow cooker, combine all ingredients. C...</td>\n",
       "      <td>www.cookbooks.com/Recipe-Details.aspx?id=10570</td>\n",
       "      <td>www.cookbooks.com</td>\n",
       "      <td>[\"frozen corn\", \"cream cheese\", \"butter\", \"gar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Chicken Funny</td>\n",
       "      <td>[\"1 large whole chicken\", \"2 (10 1/2 oz.) cans...</td>\n",
       "      <td>[\"Boil and debone chicken.\", \"Put bite size pi...</td>\n",
       "      <td>www.cookbooks.com/Recipe-Details.aspx?id=897570</td>\n",
       "      <td>www.cookbooks.com</td>\n",
       "      <td>[\"chicken\", \"chicken gravy\", \"cream of mushroo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Reeses Cups(Candy)</td>\n",
       "      <td>[\"1 c. peanut butter\", \"3/4 c. graham cracker ...</td>\n",
       "      <td>[\"Combine first four ingredients and press in ...</td>\n",
       "      <td>www.cookbooks.com/Recipe-Details.aspx?id=659239</td>\n",
       "      <td>www.cookbooks.com</td>\n",
       "      <td>[\"peanut butter\", \"graham cracker crumbs\", \"bu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "system_message = 'You are a helpful recipe assistant. You are to extract the generic ingredients from each of the recipes provided.'\n",
    "\n",
    "def create_user_message( row ):\n",
    "\treturn f'Title: {row[ 'title' ]}\\n\\nIngredients: {row[ 'ingredients' ]}\\n\\nGeneric ingredients: '\n",
    "\n",
    "\n",
    "def prepare_example_conversation( row ):\n",
    "\treturn \\\n",
    "\t{\n",
    "\t\t'messages': [\n",
    "\t\t{\n",
    "\t\t\t'role': 'system',\n",
    "\t\t\t'content': system_message\n",
    "\t\t},\n",
    "\t\t{\n",
    "\t\t\t'role': 'user',\n",
    "\t\t\t'content': create_user_message( row )\n",
    "\t\t},\n",
    "\t\t{\n",
    "\t\t\t'role': 'assistant',\n",
    "\t\t\t'content': row[ 'NER' ]\n",
    "\t\t}, ]\n",
    "\t}\n",
    "\n"
   ],
   "id": "d29e9579f6f0e9dd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# use the first 100 rows of the dataset for training\n",
    "training_df = recipe_df.loc[ 0:100 ]\n",
    "\n",
    "# apply the prepare_example_conversation function to each row of the training_df\n",
    "training_data = training_df.apply( prepare_example_conversation, axis=1 ).tolist( )\n",
    "\n",
    "for example in training_data[ :5 ]:\n",
    "\tprint( example )"
   ],
   "id": "23bdc89006166778"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "validation_df = recipe_df.loc[ 101:200 ]\n",
    "validation_data = validation_df.apply(\n",
    "\tprepare_example_conversation, axis=1 ).tolist( )"
   ],
   "id": "4773228f51c07750"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "3896c43cec6862e4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def write_jsonl( data: List[ Dict ], filename: str ) -> None:\n",
    "\twith open( filename, 'w' ) as out:\n",
    "\t\tfor kvp in data:\n",
    "\t\t\tjout = json.dumps( kvp ) + '\\n'\n",
    "\t\t\tout.write( jout )"
   ],
   "id": "6dcf445d39eee404"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "652e2134d8966009"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "training_file_name = 'tmp_recipe_finetune_training.jsonl'\n",
    "write_jsonl( training_data, training_file_name )\n",
    "\n",
    "validation_file_name = 'tmp_recipe_finetune_validation.jsonl'\n",
    "write_jsonl( validation_data, validation_file_name )"
   ],
   "id": "a492a14231147287"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "170bad89408062ff"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def upload_file( file_name: str, purpose: str ) -> str:\n",
    "\twith open( file_name, 'rb' ) as file_fd:\n",
    "\t\tresponse = client.files.create( file=file_fd, purpose=purpose )\n",
    "\treturn response.id"
   ],
   "id": "10f941082775954c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "751aceb620d9c7bb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "training_file_id = upload_file( training_file_name, 'fine-tune' )\n",
    "validation_file_id = upload_file( validation_file_name, 'fine-tune' )"
   ],
   "id": "c8dbd2395ef59d03"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "MODEL = 'openai-4o-mini-2024-07-18'\n",
    "\n",
    "response = client.fine_tuning.jobs.create(\n",
    "\ttraining_file=training_file_id,\n",
    "\tvalidation_file=validation_file_id,\n",
    "\tmodel=MODEL,\n",
    "\tsuffix='recipe-ner',\n",
    ")\n",
    "\n",
    "job_id = response.id"
   ],
   "id": "5ca162663d00cad3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Text Cleaning Pipeline",
   "id": "dcabaa53b3a578c0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T13:29:43.919284Z",
     "start_time": "2025-05-16T13:29:43.866447Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "1dd53ce7908a5042",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'tigrr' from 'C:\\\\Users\\\\terry\\\\source\\\\repos\\\\Boo\\\\src\\\\tigrr.py'>"
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 376
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Preprocessing",
   "id": "f8d915c3f2d4b28f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "af2bd8b593c87d8b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for i in range( 10 ):\n",
    "\tprint( lines[ i ] )"
   ],
   "id": "471b42f7fd2f8492",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T20:40:56.796230Z",
     "start_time": "2025-05-15T20:40:56.769797Z"
    }
   },
   "cell_type": "code",
   "source": [
    "new = r'C:\\Users\\terry\\Desktop\\Text\\Chunked'  + '\\\\' + filename\n",
    "folder = open( new, 'wt+' )\n",
    "processed = [ ]\n",
    "for i, c in enumerate( lines ):\n",
    "\tpart = ' '.join( c )\n",
    "\tline = '{ ' + f'\"{i}\"' + ' : ' + '\"' + part + '\"' + ' },' + '\\r'\n",
    "\tprocessed.append( line )\n",
    "\n",
    "for line in processed:\n",
    "\tfolder.write( line )\n",
    "\n",
    "folder.close( )"
   ],
   "id": "feb3b69467ab7c9c",
   "outputs": [],
   "execution_count": 331
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-10T14:54:28.349345Z",
     "start_time": "2025-06-10T14:54:28.251781Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from importlib import reload\n",
    "import tigrr as tgr\n",
    "reload( tgr )\n",
    "from tigrr import Text"
   ],
   "id": "7e63e87a30d13ac0",
   "outputs": [],
   "execution_count": 195
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-10T15:18:48.506593Z",
     "start_time": "2025-06-10T15:18:48.499817Z"
    }
   },
   "cell_type": "code",
   "source": [
    "src_text = r'C:\\Users\\terry\\Desktop\\Budget\\Guidance\\Regulations\\Text'\n",
    "src = r'C:\\Users\\terry\\Desktop\\Test\\Text'\n",
    "dest_cleaned = r'C:\\Users\\terry\\Desktop\\Budget\\Guidance\\Regulations\\Cleaned'\n",
    "dest = r'C:\\Users\\terry\\Desktop\\Test\\Cleaned'"
   ],
   "id": "42c39b663224ebcf",
   "outputs": [],
   "execution_count": 233
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T21:32:38.581984Z",
     "start_time": "2025-06-05T21:32:38.570707Z"
    }
   },
   "cell_type": "code",
   "source": "txtr = Text( )",
   "id": "b185730909988566",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-10T18:36:12.953762Z",
     "start_time": "2025-06-10T18:36:12.929068Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def clean_files( src: str, dest: str ) -> None:\n",
    "\ttry:\n",
    "\t\tif src is None:\n",
    "\t\t\traise Exception( 'The argument \"src\" is required.' )\n",
    "\t\telif dest is None:\n",
    "\t\t\traise Exception( 'The argument \"dest\" is required.' )\n",
    "\t\telse:\n",
    "\t\t\tsource = src\n",
    "\t\t\tdestination = dest\n",
    "\t\t\tfiles = os.listdir( source )\n",
    "\t\t\tfor f in files:\n",
    "\t\t\t\tprocessed = [ ]\n",
    "\t\t\t\tfilename = os.path.basename( f )\n",
    "\t\t\t\tsource_path = source + '\\\\' + filename\n",
    "\t\t\t\ttext = open( source_path, 'r', encoding='utf-8', errors='ignore' ).read( )\n",
    "\t\t\t\tsentences = split_sentences( text)\n",
    "\t\t\t\tfor s in sentences:\n",
    "\t\t\t\t\tif s != \" \":\n",
    "\t\t\t\t\t\tlower = s.lower( )\n",
    "\t\t\t\t\t\tspecial = remove_special( lower )\n",
    "\t\t\t\t\t\tspace = clean_space( special )\n",
    "\t\t\t\t\t\tprocessed.append( space )\n",
    "\n",
    "\t\t\t\tdest_path = destination + '\\\\' + filename\n",
    "\t\t\t\tclean = open( dest_path, 'wt', encoding='utf-8', errors='ignore' )\n",
    "\t\t\t\tlines = ' '.join( processed )\n",
    "\t\t\t\tclean.write( lines )\n",
    "\t\t\t\tclean.flush( )\n",
    "\texcept Exception as e:\n",
    "\t\tprint( \"The 'clean_files' function raised an exception:\", e )"
   ],
   "id": "55c395d65e61a4d5",
   "outputs": [],
   "execution_count": 435
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "f6bf516d1034c3f9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-10T18:41:23.256963Z",
     "start_time": "2025-06-10T18:41:18.566225Z"
    }
   },
   "cell_type": "code",
   "source": "clean_files( src, dest )",
   "id": "97c806612288ddfb",
   "outputs": [],
   "execution_count": 442
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-07T02:10:11.830308Z",
     "start_time": "2025-06-07T02:10:11.819615Z"
    }
   },
   "cell_type": "code",
   "source": [
    "a11 = r'C:\\Users\\terry\\Desktop\\AI\\fine-tuning\\datasets\\a11.xlsx'\n",
    "cfr31 = r'C:\\Users\\terry\\Desktop\\AI\\fine-tuning\\datasets\\cfr31.xlsx'\n",
    "fastbook = r'C:\\Users\\terry\\Desktop\\AI\\fine-tuning\\datasets\\fastbook.xlsx'\n",
    "redbook = r'C:\\Users\\terry\\Desktop\\AI\\fine-tuning\\datasets\\redbook.xlsx'\n",
    "ledger = r'C:\\Users\\terry\\Desktop\\AI\\fine-tuning\\datasets\\ledger.xlsx'\n",
    "instructions = '''You are the most knowledgeable Budget Analyst in the federal government who provides detailed responses based on your vast knowledge of budget legislation, and federal appropriations. Your responses to questions about federal finance are complete, transparent, and very detailed using an academic format. Your vast knowledge of and experience in Data Science makes you the best Data Analyst in the world. You are also an expert programmer who is proficient in C#, Python, S L, C++, JavaScript, and VBA. You are famous for the accuracy of your responses so you verify all your answers. This makes the quality of your code very high and it always works. Your responses are always accurate and complete! Your name is Bubba.\n",
    "'''\n",
    "\n"
   ],
   "id": "e069c9fcfcbbd2bf",
   "outputs": [],
   "execution_count": 152
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "system = '{\"messages\": [{' + f'\"role\": \"system\", \"content\": {instructions}' + '},{'\n",
    "prologue =  system + f'\"role\": \"user\", \"content\": \"{Q}\"' + '},'\n",
    "question =  f'\"role\": \"user\", \"content\": \"{Q}\"' + '},'\n",
    "answer = '{' + f'\"role\": \"assistant\", \"content\": \"{A}\" ' + '}]}'\n",
    "end = ']}'"
   ],
   "id": "314e241553d70eb4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "15adfd8026079931"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-07T11:23:00.644936Z",
     "start_time": "2025-06-07T11:23:00.527970Z"
    }
   },
   "cell_type": "code",
   "source": [
    "xl_omba11 = pd.read_excel( a11, sheet_name='Training' )\n",
    "names = [ 'ID', 'Item', 'Role', 'Content' ]\n",
    "idx = xl_omba11.index\n",
    "df_omba11 = pd.DataFrame( data=xl_omba11, columns=names, index=idx  )\n",
    "df_omba11 = df_omba11.reset_index( ).set_index( 'ID' )\n",
    "omb_rows = len( df_omba11  )"
   ],
   "id": "d4c2b57481a2bd0e",
   "outputs": [],
   "execution_count": 165
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "xl_cfr31 = pd.read_excel( cfr31, sheet_name='Training' )\n",
    "df_cfr31 = pd.DataFrame( xl_cfr31 )\n",
    "cfr_rows = len( df_cfr31 )"
   ],
   "id": "b84b674d7baf6715"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-07T13:53:29.821008Z",
     "start_time": "2025-06-07T13:53:29.660690Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for r in range( 25 ):\n",
    "\trecord = f' \"{df_omba11.iloc[ r, 2 ]}\" ; \"{df_omba11.iloc[ r, 3 ]}\" '\n",
    "\tprint(  '{' + record + '},' )"
   ],
   "id": "8e242da675e948a4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ \"user\" ; \"What is the title and number of this OMB Circular?\" },\n",
      "{ \"assistant\" ; \"The title is â€œPreparation, Submission, and Execution of the Budgetâ€ and its number is Aâ€“11.\" },\n",
      "{ \"user\" ; \"Which Executive Branch office issued this Circular, and when?\" },\n",
      "{ \"assistant\" ; \"It was issued by the Executive Office of the President, Office of Management and Budget, in December 2019. \" },\n",
      "{ \"user\" ; \"What is the primary subject covered by Circular Aâ€“11?\" },\n",
      "{ \"assistant\" ; \" Circular Aâ€“11 provides guidance on the preparation, submission, and execution of the Federal budget. \" },\n",
      "{ \"user\" ; \"Which high-level offices does the cover page explicitly name as responsible for Circular Aâ€“11?\" },\n",
      "{ \"assistant\" ; \"The cover page specifically names the Executive Office of the President and the Office of Management and Budget as its issuers.\" },\n",
      "{ \"user\" ; \"Who issued the June 28, 2019 transmittal memorandum for Circular Aâ€“11, and what is its number?\" },\n",
      "{ \"assistant\" ; \"It was issued by Russell T. Vought, Acting Director of OMB, as Transmittal Memorandum No. 93.\" },\n",
      "{ \"user\" ; \"According to the December 18, 2019 memorandum, which Executive Orders and OMB memoranda were added or updated?\" },\n",
      "{ \"assistant\" ; \"References to Executive Order 13893 (â€œIncreasing Government Accountabilityâ€¦PAYGOâ€), OMB Memorandum M-20-06 (section 31.3), and Executive Order 13834 (â€œEfficient Federal Operations,â€ Appendix 10 of the Capital Programming Guide) were added or updated.\" },\n",
      "{ \"user\" ; \"What prohibition was removed to make Circular Aâ€“11 consistent with OMB CR Bulletin 19-04?\" },\n",
      "{ \"assistant\" ; \"The prohibition on effectuating recurring discretionary changes in mandatory programs (â€œCHIMPsâ€) on an apportionment during a continuing resolution was removed (section 120.60).\" },\n",
      "{ \"user\" ; \"Which subsection of Section 15 explains how the Congress enacts and enforces the budget?\" },\n",
      "{ \"assistant\" ; \"Subsection 15.3, â€œHow does the Congress enact the budget and how is the budget enforced?â€ on page 15-2. \" },\n",
      "{ \"user\" ; \"Where does Section 20 begin, and what is its first subsection title?\" },\n",
      "{ \"assistant\" ; \"Section 20 begins on page 20-2 with subsection 20.1 titled, â€œWhat is the purpose of this section?â€\" },\n",
      "{ \"user\" ; \"What topic does subsection 20.7 cover, and on which page does it start?\" },\n",
      "{ \"assistant\" ; \"Subsection 20.7 covers â€œWhat do I need to know about governmental receipts, offsetting collections, and offsetting receipts?â€ and starts on page 20-28.\" },\n",
      "{ \"user\" ; \"Which â€œExhibitâ€ appears under Section 20 and what topic does it cover?\" },\n",
      "{ \"assistant\" ; \"Exhibit 20 (Ex-20) covers â€œTransfers of Budgetary Resources among Federal Government Accountsâ€ and begins on page 20-45.\" },\n",
      "{ \"user\" ; \"What are the first two major sections of Circular Aâ€“11 after the transmittal memorandum?\" },\n",
      "{ \"assistant\" ; \"The first two major sections are â€œGuide to the Circularâ€ (page xxiii) and â€œSummary of Changesâ€ (page xxvii).\" },\n",
      "{ \"user\" ; \"Under Part 1â€”General Information, what is Section 10 titled, and what is its first subsection?\" },\n"
     ]
    }
   ],
   "execution_count": 168
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "xl_redbook = pd.read_excel( redbook, sheet_name='Training' )\n",
    "df_redbook = pd.DataFrame( xl_redbook )\n",
    "redbook_rows = len( df_redbook )"
   ],
   "id": "a9c71f5ebcb9cf7f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "xl_ledger = pd.read_excel( ledger, sheet_name='Training' )\n",
    "df_ledger = pd.DataFrame( xl_ledger )\n",
    "ledger_rows = len( df_ledger )"
   ],
   "id": "46e83d4d44129c27"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "249580b9e0ab02c8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "f6483f7f77ed048e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "cbcae77914e300a9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
