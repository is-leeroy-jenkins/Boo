{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# NLTK\n",
    "___\n"
   ],
   "id": "700c6019c3802579"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### ðŸ“¦ One-Time Setup (NLTK Resources)",
   "id": "17b64a5a35525229"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "import nltk\n",
    "nltk.download( 'punkt' )\n",
    "nltk.download( 'stopwords' )\n",
    "nltk.download( 'wordnet' )\n",
    "nltk.download( 'omw-1.4' )\n"
   ],
   "id": "ea993d341972b4d9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Load Dependencies",
   "id": "3957678e92123d8f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer"
   ],
   "id": "9a5c2c52dd74d494"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### ðŸ§  Full Pipeline",
   "id": "dda1854408000616"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# === Preprocessing Configuration ===\n",
    "EN_STOPWORDS = set( stopwords.words( 'english' ) )\n",
    "LEMMATIZER = WordNetLemmatizer( )\n",
    "STEMMER = SnowballStemmer( \"english\" )\n",
    "\n",
    "def preprocess_line( line, lower=True, punctuation=True,\n",
    "                     stopwords=False, lemmatize=True, stem=False ):\n",
    "    \"\"\"\n",
    "    Process a single line of text with optional steps:\n",
    "    - lower\n",
    "    - punctuation removal\n",
    "    - stopword removal\n",
    "    - lemmatization\n",
    "    - stemming (optional)\n",
    "    Returns the cleaned line as a string.\n",
    "    \"\"\"\n",
    "    tokens = word_tokenize( line )\n",
    "    processed = [ ]\n",
    "    for token in tokens:\n",
    "        if lower:\n",
    "            token = token.lower( )\n",
    "\n",
    "        if punctuation and token in string.punctuation:\n",
    "            continue\n",
    "\n",
    "        if stopwords and token in EN_STOPWORDS:\n",
    "            continue\n",
    "\n",
    "        if lemmatize:\n",
    "            token = LEMMATIZER.lemmatize( token )\n",
    "\n",
    "        if stem:\n",
    "            token = STEMMER.stem( token )\n",
    "\n",
    "        processed.append( token )\n",
    "\n",
    "    return ' '.join( processed )\n",
    "\n",
    "\n",
    "def process_file( file_path, **preprocess_kwargs ):\n",
    "    \"\"\"\n",
    "        Read a text file line-by-line, apply preprocessing pipeline to each line,\n",
    "        and return a list of cleaned lines (original order preserved).\n",
    "        Pass any keyword args supported by `preprocess_line`.\n",
    "    \"\"\"\n",
    "    cleaned_lines = []\n",
    "    with open( file_path, 'r', encoding='utf-8' ) as file:\n",
    "        for line in file:\n",
    "            cleaned = preprocess_line( line, **preprocess_kwargs )\n",
    "            cleaned_lines.append( cleaned )\n",
    "    return cleaned_lines\n"
   ],
   "id": "af7f3fd7b2990387"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### ðŸ” Usage",
   "id": "91981ee2b140d645"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "file_path = 'path/to/Public_Law_118-32.txt'\n",
    "cleaned_lines = process_file( file_path, lowercase=True, remove_punct=True,\n",
    "    remove_stopwords=False, lemmatize=True, stem=True )\n",
    "\n",
    "print( f\"Total lines: {len( cleaned_lines )}\" )\n",
    "print( \"Example cleaned line:\", cleaned_lines[ 0 ] )"
   ],
   "id": "c7c6e615a319e113"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Text Cleaner for PL 118-32",
   "id": "2463270042d983cf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Load Dependencies",
   "id": "6e0771597592ec33"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-31T19:50:18.568643Z",
     "start_time": "2025-03-31T19:50:18.559980Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "import os\n",
    "from pathlib import Path\n",
    "import openai\n",
    "import pandas as pd\n",
    "import time\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n"
   ],
   "id": "63d1f338a9f2a8ca",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 1. Load File",
   "id": "c6aca1d3fff8a8ac"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# === 1. Load the Raw Text ===\n",
    "def load_text( file_path ):\n",
    "\twith open( file_path, 'r', encoding='utf-8' ) as f:\n",
    "\t\treturn f.read( )"
   ],
   "id": "a0478bcf04eb5543"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 2. Clean Document",
   "id": "5e61d59191d25e5c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def clean_text( text: str ) -> str:\n",
    "    # Step 1: Normalize normalized\n",
    "    text = text.replace('\\r\\n', '\\n').replace('\\r', '\\n')\n",
    "\n",
    "    # Step 2: Remove page headers and footers (Public Law-specific)\n",
    "    text = re.sub(r'PUBLIC LAW 118â€“32.*?\\n', '', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\n\\s*\\d+\\s*\\n', '\\n', text)  # Remove page numbers between lines\n",
    "\n",
    "    # Step 3: Remove hyphenation at line breaks (e.g., \"appropria-\\ntion\")\n",
    "    text = re.sub(r'(\\w+)-\\n(\\w+)', r'\\1\\2', text)\n",
    "\n",
    "    # Step 4: Merge broken lines where sentence continues\n",
    "    text = re.sub(r'(?<!\\n)\\n(?![\\n])', ' ', text)\n",
    "\n",
    "    # Step 5: Collapse excessive whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "    return text.strip()\n"
   ],
   "id": "42c34418a1218398"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 3. Chunk File",
   "id": "9bf93b3631a66b6a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Simple chunking by words assuming ~1.3 words per token\n",
    "def chunk_text( text, max_tokens=512 ):\n",
    "\twords = text.split( )\n",
    "\tchunk_size = int( max_tokens * 1.3 )\n",
    "\tchunks = [ ' '.join( words[ i:i + chunk_size ] ) for i in range( 0, len( words ), chunk_size ) ]\n",
    "\treturn chunks\n"
   ],
   "id": "bdf2dd9223fede76"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### ðŸ” Example",
   "id": "842abb4747e1ed6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "file_path = 'path_to/Public_Law_118-32.txt'\n",
    "raw_text = load_text( file_path )\n",
    "cleaned_text = clean_text( raw_text )\n",
    "chunks = chunk_text( cleaned_text )\n",
    "print( f'Total Chunks: {len( chunks )}' )\n",
    "print( 'Sample chunk:\\n', chunks[ 0 ][ :1000 ] )"
   ],
   "id": "1620253dd31900a3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "fc9d08f36dde47ba"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "#  OpenAI Embedding\n",
    "___"
   ],
   "id": "9f097164e0f1b335"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### API key",
   "id": "10f8b0039c619d8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create client\n",
    "client = OpenAI( )\n",
    "client.api_key = os.getenv( 'OPENAI_API_KEY' )"
   ],
   "id": "6f7c70b192899b4b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 1. Define embedding function",
   "id": "e1c76c3548b771af"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def embed_texts( texts, model='text-embedding-3-small', batch_size=10, sleep=1 ):\n",
    "    embeddings = []\n",
    "    for i in range( 0, len( texts ), batch_size ):\n",
    "        batch = texts[ i:i+batch_size ]\n",
    "        try:\n",
    "            response = openai.embeddings.create( input=batch, model=model )\n",
    "            batch_embeddings = [ e.embedding for e in response.data ]\n",
    "            embeddings.extend( batch_embeddings )\n",
    "        except Exception as e:\n",
    "            print( f'Error at batch {i}: {e}' )\n",
    "            # Retry or sleep to avoid rate limits\n",
    "            time.sleep( sleep )\n",
    "            continue\n",
    "\n",
    "    return embeddings\n"
   ],
   "id": "3d19dce1b72312ca"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 2. Embed chunks",
   "id": "7a1cbc3787a81ed2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 2. Embed chunks\n",
    "embeddings = embed_texts( chunks )"
   ],
   "id": "2e8dd3b9c7453b0c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 3.  Create DataFrame",
   "id": "7b29c98e70fdc973"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 3. Create DataFrame\n",
    "df_embeddings = pd.DataFrame( { chunks, embeddings } )\n"
   ],
   "id": "998cc4a4eb1c122f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 3. Save\n",
   "id": "ca13ccc289f545b4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 3. Save\n",
    "df_embeddings.to_parquet( 'public_law_118_32_embeddings.parquet', index=False )\n"
   ],
   "id": "eac78df8b7b91593"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 4. Preview",
   "id": "ae06c79ed3506313"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 4. Preview\n",
    "df_embeddings.head(2)"
   ],
   "id": "e9935e9a89b6aac5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Text Cleaning\n",
    "___"
   ],
   "id": "1f2312a84c08528a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Load Dependencies",
   "id": "b6af68746614991a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T01:13:42.764759Z",
     "start_time": "2025-04-03T01:13:42.698497Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import re\n",
    "import sentence_transformers\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "import pickle"
   ],
   "id": "27a64e0eee5267f6",
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sentence_transformers'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[3], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mlangchain\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtext_splitter\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mre\u001B[39;00m\n\u001B[1;32m----> 3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01msentence_transformers\u001B[39;00m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01msqlite3\u001B[39;00m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mnumpy\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mnp\u001B[39;00m\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'sentence_transformers'"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 1.  Strip irrelevant content  while preserving structure.",
   "id": "f57da739bd434166"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def clean_text( text ):\n",
    "    # Remove form feeds, line breaks, etc.\n",
    "    text = re.sub( r'\\f+', ' ', text )\n",
    "    text = re.sub( r'\\n+', ' ', text )\n",
    "    text = re.sub( r'\\s{2,}', ' ', text )\n",
    "\n",
    "    # Normalize Section markers, Title headers, etc.\n",
    "    text = re.sub( r'SECTION\\.\\s+(\\d+)\\.', r'Section \\1:', text )\n",
    "    return text.strip( )\n"
   ],
   "id": "936a3035cb982382"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 2. Chunk the Text",
   "id": "8c21a4c303c952e7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- Large documents need to be chunked (for context window limits during embedding).\n",
    "- Use semantic or structural chunking."
   ],
   "id": "19cbc1c90cb94e79"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "\n",
    "seps =[ '\\n\\n', '\\n', '.', ' ' ]\n",
    "splitter = RecursiveCharacterTextSplitter( chunk_size=1000, chunk_overlap=200, separators=seps )\n",
    "cleaned = clean_text( raw_text )\n",
    "chunks = splitter.split_text( cleaned )\n"
   ],
   "id": "40beb2e1042742e5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 3. Generate Embeddings",
   "id": "120bc6a0a62a9729"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- Use a language model (e.g., OpenAI, HuggingFace) to create vector representations of each chunk.",
   "id": "f62aab776049a508"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "\n",
    "model = SentenceTransformer( 'all-MiniLM-L6-v2' )\n",
    "embeddings = model.encode( chunks, show_progress_bar=True )\n"
   ],
   "id": "a67fd85d25aafd1c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 4. Create SQLite Database",
   "id": "d76b8831817cd4d9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- Design a table that links text chunks to their embeddings.",
   "id": "f06349ccd853f770"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "conn = sqlite3.connect( 'embeddings.db' )\n",
    "cursor = conn.cursor( )\n",
    "sql_create = '''\n",
    "CREATE TABLE IF NOT EXISTS law_embeddings\n",
    "(\n",
    "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    chunk TEXT NOT NULL,\n",
    "    embedding BLOB NOT NULL\n",
    ")\n",
    "'''\n",
    "\n",
    "cursor.execute( sql_create )\n",
    "\n",
    "for chunk, vector in zip( chunks, embeddings ):\n",
    "    blob = pickle.dumps( vector )\n",
    "    cursor.execute( 'INSERT INTO law_embeddings ( chunk, embedding ) VALUES (?, ?)', ( chunk, blob ) )\n",
    "\n",
    "conn.commit( )\n",
    "conn.close( )\n"
   ],
   "id": "28ca2f108b874b20"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "####  Retrieval (Vector Search in SQLite)",
   "id": "ca5ad81f911f7250"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- You can perform semantic search by encoding a query and comparing via cosine similarity:",
   "id": "12113e4d3566bd2f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def cosine_similarity( a, b ):\n",
    "    return np.dot( a, b ) / ( np.linalg.norm( a ) * np.linalg.norm( b ) )"
   ],
   "id": "70665fe56a643b3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "query = 'Appropriations for Department of Defense'\n",
    "query_vec = model.encode( [ query ] )[ 0 ]\n",
    "\n",
    "conn = sqlite3.connect( 'embeddings.db' )\n",
    "cursor = conn.cursor( )\n",
    "cursor.execute( 'SELECT id, chunk, embedding FROM law_embeddings' )\n",
    "\n",
    "results = []\n",
    "for row in cursor.fetchall( ):\n",
    "    chunk_id, chunk_text, blob = row\n",
    "    stored_vec = pickle.loads( blob )\n",
    "    sim = cosine_similarity( query_vec, stored_vec )\n",
    "    results.append( ( sim, chunk_text ) )\n",
    "\n",
    "# Sort and get top N\n",
    "top_matches = sorted( results, key=lambda x: x[ 0 ], reverse=True )[ :5 ]\n"
   ],
   "id": "675d18dac5bd124d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#   Embedding-Pipeline Script\n",
    "___"
   ],
   "id": "3a7f735556cd0d3f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Load Dependencies\n",
    "\n"
   ],
   "id": "e593513dcb937f5f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-31T20:28:22.663413Z",
     "start_time": "2025-03-31T20:28:22.654167Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "import sqlite3\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import openai\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ],
   "id": "cf12b8bc356c9181",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Configuration",
   "id": "99ef339bdfea1fd8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "a764c56d86acecd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Define paths\n",
    "TEXT_FILE = 'PublicLaw_118-42.txt'\n",
    "DB_FILE = 'law_embeddings.db'\n",
    "EMBEDDING_MODEL = 'all-MiniLM-L6-v2'\n",
    "CHUNK_SIZE = 1000\n",
    "CHUNK_OVERLAP = 200\n"
   ],
   "id": "e8ee553da46c407c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Load and Clean Raw Text",
   "id": "842a900a1b80cde5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def load_and_clean_text( filepath ):\n",
    "    with open( filepath, 'r', encoding='utf-8' ) as file:\n",
    "        raw_text = file.read( )\n",
    "\n",
    "    # Basic normalization\n",
    "    text = re.sub( r'\\f+', ' ', raw_text )\n",
    "    text = re.sub( r'\\n+', ' ', text )\n",
    "    text = re.sub( r'\\s{2,}', ' ', text )\n",
    "    return text.strip( )"
   ],
   "id": "c202cc689865f32"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Chunk the Clean Text\n",
   "id": "e4eb076550224fc5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def chunk_text( clean_text ):\n",
    "    splitter = RecursiveCharacterTextSplitter( chunk_size=1000, chunk_overlap=200,\n",
    "\t    separators=[ '\\n\\n', '\\n', '.', ' ' ] )\n",
    "\n",
    "    return splitter.split_text( clean_text )\n"
   ],
   "id": "e77fae8c30040047"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Generate Embeddings",
   "id": "ae3ddba5e85404c6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def get_embedding( text, model=OPENAI_MODEL ):\n",
    "    response = openai.Embedding.create( input=text, model=model )\n",
    "    return response[ 'data' [ 0 ][ 'embedding' ] ]\n",
    "\n",
    "\n",
    "def embed_chunks( chunks ):\n",
    "    embeddings = [ ]\n",
    "    for chunk in tqdm( chunks, desc='EmbeddingRequest chunks via OpenAI' ):\n",
    "        try:\n",
    "            embedding = get_embedding( chunk )\n",
    "            embeddings.append( embedding )\n",
    "        except Exception as e:\n",
    "            print( f'Error embedding chunk: {e}' )\n",
    "            embeddings.append( [ 0.0 ] * 1536 )  # Placeholder for failed requests\n",
    "    return embeddings"
   ],
   "id": "8fa7587e7a3094be"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Create SQLite DB\n",
    "\n"
   ],
   "id": "3c985e6d3ecd5e8f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def create_and_populate_db( chunks, embeddings, db_path ):\n",
    "    conn = sqlite3.connect( db_path )\n",
    "    cursor = conn.cursor( )\n",
    "    sql_create = '''\n",
    "    CREATE TABLE IF NOT EXISTS law_embeddings\n",
    "    (\n",
    "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "        chunk TEXT NOT NULL,\n",
    "        embedding BLOB NOT NULL\n",
    "    )\n",
    "    '''\n",
    "\n",
    "    cursor.execute( sql_create )\n",
    "    for chunk, vector in zip( chunks, embeddings ):\n",
    "        blob = pickle.dumps( vector )\n",
    "        sql_insert = 'INSERT INTO law_embeddings ( chunk, embedding ) VALUES ( ?, ? )'\n",
    "        cursor.execute( sql_insert, ( chunk, blob ) )\n",
    "\n",
    "    conn.commit( )\n",
    "    conn.close( )\n"
   ],
   "id": "ea15943e543e0d70"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Script",
   "id": "e82751929d4fd563"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# === MAIN ===\n",
    "def main():\n",
    "    print('Step 1: Load and clean text')\n",
    "    cleaned_text = load_and_clean_text(TEXT_FILE)\n",
    "\n",
    "    print('Step 2: Chunking text')\n",
    "    chunks = chunk_text(cleaned_text)\n",
    "    print(f'Total chunks: {len(chunks)}')\n",
    "\n",
    "    print('Step 3: EmbeddingRequest with OpenAI API')\n",
    "    embeddings = embed_chunks(chunks)\n",
    "\n",
    "    print('Step 4: Saving to SQLite')\n",
    "    create_and_populate_db(chunks, embeddings, DB_FILE)\n",
    "\n",
    "    print(f'Pipeline complete. Embeddings stored in: {DB_FILE}')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ],
   "id": "5f2d269aaa88212b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "E",
   "id": "de305bd9c7ccbd96"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# === 1. Load Model ===\n",
    "# You can try other models like 'all-MiniLM-L6-v2', 'all-mpnet-base-v2', or 'multi-qa-MiniLM-L6-cos-v1'\n",
    "model = SentenceTransformer( 'all-MiniLM-L6-v2' )\n",
    "\n",
    "# === 2. Embed Chunks ===\n",
    "def embed_with_sentence_transformers( texts, model ):\n",
    "    return model.encode( texts, show_progress_bar=True, convert_to_numpy=True )\n",
    "\n",
    "local_embeddings = embed_with_sentence_transformers( chunks, model )\n",
    "\n",
    "# === 3. Save in a DataFrame ===\n",
    "df_local = pd.DataFrame({\n",
    "    'chunk': chunks,\n",
    "    'embedding': list( local_embeddings )  # numpy arrays to list for DataFrame compatibility\n",
    "})\n",
    "\n",
    "# === 4. Save to Disk ===\n",
    "df_local.to_parquet( 'public_law_118_32_local_embeddings.parquet', index=False )\n",
    "\n",
    "# === 5. Preview ===\n",
    "df_local.head( 2 )\n",
    "\n"
   ],
   "id": "47df028a12155604"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Fine-Tuning",
   "id": "e1efd28f65f33d39"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import json\n",
    "import openai\n",
    "import os\n",
    "import pandas as pd\n",
    "from pprint import pprint\n"
   ],
   "id": "51f033fc98145091"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "client = openai.OpenAI(\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    "    organization=\"<org id>\",\n",
    "    project=\"<project id>\",\n",
    ")"
   ],
   "id": "b11d00974dbaa480"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Read in the dataset we'll use for this task.\n",
    "# This will be the RecipesNLG dataset, which we've cleaned to only contain documents from www.cookbooks.com\n",
    "recipe_df = pd.read_csv(\"data/cookbook_recipes_nlg_10k.csv\")\n",
    "\n",
    "recipe_df.head()"
   ],
   "id": "aa84f0c255158f4f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "system_message = \"You are a helpful recipe assistant. You are to extract the generic ingredients from each of the recipes provided.\"\n",
    "\n",
    "\n",
    "def create_user_message(row):\n",
    "    return f\"Title: {row['title']}\\n\\nIngredients: {row['ingredients']}\\n\\nGeneric ingredients: \"\n",
    "\n",
    "\n",
    "def prepare_example_conversation(row):\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": system_message},\n",
    "            {\"role\": \"user\", \"content\": create_user_message(row)},\n",
    "            {\"role\": \"assistant\", \"content\": row[\"NER\"]},\n",
    "        ]\n",
    "    }\n",
    "\n"
   ],
   "id": "d29e9579f6f0e9dd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# use the first 100 rows of the dataset for training\n",
    "training_df = recipe_df.loc[0:100]\n",
    "\n",
    "# apply the prepare_example_conversation function to each row of the training_df\n",
    "training_data = training_df.apply(prepare_example_conversation, axis=1).tolist()\n",
    "\n",
    "for example in training_data[:5]:\n",
    "    print(example)"
   ],
   "id": "23bdc89006166778"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "validation_df = recipe_df.loc[101:200]\n",
    "validation_data = validation_df.apply(\n",
    "    prepare_example_conversation, axis=1).tolist()"
   ],
   "id": "4773228f51c07750"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def write_jsonl(data_list: list, filename: str) -> None:\n",
    "    with open(filename, \"w\") as out:\n",
    "        for ddict in data_list:\n",
    "            jout = json.dumps(ddict) + \"\\n\"\n",
    "            out.write(jout)"
   ],
   "id": "6dcf445d39eee404"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "652e2134d8966009"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "training_file_name = \"tmp_recipe_finetune_training.jsonl\"\n",
    "write_jsonl(training_data, training_file_name)\n",
    "\n",
    "validation_file_name = \"tmp_recipe_finetune_validation.jsonl\"\n",
    "write_jsonl(validation_data, validation_file_name)"
   ],
   "id": "a492a14231147287"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "170bad89408062ff"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def upload_file(file_name: str, purpose: str) -> str:\n",
    "    with open(file_name, \"rb\") as file_fd:\n",
    "        response = client.files.create(file=file_fd, purpose=purpose)\n",
    "    return response.id"
   ],
   "id": "10f941082775954c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "751aceb620d9c7bb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "training_file_id = upload_file(training_file_name, \"fine-tune\")\n",
    "validation_file_id = upload_file(validation_file_name, \"fine-tune\")"
   ],
   "id": "c8dbd2395ef59d03"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "feadf4f400f8454c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "MODEL = \"gpt-4o-mini-2024-07-18\"\n",
    "\n",
    "response = client.fine_tuning.jobs.create(\n",
    "    training_file=training_file_id,\n",
    "    validation_file=validation_file_id,\n",
    "    model=MODEL,\n",
    "    suffix=\"recipe-ner\",\n",
    ")\n",
    "\n",
    "job_id = response.id"
   ],
   "id": "5ca162663d00cad3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
