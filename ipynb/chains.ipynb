{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# LangChain",
   "id": "926d4a83c7c0514a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### This document assumes you have a some understanding about the following:\n",
    "- Python and Object Oriented Programming\n",
    "- NLP concepts such as embeddings\n",
    "- somewhat know how LLMs work\n",
    "- know a bit about VectorDBs\n",
    "\n",
    "##### Explicitly used packages:\n",
    "- Langchain\n",
    "- Transformers\n",
    "##### Implicitly used packages:\n",
    "\n",
    "- ChromaDB\n",
    "- openai\n",
    "- sentence-transformers"
   ],
   "id": "cd0dae5f0dd7a6a6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Load Dependencies",
   "id": "3698886eb601a245"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.agents import load_tools, initialize_agent, create_sql_agent\n",
    "from langchain.agents.agent_types import AgentType\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.chains import RetrievalQA, ConversationChain\n",
    "from langchain.memory import ConversationBufferMemoryimport\n",
    "import sqlite3\n",
    "from langchain.agents.agent_toolkits import SQLDatabaseToolkit\n",
    "from langchain.sql_database import SQLDatabase\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.agents.agent_types import AgentType"
   ],
   "id": "3476ab4f2b357d0c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "1. Start with an object of the OpenAI class and set some parameters.\n",
    "2. Here, we set up the parameter called temperature.\n",
    "3. Having a lower temperature value ensures that our LLM output is deterministic in nature and not too random and \"creative\".\n",
    "4. Call the instance (object) of the OpenAI class \"llm\""
   ],
   "id": "34430b81e345d256"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## API",
   "id": "810fe52d8b3ccd1c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 1. LLMs\n",
    "- Abstraction around different model providers."
   ],
   "id": "6b4e3f3c18c79c5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "api_key = os.getenv( 'OPENAI_API_KEY' )",
   "id": "7ca6501e475a1165"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Create OpenAI object",
   "id": "a8529c5d9d3f2450"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "llm = OpenAI( model_name='gpt-4o', temperature=0.8 )\n",
    "response = llm( 'What is the capital of France?' )\n",
    "print( response )"
   ],
   "id": "e01abedf76f553d0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### 1. Load the documents using TextLoader from LangChain.\n",
    "##### 2. Define a variable to store the documents"
   ],
   "id": "ac25bdbc88d4fa76"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "data = []",
   "id": "dd90896addd4f6c6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### 1. Load the documents using TextLoader from LangChain.\n",
    "##### 2. Define a variable to store the documents"
   ],
   "id": "d610cf7292609660"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- We can append the output of loader.load() into our list variable",
   "id": "2363e3a20d478ca3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "loader = TextLoader(r'C:\\Users\\JkReddy\\Desktop\\Weill Cornell Medicine\\Subjects\\Capstone\\LangChain.txt')\n",
    "data.append( loader.load( )[ 0 ] )"
   ],
   "id": "763eba75e91511c6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- Importing Vector DB - Chroma along with TextSplitter and QA related packages from LangChain.\n",
    "- Import package for Embeddings from OpenAI"
   ],
   "id": "c47d6837f3a4c46c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Split the lengthy document into smaller chunks.\n",
    "- This is done because LLMs have a limit to the number of words they can take in as input, and they can retain more information if they have fewer words to work with.\n",
    "- Chunk overlap parameter dictates how much overlap should exist between the chunks.\n",
    "- Having more of an overlap ensures that important information is not lost during the splitting process.\n",
    "- Split documents function takes in a list as an input.\n",
    "- Each list element must contain a document loaded in by Langchain"
   ],
   "id": "c417b65d624c88c6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "text_splitter = CharacterTextSplitter( chunk_size = 1000, chunk_overlap=200 )\n",
    "texts = text_splitter.split_documents( data )"
   ],
   "id": "c358b394c1aa2135"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "1. Creating database and getting the embedding functions ready.\n",
    "2. Store the docs in the vector db.\n",
    "\n",
    "- In the below cell, we mention a directory in which we want our vector db to reside.\n",
    "- This is then followed by the creation of an embeddings object from the OpenAIEmbeddings class from the Langchain package.\n",
    "- This can used for creating the doc embeddings"
   ],
   "id": "2a02d2427a2d4c99"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Embed and store the texts\n",
    "# Supplying a persist_directory will store the embeddings on disk\n",
    "persist_directory = 'myvectordb'\n",
    "embeddings = OpenAIEmbeddings( )"
   ],
   "id": "6588ecef07dc9e90"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### By default, the model used for embeddings is the text-ada-embeddings-002\n",
    "\n",
    "- The vector db is Chroma DB which is integrated into LangChain\n",
    "- The Chroma.from_documents function takes in these parameters:\n",
    "\n",
    "##### The split up texts\n",
    "- Embedding instance from Langchain\n",
    "- Directory in which we want the persistence of our db to be asserted"
   ],
   "id": "efc29f30fb96579f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "vectordb = Chroma.from_documents( texts, embeddings, persist_directory = persist_directory )",
   "id": "a682ca95c9d76ce1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- Write Embeddings to a disk using db.persist() and wiping it clean. Reload again to test if it has been stored",
   "id": "da645482cb36e941"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "vectordb.persist()\n",
    "vectordb = None"
   ],
   "id": "2e3e5f6406ccd102"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- Reload",
   "id": "7a9e1fb0eba8ad7e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "vectordb = Chroma( persist_directory=persist_directory, embedding_function = embeddings )",
   "id": "9ef90eeb057677a5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- Create object with LLM model being passed as a parameter along with temperature parameter for controlling the nature of the LLM output.\n",
    "- OpenAI class also takes in model_name as a parameter."
   ],
   "id": "697c1d6a1b903d8d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "gpt_qa = RetrievalQA.from_chain_type( llm=OpenAI(temperature = 0.8, model_name = 'text-ada-embeddings-002' ),\n",
    "                                 chain_type = \"stuff\",\n",
    "                                 retriever = vectordb.as_retriever())"
   ],
   "id": "b1e0a9b5601a2cca"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- Our newly created qa object has the function query using which we can run a query over the db.\n",
    "- Once the right doc chunk has been retrieved, it is passed to the llm along with the query."
   ],
   "id": "e89e8897a311fcf3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "query = \"What can I eat?\"\n",
    "gpt_qa.run(query)"
   ],
   "id": "51819f83062365d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- Creating an instance of an Open Source Embedding",
   "id": "43564fecb82b1b7b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain import HuggingFacePipeline\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "embeddings = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)\n",
    "text_splitter = CharacterTextSplitter(chunk_size = 500, chunk_overlap=50)\n",
    "texts = text_splitter.split_documents(data)\n",
    "persist_directory = 'myvectordb_opensource'\n",
    "vectordb = Chroma.from_documents(texts, embeddings, persist_directory = persist_directory)\n",
    "vectordb.persist()"
   ],
   "id": "decdfd3e176195b9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- Langchain HF pipeline only supports models in the hub which function as text2text gen or text gen models.",
   "id": "dbd196446897a5e5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "llm = HuggingFacePipeline.from_model_id(model_id=\"declare-lab/flan-gpt4all-xl\",\n",
    "                                        task=\"text2text-generation\",\n",
    "                                        model_kwargs={\"temperature\":0, \"max_length\":50, \"min_length\":10})"
   ],
   "id": "f4512c3d8870cc5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "qa = RetrievalQA.from_chain_type(llm,\n",
    "                                 chain_type = \"refine\",\n",
    "                                 retriever = vectordb.as_retriever())"
   ],
   "id": "939856dcddd8d350"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "query = \"Can I have fruits?\"\n",
    "qa.run(query)"
   ],
   "id": "17e2951dfbde2868"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1. LLMs\n",
    "- Abstraction around different model providers."
   ],
   "id": "da65ffcfda626239"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "llm = OpenAI( model_name='gpt-4o', temperature=0.8 )\n",
    "response = llm( 'What is the capital of France?' )\n",
    "print( response )"
   ],
   "id": "a2c4436de50301ec"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2. Prompt Templates\n",
    "- Reusable templates for structured prompting."
   ],
   "id": "3af2da89a3be6b24"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "template = \"What is a good name for a company that makes {product}?\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "filled_prompt = prompt.format(product=\"smart shoes\")\n",
    "print(filled_prompt)"
   ],
   "id": "4b92f727570f0477"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3. Chains\n",
    "- Sequence of calls (LLMs, tools, functions). Most basic: LLMChain."
   ],
   "id": "6472e7b89f985cef"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "chain = LLMChain( llm=llm, prompt=prompt )\n",
    "print( chain.run( product=\"AI-powered drones\" ) )"
   ],
   "id": "615b57c0f3fed388"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 4. Agents\n",
    "- Dynamically select tools based on user input using a \"reasoning\" loop."
   ],
   "id": "ea5a2168b3a5ce51"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "tools = load_tools( [ \"serpapi\", \"llm-math\" ], llm=llm )\n",
    "agent = initialize_agent(\n",
    "    tools,\n",
    "    llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "agent.run( \"What is the square root of the population of Canada?\" )"
   ],
   "id": "dcacb2704550543e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 5. Vector Stores + RAG\n",
    "- Build question-answering systems over your own documents."
   ],
   "id": "865c692068b8f14"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Load and split\n",
    "loader = TextLoader(\"my_docs.txt\")\n",
    "docs = loader.load()\n",
    "splitter = RecursiveCharacterTextSplitter( chunk_size=500, chunk_overlap=50 )\n",
    "chunks = splitter.split_documents(docs)\n",
    "\n",
    "# Embed and store\n",
    "embedding = OpenAIEmbeddings()\n",
    "db = FAISS.from_documents(chunks, embedding)\n",
    "\n",
    "# Ask a question\n",
    "\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(llm=llm, retriever=db.as_retriever())\n",
    "qa.run(\"Summarize the key points from the document\")"
   ],
   "id": "63e8511914ee61da"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 6. Memory\n",
    "- Preserves context between turns of conversation."
   ],
   "id": "6d94edf6404c0d36"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "memory = ConversationBufferMemory( )\n",
    "conv_chain = ConversationChain( llm=llm, memory=memory )\n",
    "\n",
    "print(conv_chain.run( \"Hi, I'm working on an AI project.\" ) )\n",
    "print(conv_chain.run( \"What did I say my project was about?\" ) )"
   ],
   "id": "b2bff57742a69792"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### SQLite Integration Script",
   "id": "ed84ff5faea4bfe3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# STEP 1: Set your OpenAI API key\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your-openai-key\"\n",
    "\n",
    "# STEP 2: Create and populate a SQLite database\n",
    "conn = sqlite3.connect(\"people.db\")\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(\"DROP TABLE IF EXISTS employees\")\n",
    "cursor.execute(\"\"\"\n",
    "    CREATE TABLE employees (\n",
    "        id INTEGER PRIMARY KEY,\n",
    "        name TEXT,\n",
    "        title TEXT,\n",
    "        department TEXT,\n",
    "        salary INTEGER\n",
    "    )\n",
    "\"\"\")\n",
    "cursor.executemany( \"INSERT INTO employees ( name, title, department, salary ) VALUES (?, ?, ?, ?)\", [\n",
    "    (\"Alice Johnson\", \"Engineer\", \"R&D\", 90000),\n",
    "    (\"Bob Smith\", \"Manager\", \"HR\", 85000),\n",
    "    (\"Charlie Kim\", \"Analyst\", \"Finance\", 75000),\n",
    "    (\"Diana Lopez\", \"Engineer\", \"R&D\", 95000)\n",
    "])\n",
    "conn.commit()\n",
    "conn.close()\n",
    "\n",
    "# STEP 3: Connect LangChain to the database\n",
    "db = SQLDatabase.from_uri(\"sqlite:///people.db\")\n",
    "\n",
    "# STEP 4: Create the agent with SQL toolkit\n",
    "llm = ChatOpenAI(temperature=0, model=\"gpt-4\")\n",
    "\n",
    "toolkit = SQLDatabaseToolkit(db=db, llm=llm)\n",
    "\n",
    "agent_executor = create_sql_agent(\n",
    "    llm=llm,\n",
    "    toolkit=toolkit,\n",
    "    agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# STEP 5: Ask natural language questions\n",
    "\n",
    "response = agent_executor.run(\"Which employees in R&D earn more than 90000?\")\n",
    "print(response)\n"
   ],
   "id": "7d762e916cefbad6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Retrieval Script",
   "id": "8b54d1bd69ad4d52"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import sqlite3\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.agents import initialize_agent, Tool\n",
    "from langchain.agents.agent_types import AgentType\n",
    "from langchain.agents.agent_toolkits import SQLDatabaseToolkit\n",
    "from langchain.sql_database import SQLDatabase\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Set your OpenAI key\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your-openai-key\"\n",
    "\n",
    "# STEP 1: Create SQLite database\n",
    "conn = sqlite3.connect(\"people.db\")\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(\"DROP TABLE IF EXISTS employees\")\n",
    "cursor.execute(\"\"\"\n",
    "    CREATE TABLE employees (\n",
    "        id INTEGER PRIMARY KEY,\n",
    "        name TEXT,\n",
    "        title TEXT,\n",
    "        department TEXT,\n",
    "        salary INTEGER\n",
    "    )\n",
    "\"\"\")\n",
    "cursor.executemany(\"INSERT INTO employees (name, title, department, salary) VALUES (?, ?, ?, ?)\", [\n",
    "    (\"Alice Johnson\", \"Engineer\", \"R&D\", 90000),\n",
    "    (\"Bob Smith\", \"Manager\", \"HR\", 85000),\n",
    "    (\"Charlie Kim\", \"Analyst\", \"Finance\", 75000),\n",
    "    (\"Diana Lopez\", \"Engineer\", \"R&D\", 95000)\n",
    "])\n",
    "conn.commit()\n",
    "conn.close()\n",
    "\n",
    "# STEP 2: Create SQL database object\n",
    "sql_db = SQLDatabase.from_uri(\"sqlite:///people.db\")\n",
    "llm = ChatOpenAI(temperature=0, model=\"gpt-4\")\n",
    "\n",
    "# STEP 3: Load documents and create vector store\n",
    "loader = TextLoader(\"my_notes.txt\")  # Replace with your own file\n",
    "documents = loader.load()\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "docs = splitter.split_documents(documents)\n",
    "embedding = OpenAIEmbeddings()\n",
    "vectordb = FAISS.from_documents(docs, embedding)\n",
    "retriever = vectordb.as_retriever()\n",
    "\n",
    "# STEP 4: Create RetrievalQA chain\n",
    "doc_qa = RetrievalQA.from_chain_type(llm=llm, retriever=retriever)\n",
    "\n",
    "# STEP 5: Register tools for agent\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"SQL_Database\",\n",
    "        func=SQLDatabaseToolkit(db=sql_db, llm=llm).get_tools()[0].func,\n",
    "        description=\"Useful for answering questions about employees, departments, and salaries.\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"Document_QA\",\n",
    "        func=doc_qa.run,\n",
    "        description=\"Useful for answering questions about policy notes and general knowledge from documents.\"\n",
    "    )\n",
    "]\n",
    "\n",
    "# STEP 6: Initialize hybrid agent\n",
    "agent = initialize_agent(\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# STEP 7: Ask unified questions\n",
    "print(agent.run(\"Who in R&D earns over $90k?\"))  # SQL tool\n",
    "print(agent.run(\"Summarize the key points from the notes\"))  # Document tool"
   ],
   "id": "2e5bcef131e18204"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
