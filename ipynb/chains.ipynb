{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Dependencies",
   "id": "d2243af1f49f649d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.agents import load_tools, initialize_agent, create_sql_agent\n",
    "from langchain.agents.agent_types import AgentType\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.chains import RetrievalQA, ConversationChain\n",
    "from langchain.memory import ConversationBufferMemoryimport\n",
    "import sqlite3\n",
    "from langchain.agents.agent_toolkits import SQLDatabaseToolkit\n",
    "from langchain.sql_database import SQLDatabase\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.agents.agent_types import AgentType\n",
    "import os\n",
    "\n",
    "\n"
   ],
   "id": "7409f2c0f4525b17"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "abca4a7a8521309c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1. LLMs\n",
    "- Abstraction around different model providers."
   ],
   "id": "da65ffcfda626239"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "llm = OpenAI( model_name='gpt-4', temperature=0.8 )\n",
    "response = llm( 'What is the capital of France?' )\n",
    "print( response )"
   ],
   "id": "a2c4436de50301ec"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2. Prompt Templates\n",
    "- Reusable templates for structured prompting."
   ],
   "id": "3af2da89a3be6b24"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "template = \"What is a good name for a company that makes {product}?\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "filled_prompt = prompt.format(product=\"smart shoes\")\n",
    "print(filled_prompt)"
   ],
   "id": "4b92f727570f0477"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3. Chains\n",
    "- Sequence of calls (LLMs, tools, functions). Most basic: LLMChain."
   ],
   "id": "6472e7b89f985cef"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "chain = LLMChain( llm=llm, prompt=prompt )\n",
    "print( chain.run( product=\"AI-powered drones\" ) )"
   ],
   "id": "615b57c0f3fed388"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 4. Agents\n",
    "- Dynamically select tools based on user input using a \"reasoning\" loop."
   ],
   "id": "ea5a2168b3a5ce51"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "tools = load_tools( [ \"serpapi\", \"llm-math\" ], llm=llm )\n",
    "agent = initialize_agent(\n",
    "    tools,\n",
    "    llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "agent.run( \"What is the square root of the population of Canada?\" )"
   ],
   "id": "dcacb2704550543e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 5. Vector Stores + RAG\n",
    "- Build question-answering systems over your own documents."
   ],
   "id": "865c692068b8f14"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Load and split\n",
    "loader = TextLoader(\"my_docs.txt\")\n",
    "docs = loader.load()\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "chunks = splitter.split_documents(docs)\n",
    "\n",
    "# Embed and store\n",
    "embedding = OpenAIEmbeddings()\n",
    "db = FAISS.from_documents(chunks, embedding)\n",
    "\n",
    "# Ask a question\n",
    "\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(llm=llm, retriever=db.as_retriever())\n",
    "qa.run(\"Summarize the key points from the document\")"
   ],
   "id": "63e8511914ee61da"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 6. Memory\n",
    "- Preserves context between turns of conversation."
   ],
   "id": "6d94edf6404c0d36"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "memory = ConversationBufferMemory( )\n",
    "conv_chain = ConversationChain( llm=llm, memory=memory )\n",
    "\n",
    "print(conv_chain.run( \"Hi, I'm working on an AI project.\" ) )\n",
    "print(conv_chain.run( \"What did I say my project was about?\" ) )"
   ],
   "id": "b2bff57742a69792"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### SQLite Integration Script",
   "id": "ed84ff5faea4bfe3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "\n",
    "# STEP 1: Set your OpenAI API key\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your-openai-key\"\n",
    "\n",
    "# STEP 2: Create and populate a SQLite database\n",
    "conn = sqlite3.connect(\"people.db\")\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(\"DROP TABLE IF EXISTS employees\")\n",
    "cursor.execute(\"\"\"\n",
    "    CREATE TABLE employees (\n",
    "        id INTEGER PRIMARY KEY,\n",
    "        name TEXT,\n",
    "        title TEXT,\n",
    "        department TEXT,\n",
    "        salary INTEGER\n",
    "    )\n",
    "\"\"\")\n",
    "cursor.executemany( \"INSERT INTO employees ( name, title, department, salary ) VALUES (?, ?, ?, ?)\", [\n",
    "    (\"Alice Johnson\", \"Engineer\", \"R&D\", 90000),\n",
    "    (\"Bob Smith\", \"Manager\", \"HR\", 85000),\n",
    "    (\"Charlie Kim\", \"Analyst\", \"Finance\", 75000),\n",
    "    (\"Diana Lopez\", \"Engineer\", \"R&D\", 95000)\n",
    "])\n",
    "conn.commit()\n",
    "conn.close()\n",
    "\n",
    "# STEP 3: Connect LangChain to the database\n",
    "db = SQLDatabase.from_uri(\"sqlite:///people.db\")\n",
    "\n",
    "# STEP 4: Create the agent with SQL toolkit\n",
    "llm = ChatOpenAI(temperature=0, model=\"gpt-4\")\n",
    "\n",
    "toolkit = SQLDatabaseToolkit(db=db, llm=llm)\n",
    "\n",
    "agent_executor = create_sql_agent(\n",
    "    llm=llm,\n",
    "    toolkit=toolkit,\n",
    "    agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# STEP 5: Ask natural language questions\n",
    "\n",
    "response = agent_executor.run(\"Which employees in R&D earn more than 90000?\")\n",
    "print(response)\n"
   ],
   "id": "7d762e916cefbad6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Retrieval Script",
   "id": "8b54d1bd69ad4d52"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import sqlite3\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.agents import initialize_agent, Tool\n",
    "from langchain.agents.agent_types import AgentType\n",
    "from langchain.agents.agent_toolkits import SQLDatabaseToolkit\n",
    "from langchain.sql_database import SQLDatabase\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Set your OpenAI key\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your-openai-key\"\n",
    "\n",
    "# STEP 1: Create SQLite database\n",
    "conn = sqlite3.connect(\"people.db\")\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(\"DROP TABLE IF EXISTS employees\")\n",
    "cursor.execute(\"\"\"\n",
    "    CREATE TABLE employees (\n",
    "        id INTEGER PRIMARY KEY,\n",
    "        name TEXT,\n",
    "        title TEXT,\n",
    "        department TEXT,\n",
    "        salary INTEGER\n",
    "    )\n",
    "\"\"\")\n",
    "cursor.executemany(\"INSERT INTO employees (name, title, department, salary) VALUES (?, ?, ?, ?)\", [\n",
    "    (\"Alice Johnson\", \"Engineer\", \"R&D\", 90000),\n",
    "    (\"Bob Smith\", \"Manager\", \"HR\", 85000),\n",
    "    (\"Charlie Kim\", \"Analyst\", \"Finance\", 75000),\n",
    "    (\"Diana Lopez\", \"Engineer\", \"R&D\", 95000)\n",
    "])\n",
    "conn.commit()\n",
    "conn.close()\n",
    "\n",
    "# STEP 2: Create SQL database object\n",
    "sql_db = SQLDatabase.from_uri(\"sqlite:///people.db\")\n",
    "llm = ChatOpenAI(temperature=0, model=\"gpt-4\")\n",
    "\n",
    "# STEP 3: Load documents and create vector store\n",
    "loader = TextLoader(\"my_notes.txt\")  # Replace with your own file\n",
    "documents = loader.load()\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "docs = splitter.split_documents(documents)\n",
    "embedding = OpenAIEmbeddings()\n",
    "vectordb = FAISS.from_documents(docs, embedding)\n",
    "retriever = vectordb.as_retriever()\n",
    "\n",
    "# STEP 4: Create RetrievalQA chain\n",
    "doc_qa = RetrievalQA.from_chain_type(llm=llm, retriever=retriever)\n",
    "\n",
    "# STEP 5: Register tools for agent\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"SQL_Database\",\n",
    "        func=SQLDatabaseToolkit(db=sql_db, llm=llm).get_tools()[0].func,\n",
    "        description=\"Useful for answering questions about employees, departments, and salaries.\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"Document_QA\",\n",
    "        func=doc_qa.run,\n",
    "        description=\"Useful for answering questions about policy notes and general knowledge from documents.\"\n",
    "    )\n",
    "]\n",
    "\n",
    "# STEP 6: Initialize hybrid agent\n",
    "agent = initialize_agent(\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# STEP 7: Ask unified questions\n",
    "print(agent.run(\"Who in R&D earns over $90k?\"))  # SQL tool\n",
    "print(agent.run(\"Summarize the key points from the notes\"))  # Document tool"
   ],
   "id": "2e5bcef131e18204"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## LangChain",
   "id": "7ba125e5853bcbfb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "##### This document assumes you have a some understanding about the following:\n",
    "- Python and Object Oriented Programming\n",
    "- NLP concepts such as embeddings\n",
    "- somewhat know how LLMs work\n",
    "- know a bit about VectorDBs\n",
    "\n",
    "##### Explicitly used packages:\n",
    "- Langchain\n",
    "- Transformers\n",
    "##### Implicitly used packages:\n",
    "\n",
    "- ChromaDB\n",
    "- openai\n",
    "- sentence-transformers\n",
    "> Implicit packages: These packages must be installed to allow for the explicitly defined packages to perform some functions. The explicitly used packages implement the functionality of these implicit packages and simplify it for the end user by abstracting many complicated lines of code into one simple function call (read: Wrapper Classes) but you do not need to understand the inner workings of the implicit packages if you are a beginner\n",
    "\n"
   ],
   "id": "9b1803e9cebf88e0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Load Dependencies",
   "id": "e37db6a2feec2ec"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "1. Start with an object of the OpenAI class and set some parameters.\n",
    "2. Here, we set up the parameter called temperature.\n",
    "3. Having a lower temperature value ensures that our LLM output is deterministic in nature and not too random and \"creative\".\n",
    "4. Call the instance (object) of the OpenAI class \"llm\""
   ],
   "id": "f751cc8402bd9d9c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### API",
   "id": "13b33614d3b2daee"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "api_key = os.getenv( 'OPENAI_API_KEY' )",
   "id": "6e74948acd8ee168"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Create OpenAI object",
   "id": "c0c28773ba979ff2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "llm = OpenAI( temperature=0.9 )\n",
    "text = 'What would be a good company name for a company that makes colorful socks?'\n",
    "print( llm( text ) )"
   ],
   "id": "e846e577e4bd9cc0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### The pipeline for this task is as follows:\n",
    "\n",
    "1. Load doc\n",
    "2. Split lengthy docs\n",
    "3. Get doc embeddings\n",
    "4. Store doc embeddings in vector db (chroma db)\n",
    "5. Query over the db to obtain the correct chunk to answer from\n"
   ],
   "id": "b7b7d13a479cad4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "data = []",
   "id": "1cc23a0796a477b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### 1. Load the documents using TextLoader from LangChain.\n",
    "##### 2. Define a variable to store the documents"
   ],
   "id": "56db9a0ac0285580"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- We can append the output of loader.load() into our list variable",
   "id": "b0e31559cb22def8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "loader = TextLoader(r'C:\\Users\\JkReddy\\Desktop\\Weill Cornell Medicine\\Subjects\\Capstone\\LangChain.txt')\n",
    "data.append( loader.load( )[ 0 ] )"
   ],
   "id": "43c832592cf374e6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- Importing Vector DB - Chroma along with TextSplitter and QA related packages from LangChain.\n",
    "- Import package for Embeddings from OpenAI"
   ],
   "id": "3ac2578351c67ca2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Split the lengthy document into smaller chunks.\n",
    "- This is done because LLMs have a limit to the number of words they can take in as input, and they can retain more information if they have fewer words to work with.\n",
    "- Chunk overlap parameter dictates how much overlap should exist between the chunks.\n",
    "- Having more of an overlap ensures that important information is not lost during the splitting process.\n",
    "- Split documents function takes in a list as an input.\n",
    "- Each list element must contain a document loaded in by Langchain"
   ],
   "id": "96d9391fbb419e68"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "text_splitter = CharacterTextSplitter( chunk_size = 1000, chunk_overlap=200 )\n",
    "texts = text_splitter.split_documents( data )"
   ],
   "id": "bab5e31eab29bb3c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "1. Creating database and getting the embedding functions ready.\n",
    "2. Store the docs in the vector db.\n",
    "\n",
    "- In the below cell, we mention a directory in which we want our vector db to reside.\n",
    "- This is then followed by the creation of an embeddings object from the OpenAIEmbeddings class from the Langchain package.\n",
    "- This can used for creating the doc embeddings"
   ],
   "id": "c4b46c712c5ec0c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Embed and store the texts\n",
    "# Supplying a persist_directory will store the embeddings on disk\n",
    "persist_directory = 'myvectordb'\n",
    "embeddings = OpenAIEmbeddings( )"
   ],
   "id": "3a5f8fdbd62d87e7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### By default, the model used for embeddings is the text-ada-embeddings-002\n",
    "\n",
    "- The vector db is Chroma DB which is integrated into LangChain\n",
    "- The Chroma.from_documents function takes in these parameters:\n",
    "\n",
    "##### The split up texts\n",
    "- Embedding instance from Langchain\n",
    "- Directory in which we want the persistence of our db to be asserted"
   ],
   "id": "11d19830c4e39a66"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "vectordb = Chroma.from_documents( texts, embeddings, persist_directory = persist_directory )",
   "id": "b8e4eeedf49a4037"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- Write Embeddings to a disk using db.persist() and wiping it clean. Reload again to test if it has been stored",
   "id": "b25d4f22340f07d1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "vectordb.persist()\n",
    "vectordb = None"
   ],
   "id": "12de644ae2b87b7b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- Reload",
   "id": "c509821ba00c236f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "vectordb = Chroma( persist_directory=persist_directory, embedding_function = embeddings )",
   "id": "d2aa903633bdd268"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- Create object with LLM model being passed as a parameter along with temperature parameter for controlling the nature of the LLM output.\n",
    "- OpenAI class also takes in model_name as a parameter.\n"
   ],
   "id": "38b7636f9a14b74b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "gpt_qa = RetrievalQA.from_chain_type(llm=OpenAI(temperature = 0.1, model_name = \"text-davinci-003\"),\n",
    "                                 chain_type = \"stuff\",\n",
    "                                 retriever = vectordb.as_retriever())"
   ],
   "id": "8a3413f9f8e9419d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- Our newly created qa object has the function query using which we can run a query over the db.\n",
    "- Once the right doc chunk has been retrieved, it is passed to the llm along with the query."
   ],
   "id": "41a16c60ebd6bd2e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "query = \"What can I eat?\"\n",
    "gpt_qa.run(query)"
   ],
   "id": "9e40fc0e8dac312d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- Creating an instance of an Open Source Embedding",
   "id": "8acd169195ba4692"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain import HuggingFacePipeline\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "embeddings = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)\n",
    "text_splitter = CharacterTextSplitter(chunk_size = 500, chunk_overlap=50)\n",
    "texts = text_splitter.split_documents(data)\n",
    "persist_directory = 'myvectordb_opensource'\n",
    "vectordb = Chroma.from_documents(texts, embeddings, persist_directory = persist_directory)\n",
    "vectordb.persist()"
   ],
   "id": "15e21fbb24187a0e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- Langchain HF pipeline only supports models in the hub which function as text2text gen or text gen models.",
   "id": "fd6ab40345472329"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "llm = HuggingFacePipeline.from_model_id(model_id=\"declare-lab/flan-gpt4all-xl\",\n",
    "                                        task=\"text2text-generation\",\n",
    "                                        model_kwargs={\"temperature\":0, \"max_length\":50, \"min_length\":10})"
   ],
   "id": "581476832144783a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "qa = RetrievalQA.from_chain_type(llm,\n",
    "                                 chain_type = \"refine\",\n",
    "                                 retriever = vectordb.as_retriever())"
   ],
   "id": "3a25b95c0443d1bb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "query = \"Can I have fruits?\"\n",
    "qa.run(query)\n",
    "\n",
    "\n"
   ],
   "id": "5687c08e8c778d39"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
