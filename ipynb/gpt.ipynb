{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": "<a href='https://colab.research.google.com/github/is-leeroy-jenkins/Boo/blob/main/ipynb/GPT.ipynb' target='_parent'><img src='https://colab.research.google.com/assets/colab-badge.svg' alt='Open In Colab'/></a>",
   "id": "6b5948d8e77a7699"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# OpenAI API",
   "id": "b752f6d30e042eaa"
  },
  {
   "metadata": {
    "id": "6f5de4ab069df199"
   },
   "cell_type": "markdown",
   "source": "###### Load Dependencies",
   "id": "6f5de4ab069df199"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T10:07:13.595544Z",
     "start_time": "2025-05-13T10:07:07.156652Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from typing_extensions import override\n",
    "from openai import AssistantEventHandler\n",
    "from pathlib import Path\n",
    "from playwright.sync_api import sync_playwright\n",
    "import tiktoken\n",
    "import base64\n",
    "import json\n",
    "import requests\n",
    "from pydantic import BaseModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "import ipywidgets as widgets, IPython, platform, ipywidgets, jupyterlab\n",
    "from static import GptRequests, GptRoles, GptLanguages\n",
    "from booggr import Error, ErrorDialog, ChatWindow, FileDialog, FileBrowser\n",
    "from importlib import reload\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm import tqdm\n",
    "import concurrent\n",
    "import PyPDF2"
   ],
   "id": "68c9ad01631bf5d7",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Prompts",
   "id": "54591cf42e40a4d3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### System Instructions",
   "id": "2d0d4f68c0109fbf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "system_instructions = r'''\n",
    "You are the most knowledgeable Budget Analyst in the federal government who provides detailed responses based on your vast knowledge of federal appropriations.\n",
    "Your responses to questions about federal finance are complete, transparent, and very detailed using an academic format.\n",
    "Your vast knowledge of and experience in Data Science makes you the best Data Analyst in the world. You are proficient in C#, Python, SQL, C++, JavaScript, and VBA.\n",
    "You use US federal budget data from OMB, whitehouse.gov, or data.gov for any ad hoc data sets in examples you.\n",
    "You do your analysis in Python and visualizations with matplotlib or seaborn. You are famous for the accuracy of your responses so you verify all your answers.\n",
    "Your name is Bubba.\n",
    "'''"
   ],
   "id": "7e86f5df71f2650f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#####  Prompts",
   "id": "29d46dec7f275964"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "user_propmt = r'''\n",
    "What was a positive news story from today?\n",
    "'''"
   ],
   "id": "9b16424b0dff10f5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Messages",
   "id": "16c931bd619f11ae"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "messages=\\\n",
    "[\n",
    "    {\n",
    "        'role': 'system',\n",
    "\t    'content': 'SYSTEM_INSTRUCTIONS'\n",
    "    },\n",
    "    {\n",
    "        'role': 'user',\n",
    "\t    'content': 'USER_MESSAGE_CONTENT'\n",
    "    },\n",
    "    {\n",
    "        'role': 'assistant',\n",
    "\t    'content': 'ASSISTANT_MESSAGE_CONTENT'\n",
    "    },\n",
    "    {\n",
    "        'role': 'user',\n",
    "\t    'content': 'USER_MESSAGE_CONTENT'\n",
    "    },\n",
    "    {\n",
    "        'role': 'assistant',\n",
    "\t    'content': 'ASSISTANT_MESSAGE_CONTENT'\n",
    "    }\n",
    "]"
   ],
   "id": "c400a74c02d5cdca"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### System Message",
   "id": "3dee11bc80af43e8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "\n",
    "{\n",
    "    'role': 'system',\n",
    "    'content': system_instructions\n",
    "}"
   ],
   "id": "6886ea9093093163"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### User Message",
   "id": "68b00d14dd49f22d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "{\n",
    "    'role': 'user',\n",
    "    'content': user_prompt\n",
    "}"
   ],
   "id": "2e9240277bfbc35b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Developer Message",
   "id": "88d33921bd454aff"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "{\n",
    "    'role': 'developer',\n",
    "    'content': user_prompt\n",
    "}"
   ],
   "id": "ee87869d85e5de53"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Tool Message",
   "id": "c7f8ecc8516d8c0f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "{\n",
    "    'role': 'tool',\n",
    "    'content': user_prompt\n",
    "}"
   ],
   "id": "bbfb9fb7c421daab"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Response Format\n",
    "- choices\n",
    "- completion.choices[ 0 ].message"
   ],
   "id": "251d2ba4ff921465"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "[\n",
    "    {\n",
    "        'index': 0,\n",
    "        'message':\n",
    "        {\n",
    "            'role': 'assistant',\n",
    "            'content': 'Under the soft glow of the moon, Luna the unicorn danced through fields of twinkling stardust, leaving trails of dreams for every child asleep.',\n",
    "            'refusal': null\n",
    "         },\n",
    "         'logprobs': null,\n",
    "         'finish_reason': 'stop'\n",
    "    }\n",
    "]"
   ],
   "id": "bfdf448cbb8d9846"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##  Structured Output\n",
   "id": "c6a24c1f35e8a63d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create Client\n",
    "client = OpenAI( )\n",
    "client.api_key = os.getenv( 'OPENAI_API_KEY' )\n",
    "\n",
    "math_tutor_prompt = '''\n",
    "    You are a helpful math tutor. You will be provided with a math problem,\n",
    "    and your goal will be to output a step by step solution, along with a final answer.\n",
    "    For each step, just provide the output as an equation use the explanation field to detail the reasoning.\n",
    "'''\n",
    "\n",
    "def get_math_solution( question ):\n",
    "    response = client.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "        {\n",
    "            'role': 'system', \n",
    "            'content': dedent( math_tutor_prompt )\n",
    "        },\n",
    "        {\n",
    "            'role': 'user', \n",
    "            'content': question\n",
    "        }\n",
    "    ],\n",
    "    response_format=\n",
    "    {\n",
    "        'type': 'json_schema',\n",
    "        'json_schema':\n",
    "        {\n",
    "            'name': 'math_reasoning',\n",
    "            'schema':\n",
    "            {\n",
    "                'type': 'object',\n",
    "                'properties':\n",
    "                {\n",
    "                    'steps':\n",
    "                    {\n",
    "                        'type': 'array',\n",
    "                        'items':\n",
    "                        {\n",
    "                            'type': 'object',\n",
    "                            'properties':\n",
    "                            {\n",
    "                                'explanation':\n",
    "                                {\n",
    "                                    'type': 'path'\n",
    "                                },\n",
    "                                'output':\n",
    "                                {\n",
    "                                    'type': 'path'\n",
    "                                }\n",
    "                            },\n",
    "                            'required': [ 'explanation', 'output' ],\n",
    "                            'additionalProperties': False\n",
    "                        }\n",
    "                    },\n",
    "                    'final_answer':\n",
    "                    {\n",
    "                        'type': 'path'\n",
    "                    }\n",
    "                },\n",
    "                'required': [ 'steps', 'final_answer' ],\n",
    "                'additionalProperties': False\n",
    "            },\n",
    "            'strict': True\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "    return response.choices[ 0 ].message"
   ],
   "id": "ebe225d7778ff6c0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Completion API\n",
    "- Generates a model response from a list of messages comprising a conversation.\n",
    "- Parameter support can differ depending on the model used to generate the response"
   ],
   "id": "efdf3332cff187ac"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Create",
   "id": "b57c826dca20dbf2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create Client\n",
    "client = OpenAI( )\n",
    "client.api_key = os.getenv( 'OPENAI_API_KEY' )\n",
    "\n",
    "# Create Completion\n",
    "completion = client.chat.completions.create(\n",
    "\tmodel='gpt-4o',\n",
    "\tmessages=\n",
    "\t[\n",
    "\t\t{\n",
    "\t\t\t'role': 'system',\n",
    "\t\t\t'content': system_instructions\n",
    "\n",
    "\t\t},\n",
    "\t\t{\n",
    "\t\t\t'role': 'user',\n",
    "\t\t\t'content': 'What is a Treasury Symbol?'\n",
    "\t\t}\n",
    "\t]\n",
    ")\n",
    "\n",
    "print( completion.choices[ 0 ].message )"
   ],
   "id": "4d3bfe011cc31031"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Retreive\n",
    "- ( store=True )\n"
   ],
   "id": "39f725046e0f0737"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create Client\n",
    "client = OpenAI( )\n",
    "client.api_key = os.getenv( 'OPENAI_API_KEY' )\n",
    "\n",
    "# Create Completion\n",
    "completions = client.chat.completions.list( )\n",
    "first_id = completions[ 0 ].id\n",
    "first_completion = client.chat.completions.retrieve( completion_id=first_id )\n",
    "print( first_completion )"
   ],
   "id": "9e0b5e3d236d2502"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### View",
   "id": "c685a267b56ad098"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create Client\n",
    "client = OpenAI( )\n",
    "client.api_key = os.getenv( 'OPENAI_API_KEY' )\n",
    "\n",
    "# Create Completion\n",
    "completions = client.chat.completions.list( )\n",
    "first_id = completions[ 0 ].id\n",
    "first_completion = client.chat.completions.retrieve( completion_id=first_id )\n",
    "messages = client.chat.completions.messages.list( completion_id=first_id )\n",
    "print( messages )"
   ],
   "id": "9efa170085e137dc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### List",
   "id": "27b32337f2890c5e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create Client\n",
    "client = OpenAI( )\n",
    "client.api_key = os.getenv( 'OPENAI_API_KEY' )\n",
    "\n",
    "# Create Compmletion\n",
    "completions = client.chat.completions.list( )\n",
    "print( completions )\n"
   ],
   "id": "b8c1955ae558a434"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Summarize",
   "id": "d090bc4367f0d575"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create Client\n",
    "client = OpenAI( )\n",
    "client.api_key = os.getenv( 'OPENAI_API_KEY' )\n",
    "\n",
    "# Create File Reqeust\n",
    "path = 'draconomicon.pdf'\n",
    "file = client.files.create( file=open( file_path, 'rb' ),\n",
    "    purpose='user_data' )\n",
    "\n",
    "# Create Completion\n",
    "completion = client.chat.completions.create(\n",
    "    model='gpt-4o',\n",
    "    messages=[\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': [\n",
    "                {\n",
    "                    'type': 'file',\n",
    "                    'file':\n",
    "\t\t\t\t\t{\n",
    "                        'file_id': file.id,\n",
    "                    }\n",
    "                },\n",
    "                {\n",
    "                    'type': 'pages',\n",
    "                    'pages': 'What is the first dragon in the book?',\n",
    "                },\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print( completion.choices[ 0 ].message.content )"
   ],
   "id": "2f33ae0e30d9908e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Update",
   "id": "e242393b55304b89"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create Client\n",
    "client = OpenAI( )\n",
    "client.api_key = os.getenv( 'OPENAI_API_KEY' )\n",
    "\n",
    "# Create Completion\n",
    "completions = client.chat.completions.list( )\n",
    "first_id = completions[ 0 ].id\n",
    "updated_completion = client.chat.completions.update( completion_id=first_id,\n",
    "\trequest_body={ 'metadata': { 'foo': 'bar' } } )\n",
    "print( updated_completion )"
   ],
   "id": "6213187806dd07ae"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Delete",
   "id": "af694e2c08fbf7fe"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create Client\n",
    "client = OpenAI( )\n",
    "client.api_key = os.getenv( 'OPENAI_API_KEY' )\n",
    "\n",
    "# Create Completion\n",
    "completions = client.chat.completions.list( )\n",
    "first_id = completions[ 0 ].id\n",
    "delete_response = client.chat.completions.delete( completion_id=first_id )\n",
    "print( delete_response )"
   ],
   "id": "44985a4587ed531a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Assistants API\n",
    "- Can call models and use tools to perform tasks..\n",
    "- An Assistant has instructions and can leverage models, tools, and files to respond to user queries.\n",
    "##### Tools:\n",
    "\n",
    "- Code Interpreter\n",
    "- File Search\n",
    "- Function calling\n",
    "\n",
    "\n",
    "##### Integration of the Assistants API has the following flow:\n",
    "1. Create an Assistant by defining its custom instructions and picking a model (with optional Tools).\n",
    "2. Create a Thread when a user starts a conversation.\n",
    "3. Add Messages to the Thread as the user asks questions.\n",
    "4. Run the Assistant on the Thread to generate a response by calling the model and the tools.\n",
    "___"
   ],
   "id": "f8d8b0952801b089"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### List Assistants",
   "id": "a357cc85dc16c209"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create Client\n",
    "client = OpenAI()\n",
    "client.api_key = os.getenv( 'OPENAI_API_KEY' )\n",
    "\n",
    "# List Assistants\n",
    "my_assistants = client.beta.assistants.list(\n",
    "    order=\"desc\",\n",
    "    limit=\"20\",\n",
    ")\n",
    "\n",
    "print( my_assistants.data )\n"
   ],
   "id": "3235e66ae8b85397"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Retreive Assistant\n",
   "id": "8c09784334b20656"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create Client\n",
    "client = OpenAI()\n",
    "client.api_key = os.getenv( 'OPENAI_API_KEY' )\n",
    "bro = 'asst_2IpP4nE85lXLKbY6Zewwqtqe'\n",
    "bubba = 'asst_J6SAABzDixkTYi2k39OGgjPv'\n",
    "\n",
    "# Retrieve Assistant\n",
    "my_assistant = client.beta.assistants.retrieve( bro )\n",
    "print( my_assistant )\n"
   ],
   "id": "6fad50179659d1e5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Create Assistant\n",
    "- Bro"
   ],
   "id": "37f0447af22b473d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create Client\n",
    "client = OpenAI()\n",
    "client.api_key = os.getenv( 'OPENAI_API_KEY' )\n",
    "\n",
    "assistant = client.beta.assistants.create(\n",
    "  name='Boo',\n",
    "  instructions='You are a computer programming tutor. Write and run code to answer programming questions.',\n",
    "  tools=[ { 'type': 'code_interpreter' } ],\n",
    "  model='gpt-4o' )"
   ],
   "id": "57b5b9cb4ae0a5a9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Completion Response",
   "id": "69d298bf352ae5e4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "{\n",
    "  'id': 'asst_abc123',\n",
    "  'object': 'assistant',\n",
    "  'created_at': 1698984975,\n",
    "  'name': 'Bro',\n",
    "  'description': null,\n",
    "  'small_model': 'gpt-4o',\n",
    "  'instructions': 'You are a personal math tutor. When asked a question, write and run Python code to answer the question.',\n",
    "  'tools': [ { 'type': 'code_interpreter' } ],\n",
    "  'metadata': {},\n",
    "  'top_p': 0.9,\n",
    "  'temperature': 0.8,\n",
    "  'response_format': 'auto'\n",
    "}\n"
   ],
   "id": "875f6353c0b9cc24"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Create Assistant\n",
    "- Bubba"
   ],
   "id": "d572c7e3e53696f2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create Client\n",
    "client = OpenAI()\n",
    "client.api_key = os.getenv( 'OPENAI_API_KEY' )\n",
    "\n",
    "# Create Assistant\n",
    "my_assistant = client.beta.assistants.create(\n",
    "    instructions='You are a Budget bot with access to files to assist you in answering questions about federal regulations and appropriations',\n",
    "    name='Bubba',\n",
    "    tools=[{'type': 'file_search'}],\n",
    "    tool_resources={'file_search': {'vector_store_ids': ['vs_8fEoYp1zVvk5D8atfWLbEupN', 'vs_712r5W5833G6aLxIYIbuvVcK' ]}},\n",
    "    model='gpt-4o-mini'\n",
    ")\n",
    "my_assistant\n",
    "print( my_assistant )\n"
   ],
   "id": "a9e02e2d8d5ea8db"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Search Response",
   "id": "1edbaf850a309d0e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "{\n",
    "      'id': 'asst_abc123',\n",
    "      'object': 'assistant',\n",
    "      'created_at': 1699009403,\n",
    "      'name': 'HR Helper',\n",
    "      'description': null,\n",
    "      'small_model': 'gpt-4o-mmini',\n",
    "      'instructions': 'You are an HR bot, and you have access to files to answer employee questions about company policies.',\n",
    "      'tools': [ { 'type': 'file_search' } ],\n",
    "      'tool_resources':\n",
    "      {\n",
    "            'file_search':\n",
    "             {\n",
    "                 'vector_store_ids': [ 'vs_8fEoYp1zVvk5D8atfWLbEupN', 'vs_712r5W5833G6aLxIYIbuvVcK' ]\n",
    "             }\n",
    "      },\n",
    "      'metadata': {},\n",
    "      'top_p': 1.0,\n",
    "      'temperature': 1.0,\n",
    "      'response_format': 'auto'\n",
    "}\n"
   ],
   "id": "673e1e7eef9b0d03"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Create Thread",
   "id": "ee16b57af0b6ae21"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create Client\n",
    "client = OpenAI()\n",
    "client.api_key = os.getenv( 'OPENAI_API_KEY' )\n",
    "\n",
    "thread = client.beta.threads.create()"
   ],
   "id": "62b60e58aa3f1eaa"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Add Message",
   "id": "da3c6bcc2776b905"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create Client\n",
    "client = OpenAI()\n",
    "client.api_key = os.getenv( 'OPENAI_API_KEY' )\n",
    "\n",
    "# Create Thread Message\n",
    "thread_message = client.beta.threads.messages.create( 'thread_abc123',\n",
    "  role='user', content='How does AI work? Explain it in simple terms.' )\n",
    "\n",
    "print( thread_message )"
   ],
   "id": "dc41efb63f15f682"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Create Run",
   "id": "bea37cd09694d48b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create Client\n",
    "client = OpenAI()\n",
    "client.api_key = os.getenv( 'OPENAI_API_KEY' )\n",
    "\n",
    "run = client.beta.threads.runs.create_and_poll( thread_id=thread.id,\n",
    "  assistant_id=assistant.id,\n",
    "  instructions='Please address the user as Jane Doe. The user has a premium account.'\n",
    ")\n",
    "\n",
    "if run.status == 'completed':\n",
    "  messages = client.beta.threads.messages.list( thread_id=thread.id )\n",
    "  print( messages )\n",
    "else:\n",
    "  print( run.status )"
   ],
   "id": "b292ee7c18a0731b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Stream Run",
   "id": "7d9a4e3be003fb1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create Client\n",
    "client = OpenAI()\n",
    "client.api_key = os.getenv( 'OPENAI_API_KEY' )\n",
    "\n",
    "from typing_extensions import override\n",
    "from openai import AssistantEventHandler\n",
    "\n",
    "# First, we generate_text a EventHandler class to define\n",
    "# how we want to handle the events in the response stream.\n",
    "\n",
    "class EventHandler(AssistantEventHandler):\n",
    "  @override\n",
    "  def on_text_created(self, text) -> None:\n",
    "    print(f\"\\nassistant > \", end=\"\", flush=True)\n",
    "\n",
    "  @override\n",
    "  def on_text_delta(self, delta, snapshot):\n",
    "    print(delta.value, end=\"\", flush=True)\n",
    "\n",
    "  def on_tool_call_created(self, tool_call):\n",
    "    print(f\"\\nassistant > {tool_call.type}\\n\", flush=True)\n",
    "\n",
    "  def on_tool_call_delta(self, delta, snapshot):\n",
    "    if delta.type == 'code_interpreter':\n",
    "      if delta.code_interpreter.input:\n",
    "        print(delta.code_interpreter.input, end=\"\", flush=True)\n",
    "      if delta.code_interpreter.outputs:\n",
    "        print(f\"\\n\\ncleaned_lines >\", flush=True)\n",
    "        for output in delta.code_interpreter.outputs:\n",
    "          if output.type == \"logs\":\n",
    "            print(f\"\\n{output.logs}\", flush=True)\n",
    "\n",
    "# We use the `stream` SDK helper with the `EventHandler` class to generate_text the Run and stream the response.\n",
    "\n",
    "with client.beta.threads.runs.stream(\n",
    "  thread_id=thread.id,\n",
    "  assistant_id=assistant.id,\n",
    "  instructions=\"Please address the user as Jane Doe. The user has a premium account.\",\n",
    "  event_handler=EventHandler( ),\n",
    ") as stream:\n",
    "    stream.until_done( )"
   ],
   "id": "54a9d0956c5828e5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Responses API\n",
    "- Supports text and image inputs, and text outputs.\n",
    "- Create stateful interactions with the model, using the output of previous responses as input\n",
    "- Allow the model access to external systems and data using function calling\n",
    "___"
   ],
   "id": "4c575af21d7c503e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Response Format\n",
    "- `choices`\n",
    "- `response.choices[0].message.content`"
   ],
   "id": "38227c70596dcf78"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "[\n",
    "    {\n",
    "        'index': 0,\n",
    "        'message':\n",
    "        {\n",
    "            'role': 'assistant',\n",
    "            'content': 'Under the soft glow of the moon, Luna the unicorn danced through fields of twinkling stardust, leaving trails of dreams for every child asleep.',\n",
    "            'refusal': null\n",
    "         },\n",
    "         'logprobs': null,\n",
    "         'finish_reason': 'stop'\n",
    "    }\n",
    "]"
   ],
   "id": "fa6cd697329d39bc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Generate Text",
   "id": "8003c92f11eb5918"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create Client\n",
    "client = OpenAI( )\n",
    "client.api_key = os.getenv( 'OPENAI_API_KEY' )\n",
    "\n",
    "response = client.responses.create( model='gpt-4o-mini',\n",
    "    input='Write a five-sentence bedtime story about a unicorn.' )\n",
    "\n",
    "print( response.output_text )"
   ],
   "id": "c36f628f5b2933f7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Analyze Image",
   "id": "492e2e4702de788b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create Client\n",
    "client = OpenAI( )\n",
    "client.api_key = os.getenv( 'OPENAI_API_KEY' )\n",
    "\n",
    "# Create Response\n",
    "response = client.responses.create( model='gpt-4o-mini',\n",
    "    input=\n",
    "    [\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content':\n",
    "\t        [\n",
    "                { 'type': 'input_text',\n",
    "                  'pages': 'what is in this image?'\n",
    "                },\n",
    "                {\n",
    "                    'type': 'input_image',\n",
    "                    'image_url': 'https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg'\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print( response.output_text )"
   ],
   "id": "1dec09adabb5016a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Search Web",
   "id": "b7dd6de1b5ac36f6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create Client\n",
    "client = OpenAI( )\n",
    "client.api_key = os.getenv( 'OPENAI_API_KEY' )\n",
    "\n",
    "# Create Response\n",
    "response = client.responses.create( model='gpt-4o',\n",
    "    tools=[ { 'type': 'web_search_preview' } ],\n",
    "    input='What was a positive news story from today?' )\n",
    "\n",
    "print( response.output_text )"
   ],
   "id": "db29cad5d8439c08"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Web Result Format",
   "id": "54ab863f46a0e975"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "[\n",
    "  {\n",
    "    'type': 'web_search_call',\n",
    "    'id': 'ws_67c9fa0502748190b7dd390736892e100be649c1a5ff9609',\n",
    "    'status': 'completed'\n",
    "  },\n",
    "  {\n",
    "    'id': 'msg_67c9fa077e288190af08fdffda2e34f20be649c1a5ff9609',\n",
    "    'type': 'message',\n",
    "    'status': 'completed',\n",
    "    'role': 'assistant',\n",
    "    'content': [\n",
    "      {\n",
    "        'type': 'output_text',\n",
    "        'pages': 'On March 6, 2025, several news...',\n",
    "        'annotations':\n",
    "        [\n",
    "          {\n",
    "            'type': 'url_citation',\n",
    "            'start_index': 2606,\n",
    "            'end_index': 2758,\n",
    "            'url': 'https://...',\n",
    "            'title': 'Title...'\n",
    "          }\n",
    "        ]\n",
    "      }\n",
    "    ]\n",
    "  }\n",
    "]"
   ],
   "id": "b4faa744d8983321"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Function Schema",
   "id": "5c7154217da1d11d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "{\n",
    "    'type': 'function',\n",
    "    'function':\n",
    "    {\n",
    "        'name': 'get_weather',\n",
    "        'description': 'Retrieves current weather for a given location.',\n",
    "        'parameters':\n",
    "        {\n",
    "            'type': 'object',\n",
    "            'properties':\n",
    "            {\n",
    "                'location':\n",
    "                {\n",
    "                    'type': 'path',\n",
    "                    'description': 'City and country e.g. Bogotá, Colombia'\n",
    "                },\n",
    "                'units':\n",
    "                {\n",
    "                    'type': 'path',\n",
    "                    'enum':\n",
    "                    [\n",
    "                        'celsius',\n",
    "                        'fahrenheit'\n",
    "                    ],\n",
    "                    'description': 'Units the temperature will be returned in.'\n",
    "                }\n",
    "            },\n",
    "            'required':\n",
    "            [\n",
    "                'location',\n",
    "                'units'\n",
    "            ],\n",
    "            'additionalProperties': false\n",
    "        },\n",
    "\n",
    "        'strict': true\n",
    "    }\n",
    "}"
   ],
   "id": "c367017cc7dfdf54"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Search File",
   "id": "29d0165d9ab17144"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create Client\n",
    "client = OpenAI( )\n",
    "client.api_key = os.getenv( 'OPENAI_API_KEY' )\n",
    "\n",
    "# Create Response\n",
    "response = client.responses.create(\n",
    "\tmodel='gpt-4o',\n",
    "\ttools=\n",
    "\t[\n",
    "\t\t{\n",
    "\t\t\t'type': 'file_search',\n",
    "\t\t\t'vector_store_ids': [ 'vs_712r5W5833G6aLxIYIbuvVcK', 'vs_8fEoYp1zVvk5D8atfWLbEupN' ],\n",
    "\t\t\t'max_num_results': 20\n",
    "\t\t}\n",
    "\t],\n",
    "\tinput='What are the attributes of an ancient brown dragon?',\n",
    ")\n",
    "\n",
    "print( response.output_text )"
   ],
   "id": "e21a1c8fec9121d0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Stream",
   "id": "23a7bd3db3bbba25"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create Client\n",
    "client = OpenAI( )\n",
    "client.api_key = os.getenv( 'OPENAI_API_KEY' )\n",
    "\n",
    "# Create Response\n",
    "response = client.responses.create(\n",
    "\tmodel='gpt-4o',\n",
    "\tinstructions='You are a helpful assistant.',\n",
    "\tinput='Hello!',\n",
    "\tstream=True\n",
    ")\n",
    "\n",
    "for event in response:\n",
    "\tprint( event )"
   ],
   "id": "5c8f01161c70a2d8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Speech API\n",
    "- The maximum input length is 4096 characters.\n",
    "- TTS models: tts-1, tts-1-hd or gpt-4o-mini-tts.\n",
    "___"
   ],
   "id": "ed76456b47a39ae4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "9ed498076ed176f1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Stream\n",
    "- Generates audio from the input text."
   ],
   "id": "8d2f750d272fde5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create Client\n",
    "client = OpenAI( )\n",
    "client.api_key = os.getenv( 'OPENAI_API_KEY' )\n",
    "\n",
    "# Create Speech\n",
    "speech_file_path = Path( __file__ ).parent  # 'speech.mp3'\n",
    "prompt = 'The quick brown fox jumped over the lazy dog.'\n",
    "response = client.audio.speech.create( model='tts-1-hd', voice='alloy', input=prompt )\n",
    "response.stream_to_file( speech_file_path )\n"
   ],
   "id": "10bbff7ec480fedf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Transcribe\n",
    "- Transcribes audio into the input language."
   ],
   "id": "4426bb6e9a602952"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create Client\n",
    "client = OpenAI( )\n",
    "client.api_key = os.getenv( 'OPENAI_API_KEY' )\n",
    "\n",
    "# Open Audio File 'speech.mp3'\n",
    "audio_file = open( 'speech.mp3', 'rb' )\n",
    "transcript = client.audio.transcriptions.create( model='whisper-1', file=audio_file )\n"
   ],
   "id": "65fec2f02c5afaa1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Translate\n",
    "- Translates audio into English."
   ],
   "id": "81308dfbaba50f9b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create Client\n",
    "client = OpenAI( )\n",
    "client.api_key = os.getenv( 'OPENAI_API_KEY' )\n",
    "\n",
    "# Create Translation\n",
    "audio_file = open( 'speech.mp3', 'rb' )\n",
    "translation = client.audio.translations.create( model='whisper-1', file=audio_file )\n"
   ],
   "id": "5ce3f8486bc57f9c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Generate",
   "id": "b5146a7d3aabe350"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create Client\n",
    "client = OpenAI( )\n",
    "client.api_key = os.getenv( 'OPENAI_API_KEY' )\n",
    "\n",
    "# Create Transcription\n",
    "completion = client.chat.completions.create(\n",
    "    model='gpt-4o-audio-preview',\n",
    "    modalities=[ 'text', 'audio' ],\n",
    "    audio={ 'voice': 'alloy', 'format': 'wav' },\n",
    "    messages=[\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': 'Is a golden retriever a good family dog?'\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print( completion.choices[ 0 ] )\n",
    "\n",
    "# Convert to bytes\n",
    "wav_bytes = base64.b64decode( completion.choices[ 0 ].message.audio.data )\n",
    "with open( 'dog.wav', 'wb' ) as f:\n",
    "    f.write( wav_bytes )"
   ],
   "id": "b52722036f1bad5b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Analyze",
   "id": "909f7bbf1588a9dd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create Client\n",
    "client = OpenAI( )\n",
    "client.api_key = os.getenv( 'OPENAI_API_KEY' )\n",
    "\n",
    "# Fetch the audio file and convert it to a base64 encoded path\n",
    "url = 'https://cdn.openai.com/API/docs/audio/alloy.wav'\n",
    "response = requests.get( url )\n",
    "response.raise_for_status( )\n",
    "wav_data = response.content\n",
    "encoded_string = base64.b64encode( wav_data ).decode( 'utf-8' )\n",
    "\n",
    "# Create Completion\n",
    "completion = client.chat.completions.create(\n",
    "    model='gpt-4o-audio-preview',\n",
    "    modalities=['text', 'audio'],\n",
    "    audio={'voice': 'alloy', 'format': 'wav'},\n",
    "    messages=\n",
    "    [\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content':\n",
    "\t        [\n",
    "                {\n",
    "                    'type': 'pages',\n",
    "                    'documents': 'What is in this recording?'\n",
    "                },\n",
    "                {\n",
    "                    'type': 'input_audio',\n",
    "                    'input_audio':\n",
    "                    {\n",
    "                        'values': encoded_string,\n",
    "                        'format': 'wav'\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "    ]\n",
    ")\n",
    "\n",
    "print( completion.choices[0].message )"
   ],
   "id": "ff4df0c38ea5ead5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Embedding API\n",
    "- An embedding is a vector (list) of floating point numbers.\n",
    "- The distance between two vectors measures their relatedness.\n",
    "- Small distances suggest high relatedness and large distances suggest low relatedness.\n",
    "\n",
    "##### Use Cases:\n",
    "\n",
    "- **Search** (where results are ranked by relevance to a query string)\n",
    "- **Clustering** (where text strings are grouped by similarity)\n",
    "- **Recommendations** (where items with related text strings are recommended)\n",
    "- **Anomaly Detection** (where outliers with little relatedness are identified)\n",
    "- **Diversity Measurement** (where similarity distributions are analyzed)\n",
    "- **Classification** (where text strings are classified by their most similar label)\n",
    "___"
   ],
   "id": "7ecffb684060df94"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Count Tokens",
   "id": "a2ff5b7df575e1be"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create Client\n",
    "client = OpenAI( )\n",
    "client.api_key = os.getenv( 'OPENAI_API_KEY' )\n",
    "\n",
    "def num_tokens_from_string( string: str, encoding_name: str ) -> int:\n",
    "    '''Returns the num of tokens in a documents path.'''\n",
    "    encoding = tiktoken.get_encoding( encoding_name )\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n",
    "\n",
    "num_tokens_from_string( 'tiktoken is great!', 'cl100k_base' )\n"
   ],
   "id": "9ecd67de74d5960c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "9dfff87322d00e50"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Create Ada\n",
   "id": "a4f429ca2743667b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create Client\n",
    "client = OpenAI( )\n",
    "client.api_key = os.getenv( 'OPENAI_API_KEY' )\n",
    "\n",
    "client.embeddings.create(\n",
    "\tmodel='text-embedding-ada-002',\n",
    "\tinput='The food was delicious and the waiter...',\n",
    "\tencoding_format='float'\n",
    ")\n"
   ],
   "id": "54c2a485229eafbd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Create Small",
   "id": "98361c6f6abe52da"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create Client\n",
    "client = OpenAI( )\n",
    "client.api_key = os.getenv( 'OPENAI_API_KEY' )\n",
    "\n",
    "# Create Vector\n",
    "def get_embedding( text, model='text-embedding-3-small' ):\n",
    "    text = text.replace( '\\n', ' ' )\n",
    "    return client.embeddings.create( input = [text], model=model ).data[0].embedding\n"
   ],
   "id": "3061464802458c8e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Create Large",
   "id": "b23c885f58c6f013"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create Client\n",
    "client = OpenAI( )\n",
    "client.api_key = os.getenv( 'OPENAI_API_KEY' )\n",
    "\n",
    "# Create Vector\n",
    "def get_embedding( text, model='text-embedding-3-large' ):\n",
    "    text = text.replace( '\\n', ' ' )\n",
    "    return client.embeddings.create( input=[ text ], model=model ).data[ 0 ].embedding"
   ],
   "id": "95fdc9789c21b06d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Reduce Dimensions\n",
    "- Dynamically changing the dimensions enables very flexible usage.\n",
    "- When using a vector data store that only supports embeddings up to 1024 dimensions long, developers can now still use our best embedding model text-embedding-3-large and specify a value of 1024 for the dimensions API parameter, which will shorten the embedding down from 3072 dimensions, trading off some accuracy in exchange for the smaller vector size."
   ],
   "id": "840859314d40ae52"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create Client\n",
    "client = OpenAI( )\n",
    "client.api_key = os.getenv( 'OPENAI_API_KEY' )\n",
    "\n",
    "def normalize_l2( x ):\n",
    "    x = np.array( x )\n",
    "    if x.ndim == 1:\n",
    "        norm = np.linalg.norm( x )\n",
    "        if norm == 0:\n",
    "            return x\n",
    "        return x / norm\n",
    "    else:\n",
    "        norm = np.linalg.norm( x, 2, axis=1, keepdims=True )\n",
    "        return np.where( norm == 0, x, x / norm )\n",
    "\n",
    "\n",
    "response = client.embeddings.create( model='documents-embedding-3-small',\n",
    "\tinput='Testing 123', encoding_format='float' )\n",
    "\n",
    "cut_dim = response.data[ 0 ].embedding[ :256 ]\n",
    "norm_dim = normalize_l2( cut_dim )\n",
    "\n",
    "print( norm_dim )\n"
   ],
   "id": "3079893de107cb1f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Question  Answer\n",
    "- There are many common cases where the model is not trained on data which contains key facts and information you want to make accessible when generating responses to a user query.\n",
    "- One way of solving this, as shown below, is to put additional information into the context window of the model.\n",
    "- This is effective in many use cases but leads to higher token costs."
   ],
   "id": "1615ab8f676a9b76"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create Client\n",
    "client = OpenAI( )\n",
    "client.api_key = os.getenv( 'OPENAI_API_KEY' )\n",
    "\n",
    "# Create Query\n",
    "query = f'''\n",
    "\n",
    "\tUse the below article on the 2022 Winter Olympics to answer the subsequent question.\n",
    "\tIf the answer cannot be found, write 'I don't know.'\n",
    "\n",
    "\tArticle:\n",
    "\t\\'\\'\\'\n",
    "\t{wikipedia_article_on_curling}\n",
    "\t\\'\\'\\'\n",
    "\n",
    "\tQuestion: Which athletes won the gold medal in curling at the 2022 Winter Olympics?\n",
    "\n",
    "'''\n",
    "\n",
    "# Create Response\n",
    "response = client.chat.completions.create(\n",
    "    messages=\n",
    "    [\n",
    "        {\n",
    "\t        'role': 'system',\n",
    "\t        'content': system_instructions\n",
    "        },\n",
    "        {\n",
    "\t        'role': 'user',\n",
    "\t        'content': user_propmt\n",
    "        },\n",
    "    ],\n",
    "    model=GPT_MODEL,\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ],
   "id": "11c37f28d0d201c5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Text Search\n",
    "- Retrieves the most relevant documents.\n",
    "- Uses the cosine similarity between the embedding vectors of the query and each document.\n",
    " - Returns the highest scored documents.\n"
   ],
   "id": "4e983a4b52e48a8c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create Client\n",
    "client = OpenAI( )\n",
    "client.api_key = os.getenv( 'OPENAI_API_KEY' )\n",
    "\n",
    "def search_reviews( df, product_description, n=3, pprint=True ):\n",
    "    embedding = get_embedding( product_description, model='path-embedding-3-small' )\n",
    "    df[ 'similarities'] = df.ada_embedding.apply( lambda x: cosine_similarity( x, embedding ) )\n",
    "    res = df.sort_values( 'similarities', ascending=False).head( n )\n",
    "    return res\n",
    "\n",
    "res = search_reviews( df, 'delicious beans', n=3 )\n"
   ],
   "id": "71e57c9623ea351e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Code Search\n",
    "- Code search works similarly to embedding-based text search.\n",
    "- We provide a method to extract Python functions from all the Python files in a given repository.\n",
    "- Each function is then indexed by the `text-embedding-3-small` model.\n"
   ],
   "id": "847167aa109c8e7c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create Client\n",
    "client = OpenAI( )\n",
    "client.api_key = os.getenv( 'OPENAI_API_KEY' )\n",
    "\n",
    "df['code_embedding'] = df['code'].apply(lambda x: get_embedding(x, model='pages-embedding-3-small'))\n",
    "\n",
    "def search_functions(df, code_query, n=3, pprint=True, n_lines=7):\n",
    "    embedding = get_embedding(code_query, model='pages-embedding-3-small')\n",
    "    df['similarities'] = df.code_embedding.apply(lambda x: cosine_similarity(x, embedding))\n",
    "\n",
    "    res = df.sort_values('similarities', ascending=False).head(n)\n",
    "    return res\n",
    "\n",
    "res = search_functions(df, 'Completions API tests', n=3)\n"
   ],
   "id": "cf85848ee4956f79"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Recommendation\n",
    "- Shorter distances between embedding vectors represent greater similarity,"
   ],
   "id": "96281a7b515e074b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create Client\n",
    "client = OpenAI( )\n",
    "client.api_key = os.getenv( 'OPENAI_API_KEY' )\n",
    "\n",
    "def recommendations_from_strings( strings: List[str], index_of_source_string: int,\n",
    "    model='path-embedding-3-small' ) -> List[int]:\n",
    "    '''Return nearest neighbors of a given path.'''\n",
    "    # get vectors for all strings\n",
    "    embeddings = [ embedding_from_string( string, model=model ) for string in strings ]\n",
    "\n",
    "    # get the embedding of the source path\n",
    "    query_embedding = embeddings[ index_of_source_string ]\n",
    "    distances = distances_from_embeddings( query_embedding, embeddings, distance_metric='cosine' )\n",
    "\n",
    "    # get indices of nearest neighbors (function from embeddings_utils.py)\n",
    "    indices_of_nearest_neighbors = indices_of_nearest_neighbors_from_distances( distances )\n",
    "    return indices_of_nearest_neighbors\n"
   ],
   "id": "1bfbfc6ac7a982cd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Text Featurization\n",
    "- An embedding can be used as a general free-text feature encoder within a machine learning model.\n",
    "- Incorporating embeddings will improve the performance of any machine learning model, if some of the relevant inputs are free text.\n",
    "- An embedding can also be used as a categorical feature encoder within a ML model."
   ],
   "id": "bd79bebf3104d4fa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create Client\n",
    "client = OpenAI( )\n",
    "client.api_key = os.getenv( 'OPENAI_API_KEY' )\n",
    "\n",
    "data = list( df.ada_embedding.values )\n",
    "X_train, X_test, y_train, y_test = train_test_split( data, df.Score, test_size=0.2, random_state=42 )\n"
   ],
   "id": "4773339d7f7e7e46"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Files API\n",
    "- Upload up to 100 pages and 32MB of total content in a single request to the API, across multiple file inputs.\n",
    "- Only models that support both text and image inputs, such as gpt-4o, gpt-4o-mini, or o1, can accept PDF files as input\n",
    "- Recommend using the user_data purpose for files you plan to use as model inputs.\n",
    "___"
   ],
   "id": "e3fde16f4a5c82b6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Upload\n",
    "- Upload a PDF using the Files API, then reference its file ID in an API request to the model."
   ],
   "id": "72eab7cef4735604"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create client\n",
    "client = OpenAI( )\n",
    "client.api_key = os.getenv( 'OPENAI_API_KEY' )\n",
    "\n",
    "# Assign vriables\n",
    "_messages = [\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': [\n",
    "                {\n",
    "                    'type': 'file',\n",
    "                    'file':\n",
    "\t                {\n",
    "                        'file_id': file.id,\n",
    "                    }\n",
    "                },\n",
    "                {\n",
    "                    'type': 'documents',\n",
    "                    'documents': 'What is the first dragon in the book?',\n",
    "                },\n",
    "            ]\n",
    "        }\n",
    "]\n",
    "\n",
    "# Create File Request\n",
    "file = client.files.create( file=open( 'draconomicon.pdf', 'rb' ), purpose='user_data' )\n",
    "completion = client.chat.completions.create( model='gpt-4o', messages=_messages, )\n",
    "\n",
    "print( completion.choices[ 0 ].message.content )"
   ],
   "id": "9e52a8aca5a53d62"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### List",
   "id": "472bf604087b790d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create client\n",
    "client = OpenAI( )\n",
    "client.api_key = os.getenv( 'OPENAI_API_KEY' )\n",
    "\n",
    "_files = client.files.list()\n",
    "\n",
    "for i in _files:\n",
    "\tprint( i )\n"
   ],
   "id": "6b8d7344b47cd3df"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Retreive File",
   "id": "b4c2ed04a2bc01c4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create client\n",
    "client = OpenAI( )\n",
    "client.api_key = os.getenv( 'OPENAI_API_KEY' )\n",
    "\n",
    "# Reteive by file_id\n",
    "client.files.retrieve( 'file-abc123' )\n"
   ],
   "id": "6496c6c7c3e9ccee"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Reteive Contents",
   "id": "7be1b80ac4fb5e35"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create client\n",
    "client = OpenAI( )\n",
    "client.api_key = os.getenv( 'OPENAI_API_KEY' )\n",
    "content = client.files.content( 'file-abc123' )\n"
   ],
   "id": "92a1e381d4544632"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Upload",
   "id": "7da07ee006a42a88"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create client\n",
    "client = OpenAI( )\n",
    "client.api_key = os.getenv( 'OPENAI_API_KEY' )\n",
    "\n",
    "client.files.create( file=open('mydata.jsonl', 'rb'), purpose='fine-tune' )\n"
   ],
   "id": "517bc315b104e524"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Base64-encoded files\n",
    "- You can send PDF file inputs as Base64-encoded inputs as well."
   ],
   "id": "267f2f464ac377f4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create client\n",
    "client = OpenAI( )\n",
    "client.api_key = os.getenv( 'OPENAI_API_KEY' )\n",
    "\n",
    "with open( 'draconomicon.pdf', 'rb' ) as f:\n",
    "    data = f.read( )\n",
    "\n",
    "base64_string = base64.b64encode( data ).decode( 'utf-8' )\n",
    "completion = client.chat.completions.create( model='gpt-4o',\n",
    "    messages=\n",
    "    [\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': [\n",
    "                {\n",
    "                    'type': 'file',\n",
    "                    'file':\n",
    "\t                {\n",
    "                        'filename': 'draconomicon.pdf',\n",
    "                        'file_data': f'values:application/pdf;base64,{ base64_string }',\n",
    "                    }\n",
    "                },\n",
    "                {\n",
    "                    'type': 'path',\n",
    "                    'documents': 'What is the first dragon in the book?',\n",
    "                }\n",
    "            ],\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "\n",
    "print( completion.choices[ 0 ].message.content )"
   ],
   "id": "fe13e36170cd8192"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Retreival API\n",
    "- The Retrieval API allows you to perform semantic search over your data, which is a technique that surfaces semantically similar results — even when they match few or no keywords.\n",
    "- The Retrieval API is powered by vector stores, which serve as indices for your data.\n",
    "___"
   ],
   "id": "cc9fa669a5111492"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Create Upload",
   "id": "81570e1ee566cd92"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create client\n",
    "client = OpenAI( )\n",
    "client.api_key = os.getenv( 'OPENAI_API_KEY' )\n",
    "\n",
    "vector_store = client.vector_stores.create( name='Support FAQ' )\n",
    "client.vector_stores.files.upload_and_poll(  vector_store_id=vector_store.id,\n",
    "    file=open( 'customer_policies.txt', 'rb' ) )"
   ],
   "id": "8c824cffccbe5b62"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Search Stores",
   "id": "451d8d683694f0e6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create client\n",
    "client = OpenAI( )\n",
    "client.api_key = os.getenv( 'OPENAI_API_KEY' )\n",
    "\n",
    "# Create search\n",
    "user_query = 'What is the return policy?'\n",
    "results = client.vector_stores.search( vector_store_id=vector_store.id, query=user_query, )"
   ],
   "id": "3a81b00e60f48361"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Semantic Search",
   "id": "27716316afa8ae53"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create client\n",
    "client = OpenAI( )\n",
    "client.api_key = os.getenv( 'OPENAI_API_KEY' )\n",
    "\n",
    "results = client.vector_stores.search( vector_store_id=vector_store.id,\n",
    "    query='How many woodchucks are allowed per passenger?', )"
   ],
   "id": "6ce6e5f5d2cc3978"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Vector Store API\n",
    "- Vector stores are the containers that power semantic search for the Retrieval API and the Assistants API file search tool.\n",
    "- When you add a file to a vector store it will be automatically chunked, embedded, and indexed.\n",
    "___"
   ],
   "id": "564d4b602fe2ef91"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### I. Vector Store Operations",
   "id": "fb964cc0c0a36628"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Create",
   "id": "97521b55fc1f0625"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create client\n",
    "client = OpenAI( )\n",
    "client.api_key = os.getenv( 'OPENAI_API_KEY' )\n",
    "\n",
    "# Create vector store\n",
    "client.vector_stores.create( name='Support FAQ', file_ids=[ 'file_123' ] )"
   ],
   "id": "e17fac7bd5d6999f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Retrieve",
   "id": "d90fa1d06cf2d67b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create client\n",
    "client = OpenAI( )\n",
    "client.api_key = os.getenv( 'OPENAI_API_KEY' )\n",
    "\n",
    "# Retreive vector store\n",
    "client.vector_stores.retrieve( vector_store_id='vs_123' )"
   ],
   "id": "d5ac511a0ee074a4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Update",
   "id": "1d8e79e081421fe7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create client\n",
    "client = OpenAI( )\n",
    "client.api_key = os.getenv( 'OPENAI_API_KEY' )\n",
    "\n",
    "# Update vector store\n",
    "client.vector_stores.retrieve(  vector_store_id='vs_123' )"
   ],
   "id": "8353143e91fdc157"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Delete",
   "id": "88302805929453c0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create client\n",
    "client = OpenAI( )\n",
    "client.api_key = os.getenv( 'OPENAI_API_KEY' )\n",
    "\n",
    "# Delete vector store\n",
    "client.vector_stores.delete( vector_store_id='vs_123' )"
   ],
   "id": "f9020eb5ea907162"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### List",
   "id": "6ae83c9daf0a89bc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create client\n",
    "client = OpenAI( )\n",
    "client.api_key = os.getenv( 'OPENAI_API_KEY' )\n",
    "\n",
    "# List vector_stores\n",
    "client.vector_stores.list( )"
   ],
   "id": "e90ce2619c5dc151"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### II. Vector File Operations",
   "id": "a11aaecf8c694f3a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Create",
   "id": "1e7701e63313311e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create client\n",
    "client = OpenAI( )\n",
    "client.api_key = os.getenv( 'OPENAI_API_KEY' )\n",
    "\n",
    "# Create files for vector store\n",
    "client.vector_stores.files.create( vector_store_id='vs_123',\n",
    "    file_id='file_123',\n",
    "    attributes=\n",
    "    {\n",
    "        'region': 'US',\n",
    "        'category': 'Marketing',\n",
    "        'date': 1672531200\n",
    "    }\n",
    ")"
   ],
   "id": "8417ac24062d8b2f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Upload",
   "id": "5ad92c8d67fa907c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create client\n",
    "client = OpenAI( )\n",
    "client.api_key = os.getenv( 'OPENAI_API_KEY' )\n",
    "\n",
    "client.vector_stores.files.upload_and_poll( vector_store_id='vs_123',\n",
    "    file=open( 'customer_policies.txt', 'rb' ) )\n"
   ],
   "id": "f2fa2da1c6644b93"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Retreive",
   "id": "9d4ccfd66ca8e438"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create client\n",
    "client = OpenAI( )\n",
    "client.api_key = os.getenv( 'OPENAI_API_KEY' )\n",
    "\n",
    "client.vector_stores.files.retrieve( vector_store_id='vs_123', file_id='file_123' )\n"
   ],
   "id": "f395931f01ab0b69"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Update",
   "id": "6c0b68210f82332c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create client\n",
    "client = OpenAI( )\n",
    "client.api_key = os.getenv( 'OPENAI_API_KEY' )\n",
    "\n",
    "# Update file\n",
    "client.vector_stores.files.update( vector_store_id='vs_123',\n",
    "    file_id='file_123',\n",
    "    attributes=\n",
    "    {\n",
    "\t    'key': 'value'\n",
    "    }\n",
    ")\n"
   ],
   "id": "bed01979a830b021"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Delete",
   "id": "3fe27377daae7ab2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create client\n",
    "client = OpenAI( )\n",
    "client.api_key = os.getenv( 'OPENAI_API_KEY' )\n",
    "\n",
    "# Delete file\n",
    "client.vector_stores.files.delete( vector_store_id='vs_123', file_id='file_123' )\n"
   ],
   "id": "d81dd18aebc57ff5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### List",
   "id": "afcb3de796b75483"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create client\n",
    "client = OpenAI( )\n",
    "client.api_key = os.getenv( 'OPENAI_API_KEY' )\n",
    "\n",
    "# List files\n",
    "client.vector_stores.files.list( vector_store_id='vs_123' )\n"
   ],
   "id": "dbfaa44ec54f7e7d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### II. Vector Batch Operations",
   "id": "a596f86b6a95e859"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Create",
   "id": "c28a24e34997ddf9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create client\n",
    "client = OpenAI( )\n",
    "client.api_key = os.getenv( 'OPENAI_API_KEY' )\n",
    "\n",
    "client.vector_stores.file_batches.create_and_poll( vector_store_id='vs_123',\n",
    "    file_ids=[ 'file_123', 'file_456' ] )\n"
   ],
   "id": "510b7db20aac2cb7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Retrieve",
   "id": "13cc79d1eaa978a9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create client\n",
    "client = OpenAI( )\n",
    "client.api_key = os.getenv( 'OPENAI_API_KEY' )\n",
    "client.vector_stores.file_batches.retrieve( vector_store_id='vs_123', batch_id='vsfb_123' )\n"
   ],
   "id": "e456a680172302d6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Cancel",
   "id": "4c832bfab16517d0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create client\n",
    "client = OpenAI( )\n",
    "client.api_key = os.getenv( 'OPENAI_API_KEY' )\n",
    "client.vector_stores.file_batches.cancel( vector_store_id='vs_123', batch_id='vsfb_123' )\n"
   ],
   "id": "a29b3ce2e59bc4ca"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### List",
   "id": "a9428d26aa35828e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create client\n",
    "client = OpenAI( )\n",
    "client.api_key = os.getenv( 'OPENAI_API_KEY' )\n",
    "client.vector_stores.file_batches.list( vector_store_id='vs_123' )"
   ],
   "id": "eb075c9ddd4b2188"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Assistant API\n",
    "- Designed to help developers build powerful AI assistants capable of performing a variety of tasks.\n",
    "- Build AI assistants within your own applications.\n",
    "##### Supports the following tools:\n",
    "1. Code Interpreter\n",
    "2. File Search\n",
    "3. Function calling.\n",
    "___"
   ],
   "id": "1b2d5c94147874b2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Create Assistant",
   "id": "92044c9ae536b04a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create client\n",
    "client = OpenAI( )\n",
    "client.api_key = os.getenv( 'OPENAI_API_KEY' )\n",
    "\n",
    "my_assistant = client.beta.assistants.create(\n",
    "    instructions='You are a personal math tutor. When asked a question, write and run Python code to answer the question.',\n",
    "    name='Math Tutor',\n",
    "    tools=[ {'type': 'code_interpreter'} ],\n",
    "    model='gpt-4o',\n",
    ")\n",
    "print( my_assistant )\n"
   ],
   "id": "f11763c71d1595f6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### List Assistants",
   "id": "15db382efb66f38"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create client\n",
    "client = OpenAI( )\n",
    "client.api_key = os.getenv( 'OPENAI_API_KEY' )\n",
    "my_assistants = client.beta.assistants.list( order='desc', limit='20' )\n",
    "\n",
    "print( my_assistants.data )\n"
   ],
   "id": "5ab0beeb786a6c19"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Retreive Assistant",
   "id": "1536d3814b118198"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create client\n",
    "client = OpenAI( )\n",
    "client.api_key = os.getenv( 'OPENAI_API_KEY' )\n",
    "my_assistant = client.beta.assistants.retrieve( 'asst_abc123' )\n",
    "print( my_assistant )\n"
   ],
   "id": "58bb085956255bfb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Update Assistant",
   "id": "71160b625733a9c9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create client\n",
    "client = OpenAI( )\n",
    "client.api_key = os.getenv( 'OPENAI_API_KEY' )\n",
    "my_updated_assistant = client.beta.assistants.update( 'asst_abc123',\n",
    "  instructions='You are an HR bot, and you have access to files to answer employee questions about company policies. Always response with info from either of the files.',\n",
    "  name='HR Helper',\n",
    "  tools=[ {'type': 'file_search'} ],\n",
    "  model='gpt-4o'\n",
    ")\n",
    "\n",
    "print( my_updated_assistant )\n"
   ],
   "id": "7bfa42bd6935f795"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Delete Assistant",
   "id": "659f32c1c45eeff4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create client\n",
    "client = OpenAI( )\n",
    "client.api_key = os.getenv( 'OPENAI_API_KEY' )\n",
    "response = client.beta.assistants.delete( 'asst_abc123' )\n",
    "print(response)\n"
   ],
   "id": "942529e1b877bbab"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### I. Threads",
   "id": "fec63bfcf3ba531b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Create",
   "id": "fa4e117626ee37dd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create client\n",
    "client = OpenAI( )\n",
    "client.api_key = os.getenv( 'OPENAI_API_KEY' )\n",
    "\n",
    "empty_thread = client.beta.threads.create()\n",
    "print(empty_thread)\n"
   ],
   "id": "adc65cbcc1153bbb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Retreive",
   "id": "9fdbe04b6d2e6d70"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create client\n",
    "client = OpenAI( )\n",
    "client.api_key = os.getenv( 'OPENAI_API_KEY' )\n",
    "\n",
    "my_thread = client.beta.threads.retrieve('thread_abc123')\n",
    "print(my_thread)\n"
   ],
   "id": "e224876d561e3eb3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Update",
   "id": "3d98368bf4cb6f93"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create client\n",
    "client = OpenAI( )\n",
    "client.api_key = os.getenv( 'OPENAI_API_KEY' )\n",
    "\n",
    "my_updated_thread = client.beta.threads.update( 'thread_abc123',\n",
    "  metadata= { 'modified': 'true', 'user': 'abc123' } )\n",
    "\n",
    "print(my_updated_thread)\n"
   ],
   "id": "899c60c18a199a0d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Delete",
   "id": "b2ea725327fa44ba"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create client\n",
    "client = OpenAI( )\n",
    "client.api_key = os.getenv( 'OPENAI_API_KEY' )\n",
    "\n",
    "response = client.beta.threads.delete( 'thread_abc123' )\n",
    "\n",
    "print(response)\n"
   ],
   "id": "ef0cb141cb0503d9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### II. Messages",
   "id": "38d7130a7d320b4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Create",
   "id": "de6d53424750f004"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create client\n",
    "client = OpenAI( )\n",
    "client.api_key = os.getenv( 'OPENAI_API_KEY' )\n",
    "\n",
    "# Create message\n",
    "thread_message = client.beta.threads.messages.create( 'thread_abc123',  role='user',\n",
    "  content='How does AI work? Explain it in simple terms.', )\n",
    "\n",
    "print(thread_message)\n"
   ],
   "id": "fc9adcf090a1fb1e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### List",
   "id": "124287374ed4d344"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create client\n",
    "client = OpenAI( )\n",
    "client.api_key = os.getenv( 'OPENAI_API_KEY' )\n",
    "\n",
    "# List messages in a thread\n",
    "thread_messages = client.beta.threads.messages.list( 'thread_abc123' )\n",
    "print(thread_messages.data)\n"
   ],
   "id": "76ae86a4b60ad599"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Retreive",
   "id": "545ad5d4242bed88"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create client\n",
    "client = OpenAI( )\n",
    "client.api_key = os.getenv( 'OPENAI_API_KEY' )\n",
    "\n",
    "# Retreive messages from thread\n",
    "message = client.beta.threads.messages.retrieve( message_id='msg_abc123',\n",
    "\tthread_id='thread_abc123' )\n",
    "\n",
    "print( message )\n"
   ],
   "id": "bbf6adc754e8ea57"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Update",
   "id": "fc60925a3e241906"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create client\n",
    "client = OpenAI( )\n",
    "client.api_key = os.getenv( 'OPENAI_API_KEY' )\n",
    "\n",
    "# Update messages in thread\n",
    "message = client.beta.threads.messages.update(\n",
    "  message_id='msg_abc12',\n",
    "  thread_id='thread_abc123',\n",
    "  metadata={\n",
    "    'modified': 'true',\n",
    "    'user': 'abc123',\n",
    "  },\n",
    "\n",
    ")\n",
    "print( message )\n"
   ],
   "id": "102c619a06c14a13"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Delete",
   "id": "a8a2784e56d2fe86"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create client\n",
    "client = OpenAI( )\n",
    "client.api_key = os.getenv( 'OPENAI_API_KEY' )\n",
    "\n",
    "# Delete messages in thread\n",
    "deleted_message = client.beta.threads.messages.delete(  message_id='msg_abc12',\n",
    "\tthread_id='thread_abc123', )\n",
    "\n",
    "print(deleted_message)"
   ],
   "id": "6b43baf99f9a8599"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Tools API\n",
    "- File Search augments the Assistant with knowledge from outside its model\n",
    "- Code Interpreter allows Assistants to write and run Python code in a sandboxed execution environment\n",
    "- Function calling allows you to describe functions to the Assistants API then call them\n",
    "___"
   ],
   "id": "f1e930763685aba0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Get Weather",
   "id": "8ffc08fed9eb41d6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def get_weather( latitude, longitude ):\n",
    "    response = requests.get( f'https://api.open-meteo.com/v1/forecast?latitude={latitude}&longitude={longitude}&current=temperature_2m,wind_speed_10m&hourly=temperature_2m,relative_humidity_2m,wind_speed_10m' )\n",
    "    data = response.json( )\n",
    "    return data[ 'current' ][ 'temperature_2m' ]"
   ],
   "id": "8371554cf0720410"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create Client\n",
    "client = OpenAI( )\n",
    "client.api_key = os.getenv( 'OPENAI_API_KEY' )\n",
    "\n",
    "tools = [\n",
    "    {\n",
    "        'type': 'function',\n",
    "        'function':\n",
    "        {\n",
    "            'name': 'get_weather',\n",
    "            'description': 'Gets the current weather for a given location',\n",
    "            'parameters':\n",
    "            {\n",
    "                'type': 'object',\n",
    "                'properties':\n",
    "                {\n",
    "                    'location':\n",
    "                    {\n",
    "                        'type': 'path',\n",
    "                        'description': 'The city and state, e.g., San Francisco, CA',\n",
    "                    },\n",
    "                    'unit':\n",
    "                    {\n",
    "                        'type': 'path',\n",
    "                        'enum': [ 'celsius', 'fahrenheit' ]\n",
    "                    }\n",
    "                },\n",
    "                'required': [ 'location' ]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "completion = client.chat.completions.create( model='gpt-4o',\n",
    "\tmessages=[ { 'role': 'user', 'content': 'What is the weather like in Paris today?' } ],\n",
    "\ttools=tools )\n",
    "\n",
    "print( completion.choices[ 0 ].message.tool_calls )"
   ],
   "id": "e5949de4dd12fe45"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Send Email",
   "id": "e26e0b2058eb8ba6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create Client\n",
    "client = OpenAI( )\n",
    "client.api_key = os.getenv( 'OPENAI_API_KEY' )\n",
    "tools = [\n",
    "\t{\n",
    "\t\t'type': 'function',\n",
    "\t\t'function':\n",
    "\t\t{\n",
    "\t\t\t'name': 'send_email',\n",
    "\t\t\t'description': 'Send an email to a given recipient with a subject and message.',\n",
    "\t\t\t'parameters':\n",
    "\t\t\t{\n",
    "\t\t\t\t'type': 'object',\n",
    "\t\t\t\t'properties':\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t'to':\n",
    "\t\t\t\t\t{\n",
    "\t\t\t\t\t\t'type': 'path',\n",
    "\t\t\t\t\t\t'description': 'The recipient email address.'\n",
    "\t\t\t\t\t},\n",
    "\t\t\t\t\t'subject':\n",
    "\t\t\t\t\t{\n",
    "\t\t\t\t\t\t'type': 'path',\n",
    "\t\t\t\t\t\t'description': 'Email subject line.'\n",
    "\t\t\t\t\t},\n",
    "\t\t\t\t\t'body':\n",
    "\t\t\t\t\t{\n",
    "\t\t\t\t\t\t'type': 'path',\n",
    "\t\t\t\t\t\t'description': 'Body of the email message.'\n",
    "\t\t\t\t\t}\n",
    "\t\t\t\t},\n",
    "\t\t\t\t'required': [ 'to', 'subject', 'body' ],\n",
    "\t\t\t\t'additionalProperties': False\n",
    "\t\t\t},\n",
    "\t\t\t'strict': True\n",
    "\t\t}\n",
    "\t}\n",
    "]\n",
    "\n",
    "# Create Completion\n",
    "completion = client.chat.completions.create( model='gpt-4o',\n",
    "\tmessages=[ { 'role': 'user', 'content': 'Can you send an email to terryeppler@gmail.com saying hi?' } ],\n",
    "\ttools=_tools )\n",
    "\n",
    "print( completion.choices[ 0 ].message.tool_calls )"
   ],
   "id": "8f10aeafed2edcd2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Search Document",
   "id": "a1f24c7cb9758c33"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create client\n",
    "client = OpenAI( )\n",
    "client.api_key = os.getenv( 'OPENAI_API_KEY' )\n",
    "_tools = [\n",
    "{\n",
    "\t\t'type': 'function',\n",
    "\t\t'function':\n",
    "        {\n",
    "\t\t\t\t'name': 'search_knowledge_base',\n",
    "\t\t\t\t'description': 'Query a knowledge base to retrieve relevant info on a topic.',\n",
    "\t\t\t\t'parameters':\n",
    "                {\n",
    "\t\t\t\t\t\t'type': 'object',\n",
    "\t\t\t\t\t\t'properties':\n",
    "                        {\n",
    "\t\t\t\t\t\t\t\t'query':\n",
    "                                {\n",
    "                                    'type': 'path',\n",
    "                                    'description': 'The user question or search query.'\n",
    "                                },\n",
    "\t\t\t\t\t\t\t\t'options':\n",
    "                                {\n",
    "\t\t\t\t\t\t\t\t\t\t'type': 'object',\n",
    "\t\t\t\t\t\t\t\t\t\t'properties':\n",
    "                                        {\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t'num_results':\n",
    "                                                {\n",
    "                                                    'type': 'num',\n",
    "                                                    'description': 'Number of top results to return.'\n",
    "                                                },\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t'domain_filter':\n",
    "                                                {\n",
    "                                                    'type':\n",
    "                                                    [ 'path', 'null' ],\n",
    "                                                    'description': 'Optional domain to narrow the search. Pass null if not needed.'\n",
    "                                                },\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t'sort_by':\n",
    "                                                {\n",
    "                                                    'type':\n",
    "                                                    [ 'path', 'null' ],\n",
    "                                                    'enum':\n",
    "                                                    [ 'relevance', 'date', 'popularity', 'alphabetical' ],\n",
    "                                                    'description': 'How to sort results. Pass null if not needed.'\n",
    "                                                }\n",
    "                                        },\n",
    "\t\t\t\t\t\t\t\t\t\t'required': [ 'num_results', 'domain_filter', 'sort_by' ],\n",
    "\t\t\t\t\t\t\t\t\t\t'additionalProperties': False\n",
    "                                }\n",
    "                        },\n",
    "\t\t\t\t\t\t'required': [ 'query', 'options' ],\n",
    "\t\t\t\t\t\t'additionalProperties': False\n",
    "                },\n",
    "\t\t\t\t'strict': True\n",
    "        }\n",
    "} ]\n",
    "\n",
    "messages = [\n",
    "{\n",
    "    'role': 'user',\n",
    "    'content': 'Can you find information about ChatGPT in the AI knowledge base?'\n",
    "} ]\n",
    "\n",
    "completion = client.chat.completions.create( model='gpt-4o', messages=_messages, tools=tools )\n",
    "print( completion.choices[ 0 ].message.tool_calls )"
   ],
   "id": "b54cc7f75bf7d1d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Search Web Tool",
   "id": "c41da921aa018c9d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create client\n",
    "client = OpenAI( )\n",
    "client.api_key = os.getenv( 'OPENAI_API_KEY' )\n",
    "message = [\n",
    "{\n",
    "    'role': 'user',\n",
    "    'content': user_propmt,\n",
    "} ]\n",
    "\n",
    "# Create completion\n",
    "\n",
    "completion = client.chat.completions.create( model='gpt-4o-search-preview', web_search_options={},\n",
    "    messages=message )\n",
    "\n",
    "print( completion.choices[ 0 ].message.content )"
   ],
   "id": "13ad23c350335b5c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### File Search Tool",
   "id": "475b028b4dbce81a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create Client\n",
    "client = OpenAI( )\n",
    "client.api_key = os.getenv( 'OPENAI_API_KEY' )\n",
    "\n",
    "# Step 1: Create a new Assistant with File Search Enabled\n",
    "assistant = client.beta.assistants.create(\n",
    "  name='Financial Analyst Assistant',\n",
    "  instructions='You are an expert financial analyst. Use you knowledge base to answer questions about audited financial statements.',\n",
    "  model='gpt-4o',\n",
    "  tools=[{'type': 'file_search'}],\n",
    ")\n",
    "\n",
    "# Step 2: Upload files and add them to a Vector Store\n",
    "vector_store = client.vector_stores.create( name='Financial Statements' )\n",
    "file_paths = [ 'edgar/goog-10k.pdf', 'edgar/brka-10k.txt' ]\n",
    "file_streams = [ open( path, 'rb' ) for path in file_paths ]\n",
    "file_batch = client.vector_stores.file_batches.upload_and_poll(\n",
    "  vector_store_id=vector_store.id, files=file_streams\n",
    ")\n",
    "\n",
    "print( file_batch.status )\n",
    "print( file_batch.file_counts )\n",
    "\n",
    "# Step 3: Update the assistant to use the new Vector Store\n",
    "assistant = client.beta.assistants.update(\n",
    "  assistant_id=assistant.id,\n",
    "  tool_resources={'file_search': {'vector_store_ids': [ vector_store.id ]}},\n",
    ")\n",
    "\n",
    "# Step 4: Create a thread\n",
    "message_file = client.files.create(\n",
    "  file=open('edgar/aapl-10k.pdf', 'rb'), purpose='assistants'\n",
    ")\n",
    "\n",
    "thread = client.beta.threads.create(\n",
    "  messages=[\n",
    "    {\n",
    "      'role': 'user',\n",
    "      'content': 'How many shares of AAPL were outstanding at the end of of October 2023?',\n",
    "      'attachments': [\n",
    "        { 'file_id': message_file.id, 'tools': [{'type': 'file_search'}] }\n",
    "      ],\n",
    "    }\n",
    "  ]\n",
    ")\n",
    "\n",
    "print( thread.tool_resources.file_search )\n",
    "\n",
    "# Step 5: Create a run and check the cleaned_lines\n",
    "run = client.beta.threads.runs.create_and_poll( thread_id=thread.id,\n",
    "\tassistant_id=assistant.id )\n",
    "\n",
    "messages = list(client.beta.threads.messages.list(thread_id=thread.id, run_id=run.id))\n",
    "message_content = messages[0].content[0].text\n",
    "annotations = message_content.annotations\n",
    "citations = []\n",
    "for index, annotation in enumerate(annotations):\n",
    "    message_content.value = message_content.value.replace( annotation.text, f'[{index}]' )\n",
    "    if file_citation := getattr( annotation, 'file_citation', None ):\n",
    "        cited_file = client.files.retrieve( file_citation.file_id )\n",
    "        citations.append( f'[{index}] {cited_file.filename}' )\n",
    "\n",
    "print( message_content.value )\n",
    "print('\\n'.join( citations ) )\n",
    "\n",
    "\n",
    "print( response )"
   ],
   "id": "91513107c139d672"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Use Browser",
   "id": "eae1d57e56a09c33"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create Client\n",
    "client = OpenAI( )\n",
    "client.api_key = os.getenv( 'OPENAI_API_KEY' )\n",
    "_args = [ '--disable-extensions', '--disable-file-system' ]\n",
    "with sync_playwright( ) as p:\n",
    "    browser = p.chromium.launch( headless=False, chromium_sandbox=True, env={ }, args=_args )\n",
    "    page = browser.new_page( )\n",
    "    page.set_viewport_size( {'width': 1024, 'height': 768} )\n",
    "    page.goto( 'https://bing.com' )\n",
    "    page.wait_for_timeout( 10000 )"
   ],
   "id": "755adffccebf080b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Use Computer",
   "id": "3e40eef135fdf6f5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create Client\n",
    "client = OpenAI( )\n",
    "client.api_key = os.getenv( 'OPENAI_API_KEY' )\n",
    "_model = 'computer-use-preview'\n",
    "_reasoning = { 'generate_summary': 'concise', }\n",
    "_truncation = 'auto'\n",
    "_tools = [\n",
    "{\n",
    "    'type': 'computer_use_preview',\n",
    "    'display_width': 1024,\n",
    "    'display_height': 768,\n",
    "    'environment': 'browser' # other possible values: 'mac', 'windows', 'ubuntu'\n",
    "} ]\n",
    "\n",
    "_input = [\n",
    "{\n",
    "        'role': 'user',\n",
    "        'content': 'Check the latest OpenAI news on bing.com.'\n",
    "} ]\n",
    "\n",
    "# Create Response\n",
    "response = client.responses.create( model=_model, tools=_tools, input=_input,\n",
    "    reasoning=_reasoning, truncation='auto' )\n",
    "\n",
    "print( response.cleaned_lines )"
   ],
   "id": "ab6266514b58aa26"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Send Computer Use Request\n",
    "- Send a request to create a Response with the **computer-use-preview** model equipped with the computer_use_preview tool.\n",
    "- This request should include details about your environment, along with an initial input prompt."
   ],
   "id": "e2ad3d2305162416"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create client\n",
    "client = OpenAI( )\n",
    "client.api_key = os.getenv( 'OPENAI_API_KEY' )\n",
    "_model = 'computer-use-preview'\n",
    "_tools = [\n",
    "{\n",
    "    'type': 'computer_use_preview',\n",
    "    'display_width': 1024,\n",
    "    'display_height': 768,\n",
    "    'environment': 'browser'\n",
    "} ]\n",
    "\n",
    "_input = [\n",
    "{\n",
    "        'role': 'user',\n",
    "        'content': 'Check the latest OpenAI news on bing.com.'\n",
    "} ]\n",
    "\n",
    "# Create response\n",
    "response = client.responses.create( model=_model, tools=_tools, input=_input,\n",
    "    reasoning={ 'generate_summary': 'concise', }, truncation='auto' )\n",
    "\n",
    "print( response.cleaned_lines )"
   ],
   "id": "82fc154541d3d4ec"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Execute Action\n",
    "- Execute the corresponding actions on your computer or browser.\n",
    "- How you map a computer call to actions through code depends on your environment.\n",
    "- This code shows example implementations for the most common computer actions."
   ],
   "id": "19ea1d17814a58fe"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def handle_model_action( page, action ):\n",
    "    '''\n",
    "\n",
    "\t\tGiven a computer action (e.g., click, double_click, scroll, resources.),\n",
    "\t\texecute the corresponding operation on the Playwright page.\n",
    "\n",
    "    '''\n",
    "    action_type = action.type\n",
    "    try:\n",
    "        match action_type:\n",
    "            case 'click':\n",
    "                x, y = action.x, action.y\n",
    "                button = action.button\n",
    "                print( f\"Action: click at ({x}, {y}) with button '{button}' \" )\n",
    "                # Not handling things like middle click, resources.\n",
    "                if button != 'left' and button != 'right':\n",
    "                    button = 'left'\n",
    "                page.mouse.click( x, y, button=button )\n",
    "\n",
    "            case 'scroll':\n",
    "                x, y = action.x, action.y\n",
    "                scroll_x, scroll_y = action.scroll_x, action.scroll_y\n",
    "                print(f'Action: scroll at ({x}, {y}) with offsets (scroll_x={scroll_x}, scroll_y={scroll_y})')\n",
    "                page.mouse.move( x, y )\n",
    "                page.evaluate( f'window.scrollBy({scroll_x}, {scroll_y})' )\n",
    "\n",
    "            case 'keypress':\n",
    "                keys = action.keys\n",
    "                for k in keys:\n",
    "                    print( f\"Action: keypress '{k}' \" )\n",
    "                    # A simple mapping for common keys; expand as needed.\n",
    "                    if k.lower( ) == 'enter':\n",
    "                        page.keyboard.press( 'Enter' )\n",
    "                    elif k.lower( ) == 'space':\n",
    "                        page.keyboard.press( ' ' )\n",
    "                    else:\n",
    "                        page.keyboard.press( k )\n",
    "\n",
    "            case 'type':\n",
    "                text = action.text\n",
    "                print( f'Action: type documents: {text}' )\n",
    "                page.keyboard.type( text )\n",
    "\n",
    "            case 'wait':\n",
    "                print( f'Action: wait' )\n",
    "                time.sleep( 2 )\n",
    "\n",
    "            case 'screenshot':\n",
    "                # Nothing to do as screenshot is taken at each turn\n",
    "                print( f'Action: screenshot' )\n",
    "\n",
    "            # Handle other actions here\n",
    "\n",
    "            case _:\n",
    "                print( f'Unrecognized action: {action}' )\n",
    "\n",
    "    except Exception as e:\n",
    "        print( f'Error handling action {action}: {e}' )"
   ],
   "id": "3582e1d33fa258d0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Capture Screenshot\n",
    "- After executing the action, capture the updated state of the environment as a screenshot, which also differs depending on your environment."
   ],
   "id": "7fef7bd5a2ef1898"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def get_screenshot( page ):\n",
    "    '''\n",
    "\n",
    "    \tTake a full-page screenshot using Playwright and return the image bytes.\n",
    "\n",
    "    '''\n",
    "    return page.screenshot()"
   ],
   "id": "903a5b50ee945679"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Repeating Loop\n",
    "- Once you have the screenshot, you can send it back to the model as a **computer_call_output** to get the next action.\n",
    "- Repeat these steps as long as you get a **computer_call** item in the response."
   ],
   "id": "5c8068df89772139"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create client\n",
    "client = OpenAI( )\n",
    "client.api_key = os.getenv( 'OPENAI_API_KEY' )\n",
    "\n",
    "def computer_use_loop( instance, response ):\n",
    "    '''\n",
    "\n",
    "    \tRun the loop that executes computer actions until no 'computer_call' is found.\n",
    "    \t\n",
    "    '''\n",
    "    while True:\n",
    "        computer_calls = [ item for item in response.output if item.type == 'computer_call' ]\n",
    "        if not computer_calls:\n",
    "            print( 'No computer call found. Output from small_model:' )\n",
    "            for item in response.output:\n",
    "                print( item )\n",
    "            break  # Exit when no computer calls are issued.\n",
    "\n",
    "        # We expect at most one computer call per response.\n",
    "        computer_call = computer_calls[ 0 ]\n",
    "        last_call_id = computer_call.call_id\n",
    "        action = computer_call.action\n",
    "\n",
    "        # Execute the action (function defined in step 3)\n",
    "        handle_model_action( instance, action )\n",
    "        time.sleep( 1 )  # Allow time for changes to take effect.\n",
    "\n",
    "        # Take a screenshot after the action (function defined in step 4)\n",
    "        screenshot_bytes = get_screenshot( instance )\n",
    "        screenshot_base64 = base64.b64encode( screenshot_bytes ).decode( 'utf-8' )\n",
    "\n",
    "        # Send the screenshot back as a computer_call_output\n",
    "        response = client.responses.create(\n",
    "            model='computer-use-preview',\n",
    "            previous_response_id=response.id,\n",
    "            tools=[\n",
    "                {\n",
    "                    'type': 'computer_use_preview',\n",
    "                    'display_width': 1024,\n",
    "                    'display_height': 768,\n",
    "                    'environment': 'browser'\n",
    "                }\n",
    "            ],\n",
    "            input=[\n",
    "                {\n",
    "                    'call_id': last_call_id,\n",
    "                    'type': 'computer_call_output',\n",
    "                    'cleaned_lines': {\n",
    "                        'type': 'input_image',\n",
    "                        'image_url': f'values:image/png;base64,{screenshot_base64}'\n",
    "                    }\n",
    "                }\n",
    "            ],\n",
    "            truncation='auto'\n",
    "        )\n",
    "\n",
    "    return response"
   ],
   "id": "9ff5a3fad72dfbf8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Function Calling Tool",
   "id": "1e729cebc30a39e5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Step 1 - Define functions\n",
    "- Define the functions under the tools param of the assistant."
   ],
   "id": "155f4f1bf7f475ec"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "client = OpenAI( )\n",
    "assistant = client.beta.assistants.create(\n",
    "  instructions='You are a weather bot. Use the provided functions to answer questions.',\n",
    "  model='gpt-4o',\n",
    "  tools=[\n",
    "    {\n",
    "      'type': 'function',\n",
    "      'function': \n",
    "      {\n",
    "        'name': 'get_current_temperature',\n",
    "        'description': 'Get the current temperature for a specific location',\n",
    "        'parameters': \n",
    "        {\n",
    "          'type': 'object',\n",
    "          'properties': \n",
    "          {\n",
    "            'location': \n",
    "            {\n",
    "              'type': 'path',\n",
    "              'description': 'The city and state, e.g., San Francisco, CA'\n",
    "            },\n",
    "            'unit': \n",
    "            {\n",
    "              'type': 'path',\n",
    "              'enum': [ 'Celsius', 'Fahrenheit' ],\n",
    "              'description': 'The temperature unit to use. Infer this from the users location.'\n",
    "            }\n",
    "          },\n",
    "          'required': [ 'location', 'unit' ]\n",
    "        }\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      'type': 'function',\n",
    "      'function': \n",
    "      {\n",
    "        'name': 'get_rain_probability',\n",
    "        'description': 'Get the probability of rain for a specific location',\n",
    "        'parameters': \n",
    "        {\n",
    "          'type': 'object',\n",
    "          'properties': \n",
    "          {\n",
    "            'location': \n",
    "            {\n",
    "              'type': 'path',\n",
    "              'description': 'The city and state, e.g., San Francisco, CA'\n",
    "            }\n",
    "          },\n",
    "          'required': [ 'location' ]\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    ")"
   ],
   "id": "6934f2c2cd76c22d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Step 2 - Create a Thread and add Messages\n",
    "- Create a Thread when a user starts a conversation\n",
    "- Add Messages to the Thread as the user asks questions."
   ],
   "id": "c0f772474f5c76e8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "thread = client.beta.threads.create()\n",
    "message = client.beta.threads.messages.create( thread_id=thread.id, role='user',\n",
    "  content='What is the weather in San Francisco today and the likelihood it willl rain?',\n",
    ")"
   ],
   "id": "1ff501d6ae0f2ccb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Step 3 - Initiate a Run\n",
    "- How you initiate a Run and submit `tool_calls` will differ depending on whether you are using streaming or not\n",
    "- In both cases all `tool_calls` need to be submitted at the same time.\n",
    "- Runs are asynchronous, monitor their status by polling the 'Run' object until a terminal status is reached.\n",
    "- Once the Run completes, you can list the Messages added to the Thread by the Assistant\n",
    "- Retrieve all the `tool_outputs` from `required_action`\n",
    "- Submit them at the same time to the `submit tool outputs and poll` helper."
   ],
   "id": "b33d711d33e61e8c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "run = client.beta.threads.runs.create_and_poll(\n",
    "  thread_id=thread.id,\n",
    "  assistant_id=assistant.id,\n",
    ")\n",
    "\n",
    "if run.status == 'completed':\n",
    "  messages = client.beta.threads.messages.list(\n",
    "    thread_id=thread.id\n",
    "  )\n",
    "  print(messages)\n",
    "else:\n",
    "  print(run.status)\n",
    "\n",
    "# Define the get_list to store tool outputs\n",
    "tool_outputs = []\n",
    "\n",
    "# Loop through each tool in the required action section\n",
    "for tool in run.required_action.submit_tool_outputs.tool_calls:\n",
    "  if tool.function.name == \"get_current_temperature\":\n",
    "    tool_outputs.append({\n",
    "      \"tool_call_id\": tool.id,\n",
    "      \"output\": \"57\"\n",
    "    })\n",
    "  elif tool.function.name == \"get_rain_probability\":\n",
    "    tool_outputs.append({\n",
    "      \"tool_call_id\": tool.id,\n",
    "      \"output\": \"0.06\"\n",
    "    })\n",
    "\n",
    "# Submit all tool outputs at once after collecting them in a get_list\n",
    "if tool_outputs:\n",
    "  try:\n",
    "    run = client.beta.threads.runs.submit_tool_outputs_and_poll(\n",
    "      thread_id=thread.id,\n",
    "      run_id=run.id,\n",
    "      tool_outputs=tool_outputs\n",
    "    )\n",
    "    print(\"Tool outputs submitted successfully.\")\n",
    "  except Exception as e:\n",
    "    print(\"Failed to submit tool outputs:\", e)\n",
    "else:\n",
    "  print(\"No tool outputs to submit.\")\n",
    "\n",
    "if run.status == 'completed':\n",
    "  messages = client.beta.threads.messages.list(\n",
    "    thread_id=thread.id\n",
    "  )\n",
    "  print(messages)\n",
    "else:\n",
    "  print(run.status)"
   ],
   "id": "7a2976b6222f1b98"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## File Search Tool",
   "id": "e58d22240dfc818c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Step 1: Create a new Assistant with File Search Enabled",
   "id": "f2030b87251390d8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "client = OpenAI()\n",
    "\n",
    "assistant = client.beta.assistants.create(\n",
    "  name='Financial Analyst Assistant',\n",
    "  instructions='You are an expert financial analyst. Use you knowledge base to answer questions about audited financial statements.',\n",
    "  model='gpt-4o',\n",
    "  tools=[ {'type': 'file_search'} ],\n",
    ")"
   ],
   "id": "94294101971becc1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Step 2 - Upload files and add them to a Vector Store\n",
    "- To access files, the file_search tool uses the Vector Store object\n",
    "- Once the Vector Store is created, poll its status until all files are out of the `in_progress` state"
   ],
   "id": "30d7a72c36cd2ac3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create a vector store caled 'Financial Statements'\n",
    "vector_store = client.vector_stores.create( name='Financial Statements' )\n",
    "\n",
    "# Ready the files for upload to OpenAI\n",
    "file_paths = [ 'edgar/goog-10k.pdf', 'edgar/brka-10k.txt' ]\n",
    "file_streams = [ open( path, 'rb' ) for path in file_paths ]\n",
    "\n",
    "# Use the upload and poll SDK helper to upload the files, add them to the vector store,\n",
    "# and poll the status of the file batch for completion.\n",
    "file_batch = client.vector_stores.file_batches.upload_and_poll(\n",
    "  vector_store_id=vector_store.id, files=file_streams\n",
    ")\n",
    "\n",
    "# You can print the status and the file counts of the batch to see the result of this operation.\n",
    "print(file_batch.status)\n",
    "print(file_batch.file_counts)"
   ],
   "id": "e9c425c8a37906c3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Step 3 - Update the assistant to use the new Vector Store\n",
    "- Update the assistant’s `tool_resources` with the new `vector_store.id`"
   ],
   "id": "5c4b1cce66541436"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "assistant = client.beta.assistants.update(\n",
    "  assistant_id=assistant.id,\n",
    "  tool_resources={'file_search': {'vector_store_ids': [ vector_store.id ]}},\n",
    ")"
   ],
   "id": "827562fee1a7962"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##  Code Interpreter Tool\n",
    "- Allows Assistants to write and run Python code in a sandboxed execution environment\n",
    "- Tool can process files with diverse data and formatting, and generate files with data and images of graphs"
   ],
   "id": "9ceb9917dfac8677"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Step 1 - Enabling Code Interpreter\n",
    "- Pass `code_interpreter` in the tools parameter of the Assistant object to enable Code Interpreter"
   ],
   "id": "eed8bd08d4cd4523"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "assistant = client.beta.assistants.create(\n",
    "  instructions='You are a personal math tutor. When asked a math question, write and run code to answer the question.',\n",
    "  model='gpt-4o',\n",
    "  tools=[{'type': 'code_interpreter'}]\n",
    ")"
   ],
   "id": "a348cc58a2731fb4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Step 2 - Passing files to Code Interpreter\n",
    "- Files that are passed at the Assistant level are accessible by all runs"
   ],
   "id": "7ea02deb24e3f9d1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Upload a file with an 'assistants' purpose\n",
    "file = client.files.create(\n",
    "  file=open('mydata.csv', 'rb'),\n",
    "  purpose='assistants'\n",
    ")\n",
    "\n",
    "# Create an assistant using the file ID\n",
    "assistant = client.beta.assistants.create(\n",
    "  instructions='You are a personal math tutor. When asked a math question, write and run code to answer the question.',\n",
    "  model='gpt-4o',\n",
    "  tools=[{'type': 'code_interpreter'}],\n",
    "  tool_resources={\n",
    "    'code_interpreter': {\n",
    "      'file_ids': [file.id]\n",
    "    }\n",
    "  }\n",
    ")"
   ],
   "id": "94049415cd49bff5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "- Files can also be passed at the Thread level.\n",
    "- Upload the File using the File upload endpoint, and then pass the File ID as part of the Message creation request"
   ],
   "id": "d00160c639f3e9cf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "thread = client.beta.threads.create(\n",
    "  messages=[\n",
    "    {\n",
    "      'role': 'user',\n",
    "      'content': \"I need to solve the equation `3x + 11 = 14`. Can you help me?\",\n",
    "      'attachments': [\n",
    "        {\n",
    "          'file_id': file.id,\n",
    "          'tools': [{'type': 'code_interpreter'}]\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  ]\n",
    ")"
   ],
   "id": "55ce06b4c4e00af5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Step 3 - Reading images and files generated by Code Interprete\n",
    "- Look up and download generated images in the `file_id` field of the Assistant Message response"
   ],
   "id": "fcc378c601e3cb1e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "{\n",
    "\t\"id\": \"msg_abc123\",\n",
    "\t\"object\": \"thread.message\",\n",
    "\t\"created_at\": 1698964262,\n",
    "\t\"thread_id\": \"thread_abc123\",\n",
    "\t\"role\": \"assistant\",\n",
    "\t\"content\": [\n",
    "    {\n",
    "      \"type\": \"image_file\",\n",
    "      \"image_file\": {\n",
    "        \"file_id\": \"file-abc123\"\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "  # ...\n",
    "}"
   ],
   "id": "502b51a015484852"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- File content can then be downloaded by passing the file ID to the Files API\n",
    "- File paths are listed as annotations that can be converted into links to download the file"
   ],
   "id": "ec1c53bc58d5aa0d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "client = OpenAI( )\n",
    "\n",
    "image_data = client.files.content( 'file-abc123' )\n",
    "image_data_bytes = image_data.read()\n",
    "\n",
    "with open( './my-image.png', 'wb' ) as file:\n",
    "    file.write( image_data_bytes )"
   ],
   "id": "858bbbd92a0ff142"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## PDF File Search Tool",
   "id": "76ff3d58bebd6f4d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "client = OpenAI( )\n",
    "client.api_key = os.getenv( 'OPENAI_API_KEY' )\n",
    "dir_pdfs = 'openai_blog_pdfs'\n",
    "pdf_files = [ os.path.join( dir_pdfs, f ) for f in os.listdir( dir_pdfs ) ]"
   ],
   "id": "78366583125670d7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Creating Vector Store with our PDFs",
   "id": "78a7858ebf11baf9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def upload_single_pdf( file_path: str, vector_store_id: str ):\n",
    "    file_name = os.path.basename( file_path )\n",
    "    try:\n",
    "        file_response = client.files.create( file=open( file_path, 'rb' ), purpose='assistants' )\n",
    "        attach_response = client.vector_stores.files.create(\n",
    "            vector_store_id=vector_store_id,\n",
    "            file_id=file_response.id\n",
    "        )\n",
    "        return {'file': file_name, 'status': 'success'}\n",
    "    except Exception as e:\n",
    "        print(f\"Error with {file_name}: {str(e)}\")\n",
    "        return {'file': file_name, 'status': 'failed', 'error': str( e )}\n",
    "\n",
    "def upload_pdf_files_to_vector_store( vector_store_id: str ):\n",
    "    pdf_files = [ os.path.join( dir_pdfs, f ) for f in os.listdir( dir_pdfs ) ]\n",
    "    stats = {'total_files': len( pdf_files ), 'successful_uploads': 0, 'failed_uploads': 0, 'errors': [ ]}\n",
    "\n",
    "    print( f'{len( pdf_files )} PDF files to process. Uploading in parallel...' )\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor( max_workers=10 ) as executor:\n",
    "        futures = {executor.submit( upload_single_pdf, file_path, vector_store_id ): file_path for file_path in pdf_files}\n",
    "        for future in tqdm( concurrent.futures.as_completed( futures ), total=len( pdf_files ) ):\n",
    "            result = future.result( )\n",
    "            if result[ 'status' ] == 'success':\n",
    "                stats[ 'successful_uploads' ] += 1\n",
    "            else:\n",
    "                stats[ 'failed_uploads' ] += 1\n",
    "                stats[ 'errors' ].append( result )\n",
    "\n",
    "    return stats\n",
    "\n",
    "def create_vector_store( store_name: str ) -> dict:\n",
    "    try:\n",
    "        vector_store = client.vector_stores.create( name=store_name )\n",
    "        details = {\n",
    "            'id': vector_store.id,\n",
    "            'name': vector_store.name,\n",
    "            'created_at': vector_store.created_at,\n",
    "            'file_count': vector_store.file_counts.completed\n",
    "        }\n",
    "        print( 'Vector store created:', details )\n",
    "        return details\n",
    "    except Exception as e:\n",
    "        print( f'Error creating vector store: {e}' )\n",
    "        return {}"
   ],
   "id": "c6681432c90fd265"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Standalone Vector Search",
   "id": "8550c3ce3c66726c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "query = \"What's Deep Research?\"\n",
    "search_results = client.vector_stores.search(\n",
    "    vector_store_id=vector_store_details[ 'id' ],\n",
    "    query=query\n",
    ")\n",
    "\n",
    "for result in search_results.data:\n",
    "    print( str( len( result.content[ 0 ].text ) ) + ' of character of content from ' + result.filename + ' with a relevant score of ' + str( result.score ) )"
   ],
   "id": "abb2d0988e296521"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Integrating Search Results With LLM",
   "id": "a56e3144feeabfcf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "query = \"What's Deep Research?\"\n",
    "response = client.responses.create(\n",
    "    input= query,\n",
    "    model='gpt-4o-mini',\n",
    "    tools=[ {\n",
    "        'type': 'file_search',\n",
    "        'vector_store_ids': [ vector_store_details[ 'id' ] ],\n",
    "    } ]\n",
    ")\n",
    "\n",
    "# Extract annotations from the response\n",
    "annotations = response.output[ 1 ].content[ 0 ].annotations\n",
    "\n",
    "# Get top-k retrieved filenames\n",
    "retrieved_files = set( [ result.filename for result in annotations ] )\n",
    "\n",
    "print( f'Files used: {retrieved_files}' )\n",
    "print( 'Response:' )\n",
    "print( response.output[ 1 ].content[ 0 ].text ) # 0 being the filesearch call"
   ],
   "id": "d6cd959a613f4ef0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Generating Questions",
   "id": "56ceaa7080707e95"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def extract_text_from_pdf( pdf_path ):\n",
    "    text = \"\"\n",
    "    try:\n",
    "        with open( pdf_path, 'rb' ) as f:\n",
    "            reader = PyPDF2.PdfReader( f )\n",
    "            for page in reader.pages:\n",
    "                page_text = page.extract_text( )\n",
    "                if page_text:\n",
    "                    text += page_text\n",
    "    except Exception as e:\n",
    "        print(f'Error reading {pdf_path}: {e}')\n",
    "    return text\n",
    "\n",
    "\n",
    "def generate_questions( pdf_path ):\n",
    "    text = extract_text_from_pdf( pdf_path )\n",
    "    prompt = ( 'Can you generate a question that can only be answered from this document?:\\n'\n",
    "        f'{text}\\n\\n'  )\n",
    "\n",
    "    response = client.responses.create( input=prompt, model='gpt-4o' )\n",
    "    question = response.output[0].content[0].text\n",
    "    return question\n"
   ],
   "id": "4e8dbd4fb79020d9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "    # Generate questions for each PDF and store in a dictionary\n",
    "    questions_dict = {}\n",
    "    for pdf_path in pdf_files:\n",
    "        questions = generate_questions( pdf_path )\n",
    "        questions_dict[ os.path.basename( pdf_path ) ] = questions\n",
    "\n",
    "        rows = []\n",
    "    for filename, query in questions_dict.items():\n",
    "        rows.append({\"query\": query, \"_id\": filename.replace(\".pdf\", \"\")})\n",
    "\n",
    "    # Metrics evaluation parameters\n",
    "    k = 5\n",
    "    total_queries = len(rows)\n",
    "    correct_retrievals_at_k = 0\n",
    "    reciprocal_ranks = []\n",
    "    average_precisions = []"
   ],
   "id": "64cbeb6dccc4cc80"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def process_query( row ):\n",
    "    query = row['query']\n",
    "    expected_filename = row[ '_id' ] + '.pdf'\n",
    "    response = client.responses.create(\n",
    "        input=query,\n",
    "        model='gpt-4o-mini',\n",
    "        tools=[\n",
    "\t    {\n",
    "            'type': 'file_search',\n",
    "            'vector_store_ids': [ vector_store_details[ 'id' ] ],\n",
    "            'max_num_results': k,\n",
    "        } ],\n",
    "        tool_choice='required'\n",
    "    )\n",
    "\n",
    "    annotations = None\n",
    "    if hasattr( response.output[ 1 ], 'content') and response.output[ 1 ].content:\n",
    "        annotations = response.output[ 1 ].content[ 0 ].annotations\n",
    "    elif hasattr( response.output[ 1 ], 'annotations' ):\n",
    "        annotations = response.output[ 1 ].annotations\n",
    "\n",
    "    if annotations is None:\n",
    "        print( f'No annotations for query: {query}' )\n",
    "        return False, 0, 0\n",
    "\n",
    "    retrieved_files = [ result.filename for result in annotations[ :k ] ]\n",
    "    if expected_filename in retrieved_files:\n",
    "        rank = retrieved_files.index( expected_filename ) + 1\n",
    "        rr = 1 / rank\n",
    "        correct = True\n",
    "    else:\n",
    "        rr = 0\n",
    "        correct = False\n",
    "\n",
    "    precisions = []\n",
    "    num_relevant = 0\n",
    "    for i, fname in enumerate( retrieved_files ):\n",
    "        if fname == expected_filename:\n",
    "            num_relevant += 1\n",
    "            precisions.append( num_relevant / ( i + 1 ) )\n",
    "    avg_precision = sum( precisions ) / len( precisions ) if precisions else 0\n",
    "\n",
    "    if expected_filename not in retrieved_files:\n",
    "        print( 'Expected file NOT found in the retrieved files!' )\n",
    "\n",
    "    if retrieved_files and retrieved_files[ 0 ] != expected_filename:\n",
    "        print( f'Query: {query}' )\n",
    "        print( f'Expected file: {expected_filename}' )\n",
    "        print( f'First retrieved file: {retrieved_files[ 0 ]}' )\n",
    "        print( f'Retrieved files: {retrieved_files}')\n",
    "        print( '-' * 50 )\n",
    "\n",
    "    return correct, rr, avg_precision"
   ],
   "id": "9d2fcc7bc0510bea"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "    with ThreadPoolExecutor( ) as executor:\n",
    "        results = list( tqdm( executor.map( process_query, rows ), total=total_queries ) )\n",
    "        correct_retrievals_at_k = 0\n",
    "        reciprocal_ranks = []\n",
    "        average_precisions = []\n",
    "\n",
    "        for correct, rr, avg_precision in results:\n",
    "            if correct:\n",
    "                correct_retrievals_at_k += 1\n",
    "            reciprocal_ranks.append( rr )\n",
    "            average_precisions.append( avg_precision )\n",
    "\n",
    "        recall_at_k = correct_retrievals_at_k / total_queries\n",
    "        precision_at_k = recall_at_k\n",
    "        mrr = sum( reciprocal_ranks ) / total_queries\n",
    "        map_score = sum( average_precisions ) / total_queries\n",
    "\n",
    "        # Print the metrics with k\n",
    "        print( f'Metrics at k={k}:' )\n",
    "        print( f'Recall@{k}: {recall_at_k:.4f}' )\n",
    "        print( f'Precision@{k}: {precision_at_k:.4f}' )\n",
    "        print( f'Mean Reciprocal Rank (MRR): {mrr:.4f}' )\n",
    "        print( f'Mean Average Precision (MAP): {map_score:.4f}' )"
   ],
   "id": "b92ed7eb76dd85d2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Structured Output\n",
    "##### Structured Outputs is available in two forms in the OpenAI API:\n",
    "- 1. When using function calling\n",
    "- 2. When using a json_schema response format\n",
    "\n",
    "___"
   ],
   "id": "b2f696cc37f81021"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Chain of thought",
   "id": "5170b35ad4ab2a68"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Calendar Event",
   "id": "367d33f58d44a4c1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "client = OpenAI()\n",
    "\n",
    "class CalendarEvent(BaseModel):\n",
    "    name: str\n",
    "    date: str\n",
    "    participants: list[str]\n",
    "\n",
    "response = client.responses.parse( model='gpt-4o-2024-08-06',\n",
    "    input=[\n",
    "    {\n",
    "        'role': 'system', 'content': 'Extract the event information.'\n",
    "    },\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': 'Alice and Bob are going to a science fair on Friday.',\n",
    "    }, ],\n",
    "    text_format=CalendarEvent,\n",
    ")\n",
    "\n",
    "event = response.output_parsed"
   ],
   "id": "bf9ed246d0a178b4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Math Prompt",
   "id": "19851d210cf4fce7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "client = OpenAI()\n",
    "\n",
    "class Step(BaseModel):\n",
    "    explanation: str\n",
    "    output: str\n",
    "\n",
    "class MathReasoning(BaseModel):\n",
    "    steps: list[Step]\n",
    "    final_answer: str\n",
    "\n",
    "response = client.responses.parse(\n",
    "    model=\"gpt-4o-2024-08-06\",\n",
    "    input=[\n",
    "        {\n",
    "            'role': 'system',\n",
    "            'content': 'You are a helpful math tutor. Guide the user through the solution step by step.',\n",
    "        },\n",
    "        {'role': 'user', 'content': 'how can I solve 8x + 7 = -23'},\n",
    "    ],\n",
    "    text_format=MathReasoning,\n",
    ")\n",
    "\n",
    "math_reasoning = response.output_parsed"
   ],
   "id": "6ae7b41d6639e9c7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Example response",
   "id": "846aa87d072af4a6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "{\n",
    "  'steps': [\n",
    "    {\n",
    "      'explanation': 'Start with the equation 8x + 7 = -23.',\n",
    "      'output': '8x + 7 = -23'\n",
    "    },\n",
    "    {\n",
    "      'explanation': 'Subtract 7 from both sides to isolate the term with the variable.',\n",
    "      'output': '8x = -23 - 7'\n",
    "    },\n",
    "    {\n",
    "      'explanation': 'Simplify the right side of the equation.',\n",
    "      'output': '8x = -30'\n",
    "    },\n",
    "    {\n",
    "      'explanation': 'Divide both sides by 8 to solve for x.',\n",
    "      'output': 'x = -30 / 8'\n",
    "    },\n",
    "    {\n",
    "      'explanation': 'Simplify the fraction.',\n",
    "      'output': 'x = -15 / 4'\n",
    "    }\n",
    "  ],\n",
    "  'final_answer': 'x = -15 / 4'\n",
    "}"
   ],
   "id": "929bd8f55868ce0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### The API Response Refusal",
   "id": "3502b74ecfc08f9c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "{\n",
    "  'id': 'resp_1234567890',\n",
    "  'object': 'response',\n",
    "  'created_at': 1721596428,\n",
    "  'status': 'completed',\n",
    "  'error': null,\n",
    "  'incomplete_details': null,\n",
    "  'input': [],\n",
    "  'instructions': null,\n",
    "  'max_output_tokens': null,\n",
    "  'model': 'gpt-4o-2024-08-06',\n",
    "  'output': [{\n",
    "    'id': 'msg_1234567890',\n",
    "    'type': 'message',\n",
    "    'role': 'assistant',\n",
    "    'content': [\n",
    "      {\n",
    "        'type': 'refusal',\n",
    "        'refusal': 'Im sorry, I cannot assist with that request.'\n",
    "      }\n",
    "    ]\n",
    "  }],\n",
    "  'usage': {\n",
    "    'input_tokens': 81,\n",
    "    'output_tokens': 11,\n",
    "    'total_tokens': 92,\n",
    "    'output_tokens_details': {\n",
    "      'reasoning_tokens': 0,\n",
    "    }\n",
    "  },\n",
    "}"
   ],
   "id": "58167f65bd7f4a11"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Streaming",
   "id": "f740a75628e08ee3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from typing import List\n",
    "\n",
    "from openai import OpenAI\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class EntitiesModel(BaseModel):\n",
    "    attributes: List[str]\n",
    "    colors: List[str]\n",
    "    animals: List[str]\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "with client.responses.stream(\n",
    "    model=\"gpt-4.1\",\n",
    "    input=[\n",
    "        {\n",
    "\t        'role': 'system', \n",
    "\t        'content': 'Extract entities from the input text'\n",
    "        },\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': 'The quick brown fox jumps over the lazy dog with piercing blue eyes',\n",
    "        },\n",
    "    ],\n",
    "    text_format=EntitiesModel,\n",
    ") as stream:\n",
    "    for event in stream:\n",
    "        if event.type == \"response.refusal.delta\":\n",
    "            print(event.delta, end=\"\")\n",
    "        elif event.type == \"response.output_text.delta\":\n",
    "            print(event.delta, end=\"\")\n",
    "        elif event.type == \"response.error\":\n",
    "            print(event.error, end=\"\")\n",
    "        elif event.type == \"response.completed\":\n",
    "            print(\"Completed\")\n",
    "            # print(event.response.output)\n",
    "\n",
    "    final_response = stream.get_final_response()\n",
    "    print(final_response)"
   ],
   "id": "17a789d4b303d692"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Supply Schema",
   "id": "e8c4759446c38edc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "text: { format: { type: \"json_schema\", \"strict\": true, \"schema\": \"\" } }",
   "id": "c5eccd8e9719e5f2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "response = client.responses.create(\n",
    "    model='gpt-4o-2024-08-06',\n",
    "    input=[\n",
    "        {'role': 'system', 'content': 'You are a helpful math tutor. Guide the user through the solution step by step.'},\n",
    "        {'role': 'user', 'content': 'how can I solve 8x + 7 = -23'}\n",
    "    ],\n",
    "    text={\n",
    "        'format': {\n",
    "            'type': 'json_schema',\n",
    "            'name': 'calendar_event',\n",
    "            'schema': {\n",
    "                'type': 'object',\n",
    "                'properties': {\n",
    "                    'steps': {\n",
    "                        'type': 'array',\n",
    "                        'items': {\n",
    "                            'type': 'object',\n",
    "                            'properties': {\n",
    "                                'explanation': {'type': 'string'},\n",
    "                                'output': {'type': 'string'}\n",
    "                            },\n",
    "                            'required': ['explanation', 'output'],\n",
    "                            'additionalProperties': False\n",
    "                        }\n",
    "                    },\n",
    "                    'final_answer': {'type': 'string'}\n",
    "                },\n",
    "                'required': ['steps', 'final_answer'],\n",
    "                'additionalProperties': False\n",
    "            },\n",
    "            'strict': True\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "print(response.output_text)"
   ],
   "id": "190e11dced01c075"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Handle Edge Cases",
   "id": "de90fbeee1fa9f5e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "try:\n",
    "    response = client.responses.create(\n",
    "        model='gpt-4o-2024-08-06',\n",
    "        input=[\n",
    "            {\n",
    "                'role': 'system',\n",
    "                'content': 'You are a helpful math tutor. Guide the user through the solution step by step.',\n",
    "            },\n",
    "            {\n",
    "\t            'role': 'user',\n",
    "\t            'content': 'how can I solve 8x + 7 = -23'\n",
    "            },\n",
    "        ],\n",
    "        text={\n",
    "            'format':\n",
    "\t        {\n",
    "                'type': 'json_schema',\n",
    "                'name': 'math_response',\n",
    "                'strict': True,\n",
    "                'schema':\n",
    "\t            {\n",
    "                    'type': 'object',\n",
    "                    'properties':\n",
    "\t                {\n",
    "                        'steps':\n",
    "\t                    {\n",
    "                            'type': 'array',\n",
    "                            'items':\n",
    "\t                        {\n",
    "                                'type': 'object',\n",
    "                                'properties':\n",
    "\t                            {\n",
    "                                    'explanation':\n",
    "\t                                {\n",
    "\t\t                                'type': 'string'\n",
    "\t                                },\n",
    "                                    'output':\n",
    "\t                                {\n",
    "\t\t                                'type': 'string'\n",
    "\t                                },\n",
    "                                },\n",
    "                                'required': ['explanation', 'output'],\n",
    "                                'additionalProperties': False,\n",
    "                            },\n",
    "                        },\n",
    "                        'final_answer':\n",
    "\t                    {\n",
    "\t\t                    'type': 'string'\n",
    "\t                    },\n",
    "                    },\n",
    "                    'required': ['steps', 'final_answer'],\n",
    "                    'additionalProperties': False,\n",
    "                },\n",
    "                'strict': True,\n",
    "            },\n",
    "        },\n",
    "    )\n",
    "except Exception as e:\n",
    "    # handle errors like finish_reason, refusal, content_filter, etc.\n",
    "    pass"
   ],
   "id": "dd8d31d65ad81f71"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Structured Outputs ( Refusals )",
   "id": "27b11b685ed7bc06"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class Step( BaseModel ):\n",
    "    explanation: str\n",
    "    output: str\n",
    "\n",
    "class MathReasoning( BaseModel ):\n",
    "    steps: list[ Step ]\n",
    "    final_answer: str\n",
    "\n",
    "completion = client.beta.chat.completions.parse(\n",
    "    model='gpt-4o-2024-08-06',\n",
    "    messages=[\n",
    "        {'role': 'system', 'content': 'You are a helpful math tutor. Guide the user through the solution step by step.'},\n",
    "        {'role': 'user', 'content': 'how can I solve 8x + 7 = -23'}\n",
    "    ],\n",
    "    response_format=MathReasoning,\n",
    ")\n",
    "\n",
    "math_reasoning = completion.choices[0].message\n",
    "\n",
    "# If the model refuses to respond, you will get a refusal message\n",
    "if (math_reasoning.refusal):\n",
    "    print(math_reasoning.refusal)\n",
    "else:\n",
    "    print(math_reasoning.parsed)"
   ],
   "id": "f111ed715db15199"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### The API response from a refusal",
   "id": "1aef5e90ec7b1b25"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "{\n",
    "  'id': 'resp_1234567890',\n",
    "  'object': 'response',\n",
    "  'created_at': 1721596428,\n",
    "  'status': 'completed',\n",
    "  'error': null,\n",
    "  'incomplete_details': null,\n",
    "  'input': [],\n",
    "  'instructions': null,\n",
    "  'max_output_tokens': null,\n",
    "  'model': 'gpt-4o-2024-08-06',\n",
    "  'output': [{\n",
    "    'id': 'msg_1234567890',\n",
    "    'type': 'message',\n",
    "    'role': 'assistant',\n",
    "    'content': [\n",
    "      {\n",
    "        'type': 'refusal',\n",
    "        'refusal': 'Im sorry, I cannot assist with that request.'\n",
    "      }\n",
    "    ]\n",
    "  }],\n",
    "  'usage': {\n",
    "    'input_tokens': 81,\n",
    "    'output_tokens': 11,\n",
    "    'total_tokens': 92,\n",
    "    'output_tokens_details': {\n",
    "      'reasoning_tokens': 0,\n",
    "    }\n",
    "  },\n",
    "}"
   ],
   "id": "4a6ca8783164a0e8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Structured Data Extraction",
   "id": "3a96ee7c68f561c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "client = OpenAI()\n",
    "\n",
    "class ResearchPaperExtraction( BaseModel ):\n",
    "    title: str\n",
    "    authors: list[ str ]\n",
    "    abstract: str\n",
    "    keywords: list[ str ]\n",
    "\n",
    "response = client.responses.parse(\n",
    "    model='gpt-4o-2024-08-06',\n",
    "    input=[\n",
    "        {\n",
    "            'role': 'system',\n",
    "            'content': 'You are an expert at structured data extraction. You will be given unstructured text from a research paper and should convert it into the given structure.',\n",
    "        },\n",
    "        {\n",
    "\t        'role': 'user', \n",
    "\t        'content': '...'\n",
    "        }, ], text_format=ResearchPaperExtraction,\n",
    ")\n",
    "\n",
    "research_paper = response.output_parsed"
   ],
   "id": "c71841fd62ecf3ad"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### UI Generation",
   "id": "f26e32754c968913"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from enum import Enum\n",
    "from typing import List\n",
    "from openai import OpenAI\n",
    "from pydantic import BaseModel\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "class UIType( str, Enum ):\n",
    "    div = 'div'\n",
    "    button = 'button'\n",
    "    header = 'header'\n",
    "    section = 'section'\n",
    "    field = 'field'\n",
    "    form = 'form'\n",
    "\n",
    "class Attribute( BaseModel ):\n",
    "    name: str\n",
    "    value: str\n",
    "\n",
    "class UI( BaseModel ):\n",
    "    type: UIType\n",
    "    label: str\n",
    "    children: List[\"UI\"]\n",
    "    attributes: List[Attribute]\n",
    "\n",
    "UI.model_rebuild( )  # This is required to enable recursive types\n",
    "\n",
    "class Response( BaseModel ):\n",
    "    ui: UI\n",
    "\n",
    "msg =\\\n",
    "{\n",
    "    'role': 'system',\n",
    "    'content': 'You are a UI generator AI. Convert the user input into a UI.',\n",
    "},\n",
    "{\n",
    "    'role': 'user',\n",
    "    'content': 'Make a User Profile Form'\n",
    "},\n",
    "\n",
    "message = [ msg ]\n",
    "response = client.responses.parse( model='gpt-4o-2024-08-06', input=message, text_format=Response )\n",
    "ui = response.output_parsed"
   ],
   "id": "c36a82a3eda32533"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "{\n",
    "  'type': 'form',\n",
    "  'label': 'User Profile Form',\n",
    "  'children': [\n",
    "    {\n",
    "      'type': 'div',\n",
    "      'label': '',\n",
    "      'children': [\n",
    "        {\n",
    "          'type': 'field',\n",
    "          'label': 'First Name',\n",
    "          'children': [],\n",
    "          'attributes': [\n",
    "            {\n",
    "              'name': 'type',\n",
    "              'value': 'text'\n",
    "            },\n",
    "            {\n",
    "              'name': 'name',\n",
    "              'value': 'firstName'\n",
    "            },\n",
    "            {\n",
    "              'name': 'placeholder',\n",
    "              'value': 'Enter your first name'\n",
    "            }\n",
    "          ]\n",
    "        },\n",
    "        {\n",
    "          'type': 'field',\n",
    "          'label': 'Last Name',\n",
    "          'children': [],\n",
    "          'attributes': [\n",
    "            {\n",
    "              'name': 'type',\n",
    "              'value': 'text'\n",
    "            },\n",
    "            {\n",
    "              'name': 'name',\n",
    "              'value': 'lastName'\n",
    "            },\n",
    "            {\n",
    "              'name': 'placeholder',\n",
    "              'value': 'Enter your last name'\n",
    "            }\n",
    "          ]\n",
    "        }\n",
    "      ],\n",
    "      'attributes': []\n",
    "    },\n",
    "    {\n",
    "      'type': 'button',\n",
    "      'label': 'Submit',\n",
    "      'children': [],\n",
    "      'attributes': [\n",
    "        {\n",
    "          'name': 'type',\n",
    "          'value': 'submit'\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  ],\n",
    "  'attributes': [\n",
    "    {\n",
    "      'name': 'method',\n",
    "      'value': 'post'\n",
    "    },\n",
    "    {\n",
    "      'name': 'action',\n",
    "      'value': '/submit-profile'\n",
    "    }\n",
    "  ]\n",
    "}"
   ],
   "id": "f82a1a75faecc502"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "c005f78bfd1c53b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "b604a94b9d63c9ed"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "2441f6e058839dc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "8f2287e25c19a641"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "7c304624e13fd4a4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "9c9247615268918d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "1d3764aac0002ed5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ab8d036849c230d2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Llama Parse\n",
    "___"
   ],
   "id": "6e4a92090cb2199"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Load Dependencies",
   "id": "b148fc7477319d59"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "import nest_asyncio\n",
    "from llama_index.core.node_parser import MarkdownElementNodeParser\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core import Settings\n",
    "from llama_parse import LlamaParse\n",
    "from llama_index.postprocessor.flag_embedding_reranker import (\n",
    "    FlagEmbeddingReranker,\n",
    ")"
   ],
   "id": "19462d5b7879dd41"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "1869f3c5e13d7348"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "!pip install llama-index\n",
    "!pip install llama-index-postprocessor-flag-embedding-reranker\n",
    "!pip install git+https://github.com/FlagOpen/FlagEmbedding.git\n",
    "!pip install llama-parse"
   ],
   "id": "266394ed6d436dad"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Download Data",
   "id": "8de241dea64c787"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "!wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/10q/uber_10q_march_2022.pdf' -O './uber_10q_march_2022.pdf'",
   "id": "b66182fa9a947d24"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Setting API Keys",
   "id": "d3e9471f6635e2d0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# llama-parse is async-first, running the async code in a notebook requires the use of nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# API access to llama-cloud\n",
    "os.environ[\"LLAMA_CLOUD_API_KEY\"] = \"llx-...\"\n",
    "\n",
    "# Using OpenAI API for embeddings/llms\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-...\""
   ],
   "id": "fa6909ee58bca1a4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Setting LLM and Embedding Model",
   "id": "f3ebbd1bf9ec3c53"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo-0125\")\n",
    "\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model"
   ],
   "id": "85a9dc4f75f061bc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### LlamaParse PDF reader for PDF Parsing\n",
    "###### We compare two different retrieval/ queryengine strategies.\n",
    "- Using raw Markdown text as nodes for building index and applying a simple query engine for generating results.\n",
    "- Using MarkdownElementNodeParser for parsing the LlamaParse output Markdown results and building a recursive retriever query engine for generation"
   ],
   "id": "e57ae43768b023b9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# LlamaParse PDF reader for PDF Parsing\n",
    "documents = LlamaParse(result_type=\"markdown\").load_data( \"./uber_10q_march_2022.pdf\" )"
   ],
   "id": "8a81284a9710cbf6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "node_parser = MarkdownElementNodeParser( llm=OpenAI( model=\"gpt-3.5-turbo-0125\"), num_workers=8 )\n",
    "nodes = node_parser.get_nodes_from_documents( documents )"
   ],
   "id": "22766a47a3456a9d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "text_nodes, index_nodes = node_parser.get_nodes_and_objects(nodes)\n",
    "text_nodes[0]\n",
    "index_nodes[0]"
   ],
   "id": "a91e02d5c6ac6f2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Build Index",
   "id": "bfe4f82adeddf074"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "recursive_index = VectorStoreIndex(nodes=text_nodes + index_nodes)\n",
    "raw_index = VectorStoreIndex.from_documents(documents)"
   ],
   "id": "6e00a3423e8971de"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Create Query Engines",
   "id": "5242efa5160d4f04"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "reranker = FlagEmbeddingReranker(\n",
    "    top_n=5,\n",
    "    model=\"BAAI/bge-reranker-large\",\n",
    ")"
   ],
   "id": "b88cf8ae7734f6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Querying Different Query Engines",
   "id": "10442f4b62101f73"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "query = \"What is the change of free cash flow and what is the rate from the financial and operational highlights?\"\n",
    "\n",
    "response_1 = raw_query_engine.query(query)\n",
    "print(\"\\n************New LlamaParse+ Basic Query Engine************\")\n",
    "print(response_1)\n",
    "\n",
    "response_2 = recursive_query_engine.query(query)\n",
    "print(\n",
    "    \"\\n************New LlamaParse+ Recursive Retriever Query Engine************\"\n",
    ")\n",
    "print(response_2)"
   ],
   "id": "97ae58ac5e81a5da"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "606bca5a8d2b5d7c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "945dcfa81f7aa290"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "3b095cb6be63536c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": [],
   "include_colab_link": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
