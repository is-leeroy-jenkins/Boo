{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Groq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic Request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from groq import Groq\n",
    "\n",
    "client = Groq(\n",
    "    api_key=os.environ.get(\"GROQ_API_KEY\"),\n",
    ")\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Explain the importance of fast language models\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"llama-3.3-70b-versatile\",\n",
    ")\n",
    "\n",
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from groq import Groq\n",
    "import os\n",
    "\n",
    "\n",
    "client = Groq(api_key=os.getenv(\"GROQ_API_KEY\"))\n",
    "\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "]\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"Chat with history: \")\n",
    "\n",
    "    messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=messages,\n",
    "        model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "        temperature=0.5,\n",
    "        max_tokens=1024,\n",
    "        top_p=1,\n",
    "        stop=None,\n",
    "        stream=False,\n",
    "    )\n",
    "\n",
    "\n",
    "    assistant_response = chat_completion.choices[0].message.content\n",
    "    messages.append({\"role\": \"assistant\", \"content\": assistant_response})\n",
    "\n",
    "    print(assistant_response + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from groq import Groq\n",
    "\n",
    "client = Groq()\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        # Set an optional system message. This sets the behavior of the\n",
    "        # assistant and can be used to provide specific instructions for\n",
    "        # how it should behave throughout the conversation.\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful assistant.\"\n",
    "        },\n",
    "        # Set a user message for the assistant to respond to.\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Explain the importance of fast language models\",\n",
    "        }\n",
    "    ],\n",
    "\n",
    "    # The language model which will generate the completion.\n",
    "    model=\"llama-3.3-70b-versatile\"\n",
    ")\n",
    "\n",
    "# Print the completion returned by the LLM.\n",
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### System Instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from groq import Groq\n",
    "import os\n",
    "\n",
    "client = Groq(api_key=os.environ.get(\"GROQ_API_KEY\"))\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"What's in this image?\"\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": \"https://upload.wikimedia.org/wikipedia/commons/f/f2/LPU-v1-die.jpg\"\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ],\n",
    "    temperature=1,\n",
    "    max_completion_tokens=1024,\n",
    "    top_p=1,\n",
    "    stream=False,\n",
    "    stop=None,\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt='''\n",
    "You are an image classification API specializing in dog photos that responds in JSON. What dog breed is shown in this image? \n",
    "Return in the following JSON format:\n",
    "\n",
    "{\n",
    "  \"dog_breed\": \"string (e.g., 'Golden Retriever', 'German Shepherd', 'Mixed')\"\n",
    "}\n",
    "'''\n",
    "\n",
    "def image_classification(base64_image, user_prompt):\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\n",
    "                            \"url\": f\"data:image/jpeg;base64,{base64_image}\",\n",
    "                        },\n",
    "                    },\n",
    "                    {\"type\": \"text\", \"text\": user_prompt},\n",
    "                ],\n",
    "            }\n",
    "        ],\n",
    "        model=model,\n",
    "        response_format = {\"type\": \"json_object\"}\n",
    "    )\n",
    "\n",
    "    return json.loads(chat_completion.choices[0].message.content)\n",
    "\n",
    "image_classification(base64_image, user_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Image Editing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Image Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from groq import Groq\n",
    "import base64\n",
    "import os\n",
    "\n",
    "# Function to encode the image\n",
    "def encode_image(image_path):\n",
    "  with open(image_path, \"rb\") as image_file:\n",
    "    return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "# Path to your image\n",
    "image_path = \"sf.jpg\"\n",
    "\n",
    "# Getting the base64 string\n",
    "base64_image = encode_image(image_path)\n",
    "\n",
    "client = Groq(api_key=os.environ.get(\"GROQ_API_KEY\"))\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": \"What's in this image?\"},\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": f\"data:image/jpeg;base64,{base64_image}\",\n",
    "                    },\n",
    "                },\n",
    "            ],\n",
    "        }\n",
    "    ],\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    ")\n",
    "\n",
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upload JSONL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests # pip install requests first!\n",
    "\n",
    "def upload_file_to_groq(api_key, file_path):\n",
    "    url = \"https://api.groq.com/openai/v1/files\"\n",
    "\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {api_key}\"\n",
    "    }\n",
    "\n",
    "    # Prepare the file and form data\n",
    "    files = {\n",
    "        \"file\": (\"batch_file.jsonl\", open(file_path, \"rb\"))\n",
    "    }\n",
    "\n",
    "    data = {\n",
    "        \"purpose\": \"batch\"\n",
    "    }\n",
    "\n",
    "    # Make the POST request\n",
    "    response = requests.post(url, headers=headers, files=files, data=data)\n",
    "\n",
    "    return response.json()\n",
    "\n",
    "# Usage example\n",
    "api_key = os.environ.get(\"GROQ_API_KEY\")\n",
    "file_path = \"batch_file.jsonl\"  # Path to your JSONL file\n",
    "\n",
    "try:\n",
    "    result = upload_file_to_groq(api_key, file_path)\n",
    "    print(result)\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from groq import Groq\n",
    "\n",
    "client = Groq(\n",
    "    api_key=os.environ.get(\"GROQ_API_KEY\"),  # This is the default and can be omitted\n",
    ")\n",
    "file = client.files.info(\n",
    "    \"file_01jh6x76wtemjr74t1fh0faj5t\",\n",
    ")\n",
    "print(file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from groq import Groq\n",
    "\n",
    "client = Groq(\n",
    "    api_key=os.environ.get(\"GROQ_API_KEY\"),  # This is the default and can be omitted\n",
    ")\n",
    "file_list = client.files.list()\n",
    "print(file_list.data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Delete File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from groq import Groq\n",
    "\n",
    "client = Groq(\n",
    "    api_key=os.environ.get(\"GROQ_API_KEY\"),  # This is the default and can be omitted\n",
    ")\n",
    "file_delete = client.files.delete(\n",
    "    \"file_01jh6x76wtemjr74t1fh0faj5t\",\n",
    ")\n",
    "print(file_delete)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Browser Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "client = openai.OpenAI(\n",
    "    api_key=\"your-groq-api-key\",\n",
    "    base_url=\"https://api.groq.com/openai/v1\"\n",
    ")\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"openai/gpt-oss-20b\",\n",
    "    input=\"Analyze the current weather in San Francisco and provide a detailed forecast.\",\n",
    "    tool_choice=\"required\",\n",
    "    tools=[\n",
    "        {\n",
    "            \"type\": \"browser_search\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visit Website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from groq import Groq\n",
    "\n",
    "client = Groq(\n",
    "    default_headers={\n",
    "        \"Groq-Model-Version\": \"latest\"\n",
    "    }\n",
    ")\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Summarize the key points of this page: https://groq.com/blog/inside-the-lpu-deconstructing-groq-speed\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"groq/compound\",\n",
    ")\n",
    "\n",
    "message = chat_completion.choices[0].message\n",
    "\n",
    "# Print the final content\n",
    "print(message.content)\n",
    "\n",
    "# Print the reasoning process\n",
    "print(message.reasoning)\n",
    "\n",
    "# Print executed tools\n",
    "if message.executed_tools:\n",
    "    print(message.executed_tools[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Web Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from groq import Groq\n",
    "import json\n",
    "\n",
    "client = Groq()\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"groq/compound\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What happened in AI last week? Provide a list of the most important model releases and updates.\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Final output\n",
    "print(response.choices[0].message.content)\n",
    "\n",
    "# Reasoning + internal tool calls\n",
    "print(response.choices[0].message.reasoning)\n",
    "\n",
    "# Search results from the tool calls\n",
    "if response.choices[0].message.executed_tools:\n",
    "    print(response.choices[0].message.executed_tools[0].search_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wolfram Alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from groq import Groq\n",
    "\n",
    "client = Groq(\n",
    "    default_headers={\n",
    "        \"Groq-Model-Version\": \"latest\"\n",
    "    }\n",
    ")\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What is 1293392*29393?\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"groq/compound\",\n",
    "    compound_custom={\n",
    "        \"tools\": {\n",
    "            \"enabled_tools\": [\"wolfram_alpha\"],\n",
    "            \"wolfram_settings\": {\"authorization\": \"your_wolfram_alpha_api_key_here\"}\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "message = chat_completion.choices[0].message\n",
    "\n",
    "# Print the final content\n",
    "print(message.content)\n",
    "\n",
    "# Print the reasoning process\n",
    "print(message.reasoning)\n",
    "\n",
    "# Print executed tools\n",
    "if message.executed_tools:\n",
    "    print(message.executed_tools[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Browser Automation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from groq import Groq\n",
    "\n",
    "client = Groq(\n",
    "    default_headers={\n",
    "        \"Groq-Model-Version\": \"latest\"\n",
    "    }\n",
    ")\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What are the latest models on Groq and what are they good at?\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"groq/compound-mini\",\n",
    "    compound_custom={\n",
    "        \"tools\": {\n",
    "            \"enabled_tools\": [\"browser_automation\", \"web_search\"]\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "message = chat_completion.choices[0].message\n",
    "\n",
    "# Print the final content\n",
    "print(message.content)\n",
    "\n",
    "# Print the reasoning process\n",
    "print(message.reasoning)\n",
    "\n",
    "# Print executed tools\n",
    "if message.executed_tools:\n",
    "    print(message.executed_tools[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from groq import Groq\n",
    "\n",
    "client = Groq(api_key=os.environ.get(\"GROQ_API_KEY\"))\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Calculate the square root of 101 and show me the Python code you used\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"groq/compound-mini\",\n",
    ")\n",
    "\n",
    "# Final output\n",
    "print(response.choices[0].message.content)\n",
    "\n",
    "# Reasoning + internal tool calls\n",
    "print(response.choices[0].message.reasoning)\n",
    "\n",
    "# Code execution tool call\n",
    "if response.choices[0].message.executed_tools:\n",
    "    print(response.choices[0].message.executed_tools[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audio API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default\n",
    "import os\n",
    "from groq import Groq\n",
    "\n",
    "client = Groq()\n",
    "filename = os.path.dirname(__file__) + \"/sample_audio.m4a\"\n",
    "\n",
    "with open(filename, \"rb\") as file:\n",
    "    translation = client.audio.translations.create(\n",
    "      file=(filename, file.read()),\n",
    "      model=\"whisper-large-v3\",\n",
    "      prompt=\"Specify context or spelling\",  # Optional\n",
    "      response_format=\"json\",  # Optional\n",
    "      temperature=0.0  # Optional\n",
    "    )\n",
    "    print(translation.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from groq import Groq\n",
    "\n",
    "client = Groq()\n",
    "filename = os.path.dirname(__file__) + \"/sample_audio.m4a\"\n",
    "\n",
    "with open(filename, \"rb\") as file:\n",
    "    transcription = client.audio.transcriptions.create(\n",
    "      file=(filename, file.read()),\n",
    "      model=\"whisper-large-v3\",\n",
    "      prompt=\"Specify context or spelling\",  # Optional\n",
    "      response_format=\"json\",  # Optional\n",
    "      language=\"en\",  # Optional\n",
    "      temperature=0.0  # Optional\n",
    "    )\n",
    "    print(transcription.text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from groq import Groq\n",
    "\n",
    "client = Groq(api_key=os.environ.get(\"GROQ_API_KEY\"))\n",
    "\n",
    "speech_file_path = \"speech.wav\"\n",
    "model = \"playai-tts\"\n",
    "voice = \"Fritz-PlayAI\"\n",
    "text = \"I love building and shipping new features for our users!\"\n",
    "response_format = \"wav\"\n",
    "\n",
    "response = client.audio.speech.create(\n",
    "    model=model,\n",
    "    voice=voice,\n",
    "    input=text,\n",
    "    response_format=response_format\n",
    ")\n",
    "\n",
    "response.write_to_file(speech_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from groq import Groq\n",
    "\n",
    "client = Groq(api_key=os.environ.get(\"GROQ_API_KEY\"))\n",
    "\n",
    "speech_file_path = \"orpheus-english.wav\" \n",
    "model = \"canopylabs/orpheus-v1-english\"\n",
    "voice = \"troy\"\n",
    "text = \"Welcome to Orpheus text-to-speech. [cheerful] This is an example of high-quality English audio generation with vocal directions support.\"\n",
    "response_format = \"wav\"\n",
    "\n",
    "response = client.audio.speech.create(\n",
    "    model=model,\n",
    "    voice=voice,\n",
    "    input=text,\n",
    "    response_format=response_format\n",
    ")\n",
    "\n",
    "response.write_to_file(speech_file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
