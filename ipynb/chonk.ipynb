{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# LangChain",
   "id": "5f900d513c2bab0a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Example Chain",
   "id": "5350f2727e98e7aa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import BaseOutputParser\n",
    "\n",
    "# Example chain\n",
    "chain = ChatPromptTemplate() | ChatOpenAI() | CustomOutputParser()\n"
   ],
   "id": "8a1cdf86b31a81c0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# LLM",
   "id": "93c727d481819c6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "api_key = os.getenv( 'OPENAI_API_KEY' )\n",
    "llm = OpenAI()\n"
   ],
   "id": "562290776710c17"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- LLMs accept strings as inputs, or objects which can be coerced to string prompts",
   "id": "68904a821591dcd4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "response = llm.invoke( 'List the seven wonders of the world.' )\n",
    "print( response )"
   ],
   "id": "106bb7850c089758"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- Alternatively call the stream method to stream the text response",
   "id": "a11713a759a5c9cd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "for chunk in llm.stream( 'Where were the 2012 Olympics held?' ):\n",
    "    print( chunk, end=\"\", flush=True )\n"
   ],
   "id": "138a4e5d582a70ee"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Chat Models\n",
    "- Utilize language models internally\n",
    "- A distinct interface centered around chat messages as inputs and outputs\n",
    "- Essential for creating interactive chat applications"
   ],
   "id": "f8e8e2a3e01161f1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "chat = ChatOpenAI( )\n"
   ],
   "id": "f820aa7f6eef0137"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- Chat models in LangChain work with different message types",
   "id": "e6e88982e6f4350c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain.schema.messages import HumanMessage, SystemMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage( content='You are Micheal Jordan.' ),\n",
    "    HumanMessage( content='Which shoe manufacturer are you associated with?' ),\n",
    "]\n",
    "\n",
    "response = chat.invoke( messages )\n",
    "print( response.content )"
   ],
   "id": "b4ca3b57cc1b40bc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## PromptTemplate\n",
    "- Used to format a single string\n",
    "- Generally are used for simpler inputs"
   ],
   "id": "1ff488b9fc38e2f6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Simple prompt with placeholders\n",
    "prompt_template = PromptTemplate.from_template( 'Tell me a {adjective} joke about {content}.' )\n",
    "\n",
    "# Filling placeholders to generate_text a prompt\n",
    "filled_prompt = prompt_template.format( adjective='funny', content='robots' )\n",
    "print( filled_prompt )\n"
   ],
   "id": "bf348ae9ed7e4697"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## ChatPromptTemplates\n",
    "- Used to format a list of messages"
   ],
   "id": "65077406cf72deaf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt_template = ChatPromptTemplate([\n",
    "    ( 'system', 'You are a helpful assistant' ),\n",
    "    ( 'user', 'Tell me a joke about {topic}' )\n",
    "])\n",
    "\n",
    "prompt_template.invoke( {'topic': 'cats'} )\n"
   ],
   "id": "19874a4d08196af6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## MessagesPlaceholder",
   "id": "a0ab814bd965bf3a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "prompt_template = ChatPromptTemplate([\n",
    "    ('system', 'You are a helpful assistant'),\n",
    "    MessagesPlaceholder('msgs')\n",
    "])\n",
    "\n",
    "prompt_template.invoke({'msgs': [HumanMessage(content='hi!')]})\n"
   ],
   "id": "797034c2ae0b6ac9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- w/out MessagesPlaceholder",
   "id": "5babeee4369a3710"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "prompt_template = ChatPromptTemplate([\n",
    "    ('system', 'You are a helpful assistant'),\n",
    "    ('placeholder', '{msgs}') # <-- This is the changed part\n",
    "])\n"
   ],
   "id": "e0f0d2d57aba2ffb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Tool creation",
   "id": "930013227c28d34"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"\n",
    "\n",
    "        Multiply a and b.\n",
    "\n",
    "        Args:\n",
    "            a: first int\n",
    "            b: second int\n",
    "\n",
    "    \"\"\"\n",
    "    return a * b\n",
    "\n",
    "llm_with_tools = tool_calling_model.bind_tools( [ multiply ] )\n",
    "\n",
    "result = llm_with_tools.invoke( 'What is 2 multiplied by 3?' )\n"
   ],
   "id": "2c072fa641d15901"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Prompts\n",
    "- Simple instructions to complex few-shot examples"
   ],
   "id": "dc94309645e99869"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Load Dependencies",
   "id": "398653c9ce0ac30e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import langchain\n",
    "import sqlite3"
   ],
   "id": "83239161df34a542"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "1. Start with an object of the OpenAI class and set some parameters.\n",
    "2. Here, we set up the parameter called temperature.\n",
    "3. Having a lower temperature value ensures that our LLM output is deterministic in nature and not too random and \"creative\".\n",
    "4. Call the instance (object) of the OpenAI class \"llm\""
   ],
   "id": "3a2c168aa811e497"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## API",
   "id": "15a4b90a5a9014c3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 1. LLMs\n",
    "- Abstraction around different model providers."
   ],
   "id": "f1e97c6b79e17011"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "73fa591cb55fcfcd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Create OpenAI object",
   "id": "2e02bb278babb1ad"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "api_key = os.getenv( 'OPENAI_API_KEY' )\n",
    "\n",
    "llm = OpenAI( model_name='openai-4o', temperature=0.8 )\n",
    "response = llm( 'What is the capital of France?' )\n",
    "print( response )"
   ],
   "id": "4588a0c8ef9febd5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### 1. Load the documents using TextLoader from LangChain.\n",
    "##### 2. Define a variable to store the documents"
   ],
   "id": "5465550ea36b604"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "data = []",
   "id": "b2dcd731890db1d3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### 1. Load the documents using TextLoader from LangChain.\n",
    "##### 2. Define a variable to store the documents"
   ],
   "id": "384294a1b884ae22"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- We can append the output of loader.load() into our list variable",
   "id": "422e9c070042d650"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "loader = TextLoader(r'C:\\Users\\JkReddy\\Desktop\\Weill Cornell Medicine\\Subjects\\Capstone\\LangChain.txt')\n",
    "data.append( loader.load( )[ 0 ] )"
   ],
   "id": "83cef962961a3afa"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- Importing Vector DB - Chroma along with TextSplitter and QA related packages from LangChain.\n",
    "- Import package for Embeddings from OpenAI"
   ],
   "id": "aac4f935c1378851"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Split the lengthy document into smaller chunks.\n",
    "- This is done because LLMs have a limit to the number of words they can take in as input, and they can retain more information if they have fewer words to work with.\n",
    "- Chunk overlap parameter dictates how much overlap should exist between the chunks.\n",
    "- Having more of an overlap ensures that important information is not lost during the splitting process.\n",
    "- Split documents function takes in a list as an input.\n",
    "- Each list element must contain a document loaded in by Langchain"
   ],
   "id": "39eb49faab577a83"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "text_splitter = CharacterTextSplitter( chunk_size = 1000, chunk_overlap=200 )\n",
    "texts = text_splitter.split_documents( data )"
   ],
   "id": "49c67010927c1086"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "1. Creating database and getting the embedding functions ready.\n",
    "2. Store the docs in the vector db.\n",
    "\n",
    "- In the below cell, we mention a directory in which we want our vector db to reside.\n",
    "- This is then followed by the creation of an embeddings object from the OpenAIEmbeddings class from the Langchain package.\n",
    "- This can used for creating the doc embeddings"
   ],
   "id": "d44ca1244c966e94"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Embed and store the tokens\n",
    "# Supplying a persist_directory will store the vectors on disk\n",
    "persist_directory = 'myvectordb'\n",
    "embeddings = OpenAIEmbeddings( )"
   ],
   "id": "c2b56a50bd3e715f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### By default, the model used for embeddings is the text-ada-embeddings-002\n",
    "\n",
    "- The vector db is Chroma DB which is integrated into LangChain\n",
    "- The Chroma.from_documents function takes in these parameters:\n",
    "\n",
    "##### The split up texts\n",
    "- Embedding instance from Langchain\n",
    "- Directory in which we want the persistence of our db to be asserted"
   ],
   "id": "97c02c6697df99c5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "vectordb = Chroma.from_documents( texts, embeddings, persist_directory = persist_directory )",
   "id": "bb98b4c95907bee5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- Write Embeddings to a disk using db.persist() and wiping it clean. Reload again to test if it has been stored",
   "id": "86cc80dbd16f1a22"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "vectordb.persist()\n",
    "vectordb = None"
   ],
   "id": "36aadb6dc23e2a21"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- Reload",
   "id": "fd353b253ecba899"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "vectordb = Chroma( persist_directory=persist_directory, embedding_function = embeddings )",
   "id": "52bd6fa6fcad8ce5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- Create object with LLM model being passed as a parameter along with temperature parameter for controlling the nature of the LLM output.\n",
    "- OpenAI class also takes in model_name as a parameter."
   ],
   "id": "6d0bcebe1000061"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "gpt_qa = RetrievalQA.from_chain_type( llm=OpenAI(temperature = 0.8, model_name = 'documents-ada-vectors-002' ),\n",
    "                                 chain_type = 'stuff',\n",
    "                                 retriever = vectordb.as_retriever())"
   ],
   "id": "809ec8bb28155bb2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- Our newly created qa object has the function query using which we can run a query over the db.\n",
    "- Once the right doc chunk has been retrieved, it is passed to the llm along with the query."
   ],
   "id": "ee36a217bcfc7b9c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "query = \"What can I eat?\"\n",
    "gpt_qa.run(query)"
   ],
   "id": "ec2a3438c94621c0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- Creating an instance of an Open Source Embedding",
   "id": "303e9e1a0b89c497"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain import HuggingFacePipeline\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "model_name = 'sentence-transformers/all-mpnet-base-v2'\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "embeddings = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)\n",
    "text_splitter = CharacterTextSplitter(chunk_size = 500, chunk_overlap=50)\n",
    "texts = text_splitter.split_documents(data)\n",
    "persist_directory = 'myvectordb_opensource'\n",
    "vectordb = Chroma.from_documents(texts, embeddings, persist_directory = persist_directory)\n",
    "vectordb.persist()"
   ],
   "id": "8ec621a20b23b20b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- Langchain HF pipeline only supports models in the hub which function as text2text gen or text gen models.",
   "id": "1deb1d7d6d35880d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "llm = HuggingFacePipeline.from_model_id(model_id='declare-lab/flan-gpt4all-xl',\n",
    "                                        task='text2text-generation',\n",
    "                                        model_kwargs={'temperature':0, 'max_length':50, 'min_length':10})"
   ],
   "id": "1ca373aa4ce5aa0d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "qa = RetrievalQA.from_chain_type(llm,\n",
    "                                 chain_type = 'refine',\n",
    "                                 retriever = vectordb.as_retriever())"
   ],
   "id": "537f3aede815c7f9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "query = 'Can I have fruits?'\n",
    "qa.run(query)"
   ],
   "id": "84f8b9e58ae4e132"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1. LLMs\n",
    "- Abstraction around different model providers."
   ],
   "id": "cf97eb256dde9e28"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "llm = OpenAI( model_name='openai-4o', temperature=0.8 )\n",
    "response = llm( 'What is the capital of France?' )\n",
    "print( response )"
   ],
   "id": "4ab123c5d23e4a88"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2. Prompt Templates\n",
    "- Reusable templates for structured prompting."
   ],
   "id": "8c46bf07e8381496"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "template = 'What is a good name for a company that makes {product}?'\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "filled_prompt = prompt.format(product='smart shoes')\n",
    "print(filled_prompt)"
   ],
   "id": "698f744b041ecf26"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3. Chains\n",
    "- Sequence of calls (LLMs, tools, functions). Most basic: LLMChain."
   ],
   "id": "bfddef81361c913f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "chain = LLMChain( llm=llm, prompt=prompt )\n",
    "print( chain.run( product='AI-powered drones' ) )"
   ],
   "id": "6ed0b9222bedaadb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 4. Agents\n",
    "- Dynamically select tools based on user input using a \"reasoning\" loop."
   ],
   "id": "68f6a38b6a13d559"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "tools = load_tools( [ 'serpapi', 'llm-math' ], llm=llm )\n",
    "agent = initialize_agent(\n",
    "    tools,\n",
    "    llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "agent.run( 'What is the square root of the population of Canada?' )"
   ],
   "id": "fd05e4a8a6cbbb21"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 5. Vector Stores + RAG\n",
    "- Build question-answering systems over your own documents."
   ],
   "id": "f0a71e2de3bd5393"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Load and split\n",
    "loader = TextLoader( 'my_docs.txt' )\n",
    "docs = loader.load( )\n",
    "splitter = RecursiveCharacterTextSplitter( chunk_size=500, chunk_overlap=50 )\n",
    "chunks = splitter.split_documents( docs )\n",
    "\n",
    "# Embed and store\n",
    "embedding = OpenAIEmbeddings()\n",
    "db = FAISS.from_documents( chunks, embedding )\n",
    "\n",
    "# Ask a question\n",
    "\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(llm=llm, retriever=db.as_retriever())\n",
    "qa.run('Summarize the key points from the document')\n"
   ],
   "id": "169db0387346a007"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 6. Memory\n",
    "- Preserves context between turns of conversation."
   ],
   "id": "401826898d626e7b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "memory = ConversationBufferMemory( )\n",
    "conv_chain = ConversationChain( llm=llm, memory=memory )\n",
    "\n",
    "print( conv_chain.run( 'Hi, I am working on an AI project.' ) )\n",
    "print( conv_chain.run( 'What did I say my project was about?' ) )"
   ],
   "id": "cb1086fd7d988634"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### SQLite Integration Script",
   "id": "943b731fe9389d37"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# STEP 1: Set your OpenAI API key\n",
    "api_key = os.getenv( 'OPENAI_API_KEY' )\n",
    "os.environ[ \"OPENAI_API_KEY\" ] = api_key\n",
    "\n",
    "# STEP 2: Create and populate a SQLite database\n",
    "conn = sqlite3.connect( \"people.values\" )\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(\"DROP TABLE IF EXISTS employees\")\n",
    "cursor.execute(\"\"\"\n",
    "    CREATE TABLE employees (\n",
    "        id INTEGER PRIMARY KEY,\n",
    "        name TEXT,\n",
    "        title TEXT,\n",
    "        department TEXT,\n",
    "        salary INTEGER\n",
    "    )\n",
    "\"\"\")\n",
    "cursor.executemany( 'INSERT INTO employees ( name, title, department, salary ) VALUES ( ?, ?, ?, ? )', [\n",
    "    ( 'Alice Johnson', 'Engineer', 'R&D', 90000 ),\n",
    "    ( 'Bob Smith', 'Manager', 'HR', 85000 ),\n",
    "    ( 'Charlie Kim', 'Analyst', 'Finance', 75000 ),\n",
    "    ( 'Diana Lopez', 'Engineer', 'R&D', 95000 )\n",
    "] )\n",
    "conn.commit( )\n",
    "conn.close( )\n",
    "\n",
    "# STEP 3: Connect LangChain to the database\n",
    "db = SQLDatabase.from_uri( 'sqlite:///people.values' )\n",
    "\n",
    "# STEP 4: Create the agent with SQL toolkit\n",
    "llm = ChatOpenAI( temperature=0, model='openai-4o' )\n",
    "toolkit = SQLDatabaseToolkit( db, llm )\n",
    "agent_executor = create_sql_agent( llm, toolkit, AgentType.ZERO_SHOT_REACT_DESCRIPTION, True )\n",
    "\n",
    "# STEP 5: Ask natural language questions\n",
    "response = agent_executor.run( 'Which employees in R&D earn more than 90000?' )\n",
    "print(response)\n"
   ],
   "id": "8853e8f01c7b8d6a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Retrieval Script",
   "id": "1e129d2bb5de3886"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import sqlite3\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.agents import initialize_agent, Tool\n",
    "from langchain.agents.agent_types import AgentType\n",
    "from langchain.agents.agent_toolkits import SQLDatabaseToolkit\n",
    "from langchain.sql_database import SQLDatabase\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Set your OpenAI key\n",
    "os.environ['OPENAI_API_KEY'] = 'your-openai-key'\n",
    "\n",
    "# STEP 1: Create SQLite database\n",
    "conn = sqlite3.connect('people.values')\n",
    "cursor = conn.cursor()\n",
    "cursor.execute('DROP TABLE IF EXISTS employees')\n",
    "cursor.execute(\"\"\"\n",
    "    CREATE TABLE employees (\n",
    "        id INTEGER PRIMARY KEY,\n",
    "        name TEXT,\n",
    "        title TEXT,\n",
    "        department TEXT,\n",
    "        salary INTEGER\n",
    "    )\n",
    "\"\"\")\n",
    "cursor.executemany('INSERT INTO employees (name, title, department, salary) VALUES (?, ?, ?, ?)', [\n",
    "    ('Alice Johnson', 'Engineer', 'R&D', 90000),\n",
    "    ('Bob Smith', 'Manager', 'HR', 85000),\n",
    "    ('Charlie Kim', 'Analyst', 'Finance', 75000),\n",
    "    ('Diana Lopez', 'Engineer', 'R&D', 95000)\n",
    "])\n",
    "conn.commit()\n",
    "conn.close()\n",
    "\n",
    "# STEP 2: Create SQL database object\n",
    "sql_db = SQLDatabase.from_uri('sqlite:///people.values')\n",
    "llm = ChatOpenAI(temperature=0, model='openai-4')\n",
    "\n",
    "# STEP 3: Load documents and generate_text vector store\n",
    "loader = TextLoader('my_notes.txt')  # Replace with your own file\n",
    "documents = loader.load()\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "docs = splitter.split_documents(documents)\n",
    "embedding = OpenAIEmbeddings()\n",
    "vectordb = FAISS.from_documents(docs, embedding)\n",
    "retriever = vectordb.as_retriever()\n",
    "\n",
    "# STEP 4: Create RetrievalQA chain\n",
    "doc_qa = RetrievalQA.from_chain_type(llm=llm, retriever=retriever)\n",
    "\n",
    "# STEP 5: Register tools for agent\n",
    "tools = [\n",
    "    Tool(\n",
    "        name='SQL_Database',\n",
    "        func=SQLDatabaseToolkit(db=sql_db, llm=llm).get_tools()[0].func,\n",
    "        description='Useful for answering questions about employees, departments, and salaries.'\n",
    "    ),\n",
    "    Tool(\n",
    "        name='Document_QA',\n",
    "        func=doc_qa.run,\n",
    "        description='Useful for answering questions about policy notes and general knowledge from documents.'\n",
    "    )\n",
    "]\n",
    "\n",
    "# STEP 6: Initialize hybrid agent\n",
    "agent = initialize_agent(\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# STEP 7: Ask unified questions\n",
    "print(agent.run('Who in R&D earns over $90k?'))  # SQL tool\n",
    "print(agent.run('Summarize the key points from the notes'))  # Document tool"
   ],
   "id": "51bf8c1e4f353864"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
