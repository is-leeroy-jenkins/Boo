{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T21:33:25.270688Z",
     "start_time": "2025-04-25T21:33:25.051747Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import langchain\n",
    "import langchain_community\n",
    "import langchain_pinecone\n",
    "import langchain_openai\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "import pinecone\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "from langchain.schema import (\n",
    "    SystemMessage,\n",
    "    HumanMessage,\n",
    "    AIMessage\n",
    ")\n",
    "\n",
    "from pinecone import Pinecone, ServerlessSpec, CloudProvider, AwsRegion, Metric\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "from getpass import getpass\n"
   ],
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "The official Pinecone python package has been renamed from `pinecone-client` to `pinecone`. Please remove `pinecone-client` from your project dependencies and add `pinecone` instead. See the README at https://github.com/pinecone-io/pinecone-python-client for more information on using the python SDK.",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mException\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[3]\u001B[39m\u001B[32m, line 3\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mlangchain\u001B[39;00m\n\u001B[32m      2\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mlangchain_community\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m3\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mlangchain_pinecone\u001B[39;00m\n\u001B[32m      4\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mlangchain_openai\u001B[39;00m\n\u001B[32m      5\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mlangchain_openai\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m OpenAIEmbeddings\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\source\\repos\\Boo\\venv\\Lib\\site-packages\\langchain_pinecone\\__init__.py:1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mlangchain_pinecone\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01membeddings\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m PineconeEmbeddings\n\u001B[32m      2\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mlangchain_pinecone\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mvectorstores\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m Pinecone, PineconeVectorStore\n\u001B[32m      3\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mlangchain_pinecone\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mvectorstores_sparse\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m PineconeSparseVectorStore\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\source\\repos\\Boo\\venv\\Lib\\site-packages\\langchain_pinecone\\embeddings.py:6\u001B[39m\n\u001B[32m      4\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mlangchain_core\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01membeddings\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m Embeddings\n\u001B[32m      5\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mlangchain_core\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mutils\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m secret_from_env\n\u001B[32m----> \u001B[39m\u001B[32m6\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mpinecone\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m Pinecone \u001B[38;5;28;01mas\u001B[39;00m PineconeClient  \u001B[38;5;66;03m# type: ignore[import-untyped]\u001B[39;00m\n\u001B[32m      7\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mpinecone\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[32m      8\u001B[39m     PineconeAsyncio \u001B[38;5;28;01mas\u001B[39;00m PineconeAsyncioClient,  \u001B[38;5;66;03m# type: ignore[import-untyped]\u001B[39;00m\n\u001B[32m      9\u001B[39m )\n\u001B[32m     10\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mpinecone\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m SparseValues\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\source\\repos\\Boo\\venv\\Lib\\site-packages\\pinecone\\__init__.py:5\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m      2\u001B[39m \u001B[33;03m.. include:: ../README.md\u001B[39;00m\n\u001B[32m      3\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m5\u001B[39m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m(\n\u001B[32m      6\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mThe official Pinecone python package has been renamed from `pinecone-client` to `pinecone`. Please remove `pinecone-client` from your project dependencies and add `pinecone` instead. See the README at https://github.com/pinecone-io/pinecone-python-client for more information on using the python SDK.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m      7\u001B[39m )\n",
      "\u001B[31mException\u001B[39m: The official Pinecone python package has been renamed from `pinecone-client` to `pinecone`. Please remove `pinecone-client` from your project dependencies and add `pinecone` instead. See the README at https://github.com/pinecone-io/pinecone-python-client for more information on using the python SDK."
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ljRgQzDLZrv6"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pinecone-io/examples/blob/master/learn/generation/langchain/rag-chatbot.ipynb) [![Open nbviewer](https://raw.githubusercontent.com/pinecone-io/examples/master/assets/nbviewer-shield.svg)](https://nbviewer.org/github/pinecone-io/examples/blob/master/learn/generation/langchain/rag-chatbot.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "85Xhv9IcZrv7"
   },
   "source": [
    "# Building RAG Chatbots with LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yHSntiUHZrv7"
   },
   "source": [
    "In this example, we'll work on building an AI chatbot from start-to-finish. We will be using LangChain, OpenAI, and Pinecone vector DB, to build a chatbot capable of learning from the external world using **R**etrieval **A**ugmented **G**eneration (RAG).\n",
    "\n",
    "We will be using a dataset sourced from the Deepseek R1 ArXiv paper to help our chatbot answer questions about the latest and greatest in the world of AI.\n",
    "\n",
    "By the end of the example we'll have a functioning chatbot and RAG pipeline that can hold a conversation and provide informative responses based on a knowledge base.\n",
    "\n",
    "### Before you begin\n",
    "\n",
    "You'll need to get an [OpenAI API key](https://platform.openai.com/account/api-keys) and [Pinecone API key](https://app.pinecone.io)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UIWCb3LVZrv7"
   },
   "source": [
    "### Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NUfghzFtZrv8"
   },
   "source": [
    "Before we start building our chatbot, we need to install some Python libraries. Here's a brief overview of what each library does:\n",
    "\n",
    "- **langchain**: This is a library for GenAI. We'll use it to chain together different language models and components for our chatbot.\n",
    "- **openai**: This is the official OpenAI Python client. We'll use it to interact with the OpenAI API and generate responses for our chatbot.\n",
    "- **datasets**: This library provides a vast array of datasets for machine learning. We'll use it to load our knowledge base for the chatbot.\n",
    "- **pinecone-client**: This is the official Pinecone Python client. We'll use it to interact with the Pinecone API and store our chatbot's knowledge base in a vector database.\n",
    "\n",
    "You can install these libraries using pip like so:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fqiSJjhmZrv9"
   },
   "source": [
    "### Building a Chatbot (no RAG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e_qCi2M0Zrv9"
   },
   "source": [
    "We will be relying heavily on the LangChain library to bring together the different components needed for our chatbot. To begin, we'll create a simple chatbot without any retrieval augmentation. We do this by initializing a `ChatOpenAI` object. For this we do need an [OpenAI API key](https://platform.openai.com/account/api-keys)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "hJXUocdtZrv9"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\") or getpass(\n",
    "    \"Enter your OpenAI API key: \"\n",
    ")\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    openai_api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "    model='gpt-4o-mini'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DYaKQneDZrv9"
   },
   "source": [
    "Chats with OpenAI's `gpt-3.5-turbo` and `gpt-4` chat models are typically structured (in plain text) like this:\n",
    "\n",
    "```\n",
    "System: You are a helpful assistant.\n",
    "\n",
    "User: Hi AI, how are you today?\n",
    "\n",
    "Assistant: I'm great thank you. How can I help you?\n",
    "\n",
    "User: I'd like to understand string theory.\n",
    "\n",
    "Assistant:\n",
    "```\n",
    "\n",
    "The final `\"Assistant:\"` without a response is what would prompt the model to continue the conversation. In the official OpenAI `ChatCompletion` endpoint these would be passed to the model in a format like:\n",
    "\n",
    "```python\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hi AI, how are you today?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"I'm great thank you. How can I help you?\"}\n",
    "    {\"role\": \"user\", \"content\": \"I'd like to understand path theory.\"}\n",
    "]\n",
    "```\n",
    "\n",
    "In LangChain there is a slightly different format. We use three _message_ objects like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "cbfvMP4MZrv-"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "    HumanMessage(content=\"Hi AI, how are you today?\"),\n",
    "    AIMessage(content=\"I'm great thank you. How can I help you?\"),\n",
    "    HumanMessage(content=\"I'd like to understand path theory.\")\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VQNiV_YAZrv-"
   },
   "source": [
    "The format is very similar, we're just swapped the role of `\"user\"` for `HumanMessage`, and the role of `\"assistant\"` for `AIMessage`.\n",
    "\n",
    "We generate the next response from the AI by passing these messages to the `ChatOpenAI` object."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jfsMnGrsZrv-",
    "outputId": "67cfebd5-3abb-42fb-bc02-0549964265e9"
   },
   "source": [
    "res = chat(messages)\n",
    "res"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9xhDvRR_Zrv-"
   },
   "source": [
    "In response we get another AI message object. We can print it more clearly like so:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O056MNkIZrv-",
    "outputId": "c7795037-1b51-4453-9bf9-9d428f460a50"
   },
   "source": [
    "print(res.content)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wTTa9OtsZrv-"
   },
   "source": [
    "Because `res` is just another `AIMessage` object, we can append it to `messages`, add another `HumanMessage`, and generate the next response in the conversation."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wrMsBypWZrv-",
    "outputId": "9da99810-1dbe-4677-cc22-dd340311a22a"
   },
   "source": [
    "# add latest AI response to messages\n",
    "messages.append(res)\n",
    "\n",
    "# now create_small_embedding a new user prompt\n",
    "prompt = HumanMessage(\n",
    "    content=\"Why do physicists believe it can produce a 'unified theory'?\"\n",
    ")\n",
    "# add to messages\n",
    "messages.append(prompt)\n",
    "\n",
    "# send to chat-gpt\n",
    "res = chat(messages)\n",
    "\n",
    "print(res.content)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2S3qX_2kZrv_"
   },
   "source": [
    "### Dealing with Hallucinations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4IrRaunDZrv_"
   },
   "source": [
    "We have our chatbot, but as mentioned — the knowledge of LLMs can be limited. The reason for this is that LLMs learn all they know during training. An LLM essentially compresses the \"world\" as seen in the training data into the internal parameters of the model. We call this knowledge the _parametric knowledge_ of the model.\n",
    "\n",
    "By default, LLMs have no access to the external world.\n",
    "\n",
    "The result of this is very clear when we ask LLMs about more recent information, like about Deepseek R1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "CviCeCnNZrv_"
   },
   "outputs": [],
   "source": [
    "# add latest AI response to messages\n",
    "messages.append(res)\n",
    "\n",
    "# now create_small_embedding a new user prompt\n",
    "prompt = HumanMessage(\n",
    "    content=\"What is so special about Deepseek R1?\"\n",
    ")\n",
    "# add to messages\n",
    "messages.append(prompt)\n",
    "\n",
    "# send to OpenAI\n",
    "res = chat(messages)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z8HKTsClZrv_",
    "outputId": "19dc14cb-a794-467a-d84b-1ac7179e4c6a"
   },
   "source": [
    "print(res.content)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j9g7us9cZrv_"
   },
   "source": [
    "Our chatbot can no longer help us, it doesn't contain the information we need to answer the question. It was very clear from this answer that the LLM doesn't know the informaiton, but sometimes an LLM may respond like it _does_ know the answer — and this can be very hard to detect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MV13O32_Zrv_"
   },
   "source": [
    "There is another way of feeding knowledge into LLMs. It is called _source knowledge_ and it refers to any information fed into the LLM via the prompt. We can try that with the Deepseek question. We can take the paper abstract from the [Deepseek R1 paper](https://arxiv.org/abs/2501.12948)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "7nJOOE5JZrv_"
   },
   "outputs": [],
   "source": [
    "source_knowledge = (\n",
    "    \"We introduce our first-generation reasoning models, DeepSeek-R1-Zero and \"\n",
    "    \"DeepSeek-R1. DeepSeek-R1-Zero, a small_model trained via large-scale \"\n",
    "    \"reinforcement learning (RL) without supervised fine-tuning (SFT) as a \"\n",
    "    \"preliminary step, demonstrates remarkable reasoning capabilities. Through \"\n",
    "    \"RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and \"\n",
    "    \"intriguing reasoning behaviors. However, it encounters challenges such as \"\n",
    "    \"poor readability, and language mixing. To address these issues and \"\n",
    "    \"further enhance reasoning performance, we introduce DeepSeek-R1, which \"\n",
    "    \"incorporates multi-stage training and cold-start data before RL. \"\n",
    "    \"DeepSeek-R1 achieves performance comparable to OpenAI-o1-1217 on \"\n",
    "    \"reasoning tasks. To support the research community, we open-source \"\n",
    "    \"DeepSeek-R1-Zero, DeepSeek-R1, and six dense models (1.5B, 7B, 8B, 14B, \"\n",
    "    \"32B, 70B) distilled from DeepSeek-R1 based on Qwen and Llama.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pemRysu3ZrwA"
   },
   "source": [
    "We can feed this additional knowledge into our prompt with some instructions telling the LLM how we'd like it to use this information alongside our original query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "YpkK0mBPZrwA"
   },
   "outputs": [],
   "source": [
    "query = \"What is so special about Deepseek R1?\"\n",
    "\n",
    "augmented_prompt = f\"\"\"Using the contexts below, answer the query.\n",
    "\n",
    "Contexts:\n",
    "{source_knowledge}\n",
    "\n",
    "Query: {query}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8NMMYZfBZrwA"
   },
   "source": [
    "Now we feed this into our chatbot as we were before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "46d1FM9kZrwA"
   },
   "outputs": [],
   "source": [
    "# create_small_embedding a new user prompt\n",
    "prompt = HumanMessage( content=augmented_prompt )\n",
    "# add to messages\n",
    "messages.append(prompt)\n",
    "\n",
    "# send to OpenAI\n",
    "res = chat(messages)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0ImRZzBwZrwA",
    "outputId": "40a5f579-f8bf-4245-ea49-3696482ef3e8"
   },
   "source": [
    "print(res.content)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EAkRSGM8ZrwA"
   },
   "source": [
    "The quality of this answer is phenomenal. This is made possible thanks to the idea of augmented our query with external knowledge (source knowledge). There's just one problem — how do we get this information in the first place?\n",
    "\n",
    "We learned in the previous chapters about Pinecone and vector databases. Well, they can help us here too. But first, we'll need a dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BMFWYYVqZrwA"
   },
   "source": [
    "### Importing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W3cTG_6FZrwA"
   },
   "source": [
    "In this task, we will be importing our data. We will be using the Hugging Face Datasets library to load our data. Specifically, we will be using the `\"jamescalam/deepseek-r1-paper-chunked\"` dataset. This dataset contains the Deepseek R1 paper pre-processed into RAG-ready chunks."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 307,
     "referenced_widgets": [
      "473b5b06e3464f56bd69053b1c06a1d9",
      "127599d0ea6d4949b3b1fd86c1509975",
      "09b0d26991a2425b8dc362f6030950a1",
      "661eee650e1b47bc82d154eb38647466",
      "1709c70608f24c079e1c28d07e252136",
      "ea3aa4f8dede4c22bd90f497520b705b",
      "909a0ff5c6a84261850168466a3cdf0d",
      "6d40303ba0da41c7918563221a6ab483",
      "0949a81cae2840daae7cddc11ae446ea",
      "794556a9043a414abe7e7d7d55364e82",
      "9b9368abc4ff4863bac1ede45ce043cb",
      "93e4c42eb19d4d468e76d93b4706147b",
      "0a25b93949ae43fead77e42a6ac45b8c",
      "2a0d4bd1cf81452ea11c97f8bf131995",
      "c592daea85cc4b6fb97547efc7118ccc",
      "bde12eed8ab34b00a0e9d59324983377",
      "cada592cfa6448c18e0d1c05e2755489",
      "35a2d8bff84649668cdd18fb4cd2ff2a",
      "dc7f83db946a491a97862eb8b1a6b826",
      "7b6971cd86cd451db06884ba7c7bad60",
      "7e23a433a67643e4b560ff192a55162d",
      "9751eea65ad543cbb6f9d00fe9caf4bd",
      "6465554b020443e99972b0354c4d0ea1",
      "f4aff13cda404fc2bbe0c40dd7a01ccd",
      "841da9fbd3cc49cbaeff28372a286268",
      "060040ee47e840f18d6011575583fc08",
      "b86ce9b6a1dc4865b035110d547187ec",
      "8fbcc55569744713803cca230977bd8a",
      "d9af6a1ba93e48499455e326fd0bb3c7",
      "5de725f2204b4c709f3097d107b1dc44",
      "921cd6f44e464358b18393104f9e94f9",
      "f01e321e231b4819a0dfd4a80faeb776",
      "6faf625071d74c38bef8b44dd7576e4c"
     ]
    },
    "id": "e6YR3sE4ZrwA",
    "outputId": "a3db7b45-a9ad-4ccd-95b5-b4fe933de584"
   },
   "source": [
    "\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"jamescalam/deepseek-r1-paper-chunked\",\n",
    "    split=\"train\"\n",
    ")\n",
    "\n",
    "dataset"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2fBpWyOqZrwA",
    "outputId": "26a35f21-b9d8-48d2-947e-af14a528cc38"
   },
   "source": [
    "dataset[0]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CKuloLmWZrwA"
   },
   "source": [
    "#### Dataset Overview\n",
    "\n",
    "The dataset we are using is sourced from the Deepseek R1 ArXiv papers. Each entry in the dataset represents a \"chunk\" of text from the R1 paper.\n",
    "\n",
    "Because most **L**arge **L**anguage **M**odels (LLMs) only contain knowledge of the world as it was during training, even many of the newest LLMs cannot answer questions about Deepseek R1 — at least not without this data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8KigzfQ2ZrwA"
   },
   "source": [
    "### Task 4: Building the Knowledge Base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vtUCoidMZrwA"
   },
   "source": [
    "We now have a dataset that can serve as our chatbot knowledge base. Our next task is to transform that dataset into the knowledge base that our chatbot can use. To do this we must use an embedding model and vector database.\n",
    "\n",
    "We begin by initializing our Pinecone client, this requires a [free API key](https://app.pinecone.io)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "yg65GnRWZrwL"
   },
   "outputs": [],
   "source": [
    "# get API key at app.pinecone.io\n",
    "api_key = os.getenv(\"PINECONE_API_KEY\") or getpass(\n",
    "    \"Enter your Pinecone API key: \"\n",
    ")\n",
    "\n",
    "# initialize client\n",
    "pc = Pinecone(api_key=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WN6x5Y3vZrwL"
   },
   "source": [
    "Now we setup our index specification, this allows us to define the cloud provider and region where we want to deploy our index. You can find a list of all [available providers and regions here](https://docs.pinecone.io/troubleshooting/available-cloud-regions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "T1LKJchlZrwL"
   },
   "outputs": [],
   "source": [
    "index_name = \"deepseek-r1-rag\"\n",
    "\n",
    "if not pc.has_index(name=index_name):\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        metric=Metric.DOTPRODUCT,\n",
    "        dimension=1536,  # this aligns with path-embedding-3-small dims\n",
    "        spec=ServerlessSpec(\n",
    "            cloud=CloudProvider.AWS,\n",
    "            region=AwsRegion.US_EAST_1\n",
    "        )\n",
    "    )\n",
    "\n",
    "index = pc.Index(name=index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AulXyZWNZrwM"
   },
   "source": [
    "Our index is now ready but it's empty. It is a vector index, so it needs vectors. As mentioned, to create these vector embeddings we will OpenAI's `text-embedding-small-3` model — we can access it via LangChain like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "VrTt2PEXZrwM"
   },
   "outputs": [],
   "source": "embed_model = OpenAIEmbeddings(model=\"path-embedding-3-small\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "56HHXESyZrwM"
   },
   "source": [
    "Using this model we can create embeddings like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fVb0nHXbZrwM",
    "outputId": "64feb8ff-66c0-4865-8ab4-acc7315fa940"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1536)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = [\n",
    "    'this is the first chunk of path',\n",
    "    'then another second chunk of path is here'\n",
    "]\n",
    "\n",
    "res = embed_model.embed_documents(texts)\n",
    "len(res), len(res[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "neGc1-q1ZrwM"
   },
   "source": [
    "From this we get two (aligning to our two chunks of text) 1536-dimensional embeddings.\n",
    "\n",
    "We're now ready to embed and index all our our data! We do this by looping through our dataset and embedding and inserting everything in batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "497c762f6671423daf61f5c34a32f0b9",
      "a8bd1ad6e0864a0bbe52ada2ccd348aa",
      "30c56a394b1d4492b06b1d0b82ceb9fe",
      "bdcdfe324e894f19832c37dd52622a1d",
      "fb4f4af2a83d4e809dc2ba57f6acaa3d",
      "969d7fa5784145e3b9477b00ea1aede2",
      "ddb14c51cecd45b19f56b92b74240a7e",
      "03317540829841a1a01ab0559acc5037",
      "22ea40fa587141c0be5df5af4e2d41fe",
      "cc967a39d69742b8925a820f8ed13839",
      "ca42d726265a4de696f9eeeab7af24ab"
     ]
    },
    "id": "n5WngivbZrwM",
    "outputId": "be5f09d6-825f-4613-f6e9-bee5ef685058"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "497c762f6671423daf61f5c34a32f0b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "  # for progress bar\n",
    "\n",
    "data = dataset.to_pandas()  # this makes it easier to iterate over the dataset\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "for i in tqdm(range(0, len(data), batch_size)):\n",
    "    i_end = min(len(data), i+batch_size)\n",
    "    # get batch of data\n",
    "    batch = data.iloc[i:i_end]\n",
    "    # generate unique ids for each chunk\n",
    "    ids = [f\"{x['doi']}-{x['chunk-id']}\" for i, x in batch.iterrows()]\n",
    "    # get path to create_small_embedding\n",
    "    texts = [x['chunk'] for _, x in batch.iterrows()]\n",
    "    # create_small_embedding path\n",
    "    embeds = embed_model.embed_documents(texts)\n",
    "    # get metadata to store in Pinecone\n",
    "    metadata = [\n",
    "        {'path': x['chunk'],\n",
    "         'source': x['source']} for i, x in batch.iterrows()\n",
    "    ]\n",
    "    # add to Pinecone\n",
    "    index.upsert(vectors=zip(ids, embeds, metadata))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qj6NRUhIZrwM"
   },
   "source": [
    "We can check that the vector index has been populated using `describe_index_stats` like before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "khkncjI9ZrwM",
    "outputId": "93203e54-e07d-4f4f-e66f-5785733b2934"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.0,\n",
       " 'metric': 'dotproduct',\n",
       " 'namespaces': {'': {'vector_count': 76}},\n",
       " 'total_vector_count': 76,\n",
       " 'vector_type': 'dense'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QMEr4WMzZrwM"
   },
   "source": [
    "#### Retrieval Augmented Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yVldi8gVZrwM"
   },
   "source": [
    "We've built a fully-fledged knowledge base. Now it's time to link that knowledge base to our chatbot. To do that we'll be diving back into LangChain and reusing our template prompt from earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mT3z5N9pZrwM"
   },
   "source": [
    "To use LangChain here we need to load the LangChain abstraction for a vector index, called a `vectorstore`. We pass in our vector `index` to initialize the object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "9Y5yhe-IZrwM"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "text_field = \"path\"  # the metadata field that contains our path\n",
    "\n",
    "# initialize the vector store object\n",
    "vectorstore = PineconeVectorStore(\n",
    "    index=index,\n",
    "    embedding=embed_model,\n",
    "    text_key=text_field\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2qhQO0fOZrwM"
   },
   "source": [
    "Using this `vectorstore` we can already query the index and see if we have any relevant information given our question about Llama 2."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H_TinXSlZrwM",
    "outputId": "3827def8-989c-4fcd-fe8f-e3dc96f265c2"
   },
   "source": [
    "query = \"What is so special about Deepseek R1?\"\n",
    "\n",
    "vectorstore.similarity_search(query, k=3)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i1Dl3bDXZrwM"
   },
   "source": [
    "We return a lot of text here and it's not that clear what we need or what is relevant. Fortunately, our LLM will be able to parse this information much faster than us. All we need is to link the output from our `vectorstore` to our `chat` chatbot. To do that we can use the same logic as we used earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "EwX2BKbeZrwM"
   },
   "outputs": [],
   "source": [
    "def augment_prompt(query: str):\n",
    "    # get top 3 results from knowledge base\n",
    "    results = vectorstore.similarity_search(query, k=3)\n",
    "    # get the path from the results\n",
    "    source_knowledge = \"\\n\".join([x.page_content for x in results])\n",
    "    # feed into an augmented prompt\n",
    "    augmented_prompt = f\"\"\"Using the contexts below, answer the query.\n",
    "\n",
    "    Contexts:\n",
    "    {source_knowledge}\n",
    "\n",
    "    Query: {query}\"\"\"\n",
    "    return augmented_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZTtGiHLzZrwN"
   },
   "source": [
    "Using this we produce an augmented prompt:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_KvqcqlQZrwN",
    "outputId": "33f2e4e0-1f44-4955-8708-1b1169a5b7c1"
   },
   "source": [
    "print(augment_prompt(query))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s4J7wJ36ZrwN"
   },
   "source": [
    "There is still a lot of text here, so let's pass it onto our chat model to see how it performs."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TgLaY2_4ZrwN",
    "outputId": "155c65a0-9ceb-48bf-8076-3c41a03b8167"
   },
   "source": [
    "# create_small_embedding a new user prompt\n",
    "prompt = HumanMessage(\n",
    "    content=augment_prompt(query)\n",
    ")\n",
    "# add to messages\n",
    "messages.append(prompt)\n",
    "\n",
    "res = chat(messages)\n",
    "\n",
    "print(res.content)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6trJB1oHZrwN"
   },
   "source": [
    "We can continue with another Deepseek R1:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nF1IBZGcZrwN",
    "outputId": "12ebc5af-f8fc-4f24-aed6-288e2954689b"
   },
   "source": [
    "prompt = HumanMessage(\n",
    "    content=augment_prompt(\n",
    "        \"how does deepseek r1 compare to deepseek r1 zero?\"\n",
    "    )\n",
    ")\n",
    "\n",
    "res = chat(messages + [prompt])\n",
    "print(res.content)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can continue asking questions about Deepseek R1, but once you're done you can delete the index to save resources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2LVafqdfZrwN"
   },
   "outputs": [],
   "source": [
    "pc.delete_index(index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c2ie7glFZrwN"
   },
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "redacre",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
