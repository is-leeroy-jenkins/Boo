{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# NLTK\n",
    "___\n"
   ],
   "id": "700c6019c3802579"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### ðŸ“¦ One-Time Setup (NLTK Resources)",
   "id": "17b64a5a35525229"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "import nltk\n",
    "nltk.download( 'punkt' )\n",
    "nltk.download( 'stopwords' )\n",
    "nltk.download( 'wordnet' )\n",
    "nltk.download( 'omw-1.4' )\n"
   ],
   "id": "ea993d341972b4d9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Load Dependencies",
   "id": "3957678e92123d8f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer"
   ],
   "id": "9a5c2c52dd74d494"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### ðŸ§  Full Pipeline",
   "id": "dda1854408000616"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# === Preprocessing Configuration ===\n",
    "EN_STOPWORDS = set( stopwords.words( 'english' ) )\n",
    "LEMMATIZER = WordNetLemmatizer( )\n",
    "STEMMER = SnowballStemmer( \"english\" )\n",
    "\n",
    "def preprocess_line( line, lower=True, punctuation=True,\n",
    "                     stopwords=True, lemmatize=True, stem=False ):\n",
    "    \"\"\"\n",
    "    Process a single line of text with optional steps:\n",
    "    - lower\n",
    "    - punctuation removal\n",
    "    - stopword removal\n",
    "    - lemmatization\n",
    "    - stemming (optional)\n",
    "    Returns the cleaned line as a string.\n",
    "    \"\"\"\n",
    "    tokens = word_tokenize( line )\n",
    "    processed = []\n",
    "    for token in tokens:\n",
    "        if lower:\n",
    "            token = token.lower( )\n",
    "\n",
    "        if punctuation and token in string.punctuation:\n",
    "            continue\n",
    "\n",
    "        if stopwords and token in EN_STOPWORDS:\n",
    "            continue\n",
    "\n",
    "        if lemmatize:\n",
    "            token = LEMMATIZER.lemmatize( token )\n",
    "\n",
    "        if stem:\n",
    "            token = STEMMER.stem( token )\n",
    "\n",
    "        processed.append( token )\n",
    "\n",
    "    return ' '.join( processed )\n",
    "\n",
    "\n",
    "def process_file( file_path, **preprocess_kwargs ):\n",
    "    \"\"\"\n",
    "        Read a text file line-by-line, apply preprocessing pipeline to each line,\n",
    "        and return a list of cleaned lines (original order preserved).\n",
    "        Pass any keyword args supported by `preprocess_line`.\n",
    "    \"\"\"\n",
    "    cleaned_lines = []\n",
    "    with open( file_path, 'r', encoding='utf-8' ) as file:\n",
    "        for line in file:\n",
    "            cleaned = preprocess_line( line, **preprocess_kwargs )\n",
    "            cleaned_lines.append( cleaned )\n",
    "    return cleaned_lines\n"
   ],
   "id": "af7f3fd7b2990387"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### ðŸ” Usage",
   "id": "91981ee2b140d645"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "file_path = 'path/to/Public_Law_118-32.txt'\n",
    "cleaned_lines = process_file(\n",
    "    file_path,\n",
    "    lowercase=True,\n",
    "    remove_punct=True,\n",
    "    remove_stopwords=True,\n",
    "    lemmatize=True,\n",
    "    stem=False  # Set to True if you want aggressive shortening\n",
    ")\n",
    "\n",
    "print( f\"Total lines: {len( cleaned_lines )}\" )\n",
    "print( \"Example cleaned line:\", cleaned_lines[ 0 ] )"
   ],
   "id": "c7c6e615a319e113"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Text Cleaner for PL 118-32",
   "id": "2463270042d983cf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Load Dependencies",
   "id": "6e0771597592ec33"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import re\n",
    "import os\n",
    "from pathlib import Path\n",
    "import openai\n",
    "import pandas as pd\n",
    "import time\n",
    "\n"
   ],
   "id": "63d1f338a9f2a8ca"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 1. Load File",
   "id": "c6aca1d3fff8a8ac"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# === 1. Load the Raw Text ===\n",
    "def load_text( file_path ):\n",
    "\twith open( file_path, 'r', encoding='utf-8' ) as f:\n",
    "\t\treturn f.read( )"
   ],
   "id": "a0478bcf04eb5543"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 2. Clean Document",
   "id": "c78317d9a750c73a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def clean_text( text: str ) -> str:\n",
    "    # Step 1: Normalize newlines\n",
    "    text = text.replace('\\r\\n', '\\n').replace('\\r', '\\n')\n",
    "\n",
    "    # Step 2: Remove page headers and footers (Public Law-specific)\n",
    "    text = re.sub(r'PUBLIC LAW 118â€“32.*?\\n', '', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\n\\s*\\d+\\s*\\n', '\\n', text)  # Remove page numbers between lines\n",
    "\n",
    "    # Step 3: Remove hyphenation at line breaks (e.g., \"appropria-\\ntion\")\n",
    "    text = re.sub(r'(\\w+)-\\n(\\w+)', r'\\1\\2', text)\n",
    "\n",
    "    # Step 4: Merge broken lines where sentence continues\n",
    "    text = re.sub(r'(?<!\\n)\\n(?![\\n])', ' ', text)\n",
    "\n",
    "    # Step 5: Collapse excessive whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "    # Step 6: Normalize section markers (optional but useful)\n",
    "    text = re.sub(r'(SEC\\.\\s*\\d+[A-Z]*\\.)', r'\\n\\n\\1', text)  # Add spacing before sections\n",
    "\n",
    "    return text.strip()\n"
   ],
   "id": "42c34418a1218398"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 3. Chunk File",
   "id": "9bf93b3631a66b6a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Simple chunking by words assuming ~1.3 words per token\n",
    "def chunk_text( text, max_tokens=512 ):\n",
    "\twords = text.split( )\n",
    "\tchunk_size = int( max_tokens * 1.3 )\n",
    "\tchunks = [ ' '.join( words[ i:i + chunk_size ] ) for i in range( 0, len( words ), chunk_size ) ]\n",
    "\treturn chunks\n"
   ],
   "id": "bdf2dd9223fede76"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### ðŸ” Example",
   "id": "842abb4747e1ed6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "file_path = 'path_to/Public_Law_118-32.txt'\n",
    "raw_text = load_text( file_path )\n",
    "cleaned_text = clean_text( raw_text )\n",
    "chunks = chunk_text( cleaned_text )\n",
    "print( f'Total Chunks: {len( chunks )}' )\n",
    "print( 'Sample chunk:\\n', chunks[ 0 ][ :1000 ] )"
   ],
   "id": "1620253dd31900a3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "fc9d08f36dde47ba"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "#  OpenAI Embedding\n",
    "___"
   ],
   "id": "9f097164e0f1b335"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### API key",
   "id": "10f8b0039c619d8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "openai.api_key = os.getenv( 'OPENAI_API_KEY' )",
   "id": "6f7c70b192899b4b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 1. Define embedding function",
   "id": "e1c76c3548b771af"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def embed_texts( texts, model='text-embedding-3-small', batch_size=10, sleep=1 ):\n",
    "    embeddings = []\n",
    "    for i in range( 0, len( texts ), batch_size ):\n",
    "        batch = texts[ i:i+batch_size ]\n",
    "        try:\n",
    "            response = openai.embeddings.create( input=batch, model=model )\n",
    "            batch_embeddings = [ e.embedding for e in response.data ]\n",
    "            embeddings.extend( batch_embeddings )\n",
    "        except Exception as e:\n",
    "            print( f'Error at batch {i}: {e}' )\n",
    "            # Retry or sleep to avoid rate limits\n",
    "            time.sleep( sleep )\n",
    "            continue\n",
    "\n",
    "    return embeddings\n"
   ],
   "id": "3d19dce1b72312ca"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 2. Embed chunks",
   "id": "7a1cbc3787a81ed2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 2. Embed chunks\n",
    "embeddings = embed_texts( chunks )"
   ],
   "id": "2e8dd3b9c7453b0c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 3.  Create DataFrame",
   "id": "7b29c98e70fdc973"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 3. Create DataFrame\n",
    "df_embeddings = pd.DataFrame( { chunks, embeddings } )\n"
   ],
   "id": "998cc4a4eb1c122f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 3. Save\n",
   "id": "ca13ccc289f545b4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 3. Save\n",
    "df_embeddings.to_parquet( 'public_law_118_32_embeddings.parquet', index=False )\n"
   ],
   "id": "eac78df8b7b91593"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 4. Preview",
   "id": "ae06c79ed3506313"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 4. Preview\n",
    "df_embeddings.head(2)"
   ],
   "id": "e9935e9a89b6aac5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Text Cleaning\n",
    "___"
   ],
   "id": "1f2312a84c08528a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 1.  Strip irrelevant content  while preserving structure.",
   "id": "f57da739bd434166"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import re\n",
    "\n",
    "def clean_text( text ):\n",
    "    # Remove form feeds, line breaks, etc.\n",
    "    text = re.sub( r'\\f+', ' ', text )\n",
    "    text = re.sub( r'\\n+', ' ', text )\n",
    "    text = re.sub( r'\\s{2,}', ' ', text )\n",
    "\n",
    "    # Normalize Section markers, Title headers, etc.\n",
    "    text = re.sub( r'SEC\\.\\s+(\\d+)\\.', r'Section \\1:', text )\n",
    "    return text.strip( )\n"
   ],
   "id": "936a3035cb982382"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 2. Chunk the Text",
   "id": "8c21a4c303c952e7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- Large documents need to be chunked (for context window limits during embedding).\n",
    "- Use semantic or structural chunking."
   ],
   "id": "19cbc1c90cb94e79"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "seps =[ '\\n\\n', '\\n', '.', ' ' ]\n",
    "splitter = RecursiveCharacterTextSplitter( chunk_size=1000, chunk_overlap=200, separators=seps )\n",
    "cleaned = clean_text( raw_text )\n",
    "chunks = splitter.split_text( cleaned )\n"
   ],
   "id": "40beb2e1042742e5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 3. Generate Embeddings",
   "id": "120bc6a0a62a9729"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- Use a language model (e.g., OpenAI, HuggingFace) to create vector representations of each chunk.",
   "id": "f62aab776049a508"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer( 'all-MiniLM-L6-v2' )\n",
    "embeddings = model.encode( chunks, show_progress_bar=True )\n"
   ],
   "id": "a67fd85d25aafd1c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 4. Create SQLite Database",
   "id": "d76b8831817cd4d9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- Design a table that links text chunks to their embeddings.",
   "id": "f06349ccd853f770"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import sqlite3\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "conn = sqlite3.connect( 'embeddings.db' )\n",
    "cursor = conn.cursor( )\n",
    "\n",
    "cursor.execute('''\n",
    "CREATE TABLE IF NOT EXISTS law_embeddings (\n",
    "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    chunk TEXT NOT NULL,\n",
    "    embedding BLOB NOT NULL\n",
    ")\n",
    "''')\n",
    "\n",
    "for chunk, vector in zip( chunks, embeddings ):\n",
    "    blob = pickle.dumps( vector )\n",
    "    cursor.execute( 'INSERT INTO law_embeddings (chunk, embedding) VALUES (?, ?)', (chunk, blob) )\n",
    "\n",
    "conn.commit( )\n",
    "conn.close( )\n"
   ],
   "id": "28ca2f108b874b20"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "####  Retrieval (Vector Search in SQLite)",
   "id": "ca5ad81f911f7250"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- You can perform semantic search by encoding a query and comparing via cosine similarity:",
   "id": "12113e4d3566bd2f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def cosine_similarity( a, b ):\n",
    "    return np.dot( a, b ) / ( np.linalg.norm( a ) * np.linalg.norm( b ) )\n",
    "\n",
    "query = 'Appropriations for Department of Defense'\n",
    "query_vec = model.encode( [ query ] )[ 0 ]\n",
    "\n",
    "conn = sqlite3.connect( 'embeddings.db' )\n",
    "cursor = conn.cursor( )\n",
    "cursor.execute( 'SELECT id, chunk, embedding FROM law_embeddings' )\n",
    "\n",
    "results = []\n",
    "for row in cursor.fetchall( ):\n",
    "    chunk_id, chunk_text, blob = row\n",
    "    stored_vec = pickle.loads( blob )\n",
    "    sim = cosine_similarity( query_vec, stored_vec )\n",
    "    results.append( ( sim, chunk_text ) )\n",
    "\n",
    "# Sort and get top N\n",
    "top_matches = sorted( results, key=lambda x: x[ 0 ], reverse=True )[ :5 ]\n"
   ],
   "id": "675d18dac5bd124d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#   Embedding-Pipeline Script\n",
    "___"
   ],
   "id": "3a7f735556cd0d3f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Load Dependencies\n",
    "\n"
   ],
   "id": "e593513dcb937f5f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import re\n",
    "import sqlite3\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import openai\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from sentence_transformers import SentenceTransformer"
   ],
   "id": "cf12b8bc356c9181"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Configuration",
   "id": "99ef339bdfea1fd8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Define paths\n",
    "TEXT_FILE = 'PublicLaw_118-42.txt'\n",
    "DB_FILE = 'law_embeddings.db'\n",
    "EMBEDDING_MODEL = 'all-MiniLM-L6-v2'\n",
    "CHUNK_SIZE = 1000\n",
    "CHUNK_OVERLAP = 200\n"
   ],
   "id": "e8ee553da46c407c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Load and Clean Raw Text",
   "id": "842a900a1b80cde5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def load_and_clean_text( filepath ):\n",
    "    with open( filepath, 'r', encoding='utf-8' ) as file:\n",
    "        raw_text = file.read( )\n",
    "\n",
    "    # Basic normalization\n",
    "    text = re.sub( r'\\f+', ' ', raw_text )\n",
    "    text = re.sub( r'\\n+', ' ', text )\n",
    "    text = re.sub( r'\\s{2,}', ' ', text )\n",
    "    text = re.sub( r'SEC\\.\\s+(\\d+)\\.', r'Section \\1:', text )\n",
    "    return text.strip( )"
   ],
   "id": "c202cc689865f32"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Chunk the Clean Text\n",
   "id": "e4eb076550224fc5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def chunk_text( clean_text ):\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=CHUNK_SIZE,\n",
    "        chunk_overlap=CHUNK_OVERLAP,\n",
    "        separators=[ '\\n\\n', '\\n', '.', ' ' ]\n",
    "    )\n",
    "    return splitter.split_text( clean_text )\n"
   ],
   "id": "e77fae8c30040047"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Generate Embeddings",
   "id": "ae3ddba5e85404c6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def get_embedding( text, model=OPENAI_MODEL ):\n",
    "    response = openai.Embedding.create(\n",
    "        input=text,\n",
    "        model=model\n",
    "    )\n",
    "    return response[ 'data' [ 0 ][ 'embedding' ] ]\n",
    "\n",
    "\n",
    "def embed_chunks( chunks ):\n",
    "    embeddings = [ ]\n",
    "    for chunk in tqdm( chunks, desc='Embedding chunks via OpenAI' ):\n",
    "        try:\n",
    "            embedding = get_embedding( chunk )\n",
    "            embeddings.append( embedding )\n",
    "        except Exception as e:\n",
    "            print( f'Error embedding chunk: {e}' )\n",
    "            embeddings.append( [ 0.0 ] * 1536 )  # Placeholder for failed requests\n",
    "    return embeddings"
   ],
   "id": "8fa7587e7a3094be"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Create SQLite DB\n",
    "\n"
   ],
   "id": "3c985e6d3ecd5e8f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def create_and_populate_db( chunks, embeddings, db_path ):\n",
    "    conn = sqlite3.connect( db_path )\n",
    "    cursor = conn.cursor( )\n",
    "\n",
    "    cursor.execute('''\n",
    "    CREATE TABLE IF NOT EXISTS law_embeddings (\n",
    "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "        chunk TEXT NOT NULL,\n",
    "        embedding BLOB NOT NULL\n",
    "    )\n",
    "    ''')\n",
    "\n",
    "    for chunk, vector in zip( chunks, embeddings ):\n",
    "        blob = pickle.dumps( vector )\n",
    "        cursor.execute(\n",
    "            'INSERT INTO law_embeddings ( chunk, embedding ) VALUES ( ?, ? )',\n",
    "            ( chunk, blob )\n",
    "        )\n",
    "\n",
    "    conn.commit( )\n",
    "    conn.close( )\n"
   ],
   "id": "ea15943e543e0d70"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Script",
   "id": "e82751929d4fd563"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# === MAIN ===\n",
    "def main():\n",
    "    print('Step 1: Load and clean text')\n",
    "    cleaned_text = load_and_clean_text(TEXT_FILE)\n",
    "\n",
    "    print('Step 2: Chunking text')\n",
    "    chunks = chunk_text(cleaned_text)\n",
    "    print(f'Total chunks: {len(chunks)}')\n",
    "\n",
    "    print('Step 3: Embedding with OpenAI API')\n",
    "    embeddings = embed_chunks(chunks)\n",
    "\n",
    "    print('Step 4: Saving to SQLite')\n",
    "    create_and_populate_db(chunks, embeddings, DB_FILE)\n",
    "\n",
    "    print(f'Pipeline complete. Embeddings stored in: {DB_FILE}')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ],
   "id": "5f2d269aaa88212b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "de305bd9c7ccbd96"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# === 1. Load Model ===\n",
    "# You can try other models like 'all-MiniLM-L6-v2', 'all-mpnet-base-v2', or 'multi-qa-MiniLM-L6-cos-v1'\n",
    "model = SentenceTransformer( 'all-MiniLM-L6-v2' )\n",
    "\n",
    "# === 2. Embed Chunks ===\n",
    "def embed_with_sentence_transformers( texts, model ):\n",
    "    return model.encode( texts, show_progress_bar=True, convert_to_numpy=True )\n",
    "\n",
    "local_embeddings = embed_with_sentence_transformers( chunks, model )\n",
    "\n",
    "# === 3. Save in a DataFrame ===\n",
    "df_local = pd.DataFrame({\n",
    "    'chunk': chunks,\n",
    "    'embedding': list( local_embeddings )  # numpy arrays to list for DataFrame compatibility\n",
    "})\n",
    "\n",
    "# === 4. Save to Disk ===\n",
    "df_local.to_parquet( 'public_law_118_32_local_embeddings.parquet', index=False )\n",
    "\n",
    "# === 5. Preview ===\n",
    "df_local.head( 2 )\n",
    "\n"
   ],
   "id": "47df028a12155604"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "### ðŸ” Example Usage\n",
    "\n"
   ],
   "id": "884b394a9358280e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "\n",
   "id": "65d9419c6c6eb8cc"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
