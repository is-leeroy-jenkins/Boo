{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# NLTK\n",
    "___\n"
   ],
   "id": "700c6019c3802579"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 📦 One-Time Setup (NLTK Resources)",
   "id": "17b64a5a35525229"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "import nltk\n",
    "nltk.download( 'punkt' )\n",
    "nltk.download( 'stopwords' )\n",
    "nltk.download( 'wordnet' )\n",
    "nltk.download( 'omw-1.4' )\n"
   ],
   "id": "ea993d341972b4d9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Load Dependencies",
   "id": "3957678e92123d8f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer"
   ],
   "id": "9a5c2c52dd74d494"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 🧠 Full Pipeline",
   "id": "dda1854408000616"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# === Preprocessing Configuration ===\n",
    "EN_STOPWORDS = set( stopwords.words( 'english' ) )\n",
    "LEMMATIZER = WordNetLemmatizer( )\n",
    "STEMMER = SnowballStemmer( \"english\" )\n",
    "\n",
    "def preprocess_line( line, lower=True, punctuation=True,\n",
    "                     stopwords=False, lemmatize=True, stem=False ):\n",
    "    \"\"\"\n",
    "    Process a single line of text with optional steps:\n",
    "    - lower\n",
    "    - punctuation removal\n",
    "    - stopword removal\n",
    "    - lemmatization\n",
    "    - stemming (optional)\n",
    "    Returns the cleaned line as a string.\n",
    "    \"\"\"\n",
    "    tokens = word_tokenize( line )\n",
    "    processed = [ ]\n",
    "    for token in tokens:\n",
    "        if lower:\n",
    "            token = token.lower( )\n",
    "\n",
    "        if punctuation and token in string.punctuation:\n",
    "            continue\n",
    "\n",
    "        if stopwords and token in EN_STOPWORDS:\n",
    "            continue\n",
    "\n",
    "        if lemmatize:\n",
    "            token = LEMMATIZER.lemmatize( token )\n",
    "\n",
    "        if stem:\n",
    "            token = STEMMER.stem( token )\n",
    "\n",
    "        processed.append( token )\n",
    "\n",
    "    return ' '.join( processed )\n",
    "\n",
    "\n",
    "def process_file( file_path, **preprocess_kwargs ):\n",
    "    \"\"\"\n",
    "        Read a text file line-by-line, apply preprocessing pipeline to each line,\n",
    "        and return a list of cleaned lines (original order preserved).\n",
    "        Pass any keyword args supported by `preprocess_line`.\n",
    "    \"\"\"\n",
    "    cleaned_lines = []\n",
    "    with open( file_path, 'r', encoding='utf-8' ) as file:\n",
    "        for line in file:\n",
    "            cleaned = preprocess_line( line, **preprocess_kwargs )\n",
    "            cleaned_lines.append( cleaned )\n",
    "    return cleaned_lines\n"
   ],
   "id": "af7f3fd7b2990387"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 🔍 Usage",
   "id": "91981ee2b140d645"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "file_path = 'path/to/Public_Law_118-32.txt'\n",
    "cleaned_lines = process_file( file_path, lowercase=True, remove_punct=True,\n",
    "    remove_stopwords=False, lemmatize=True, stem=True )\n",
    "\n",
    "print( f\"Total lines: {len( cleaned_lines )}\" )\n",
    "print( \"Example cleaned line:\", cleaned_lines[ 0 ] )"
   ],
   "id": "c7c6e615a319e113"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Text Cleaner for PL 118-32",
   "id": "2463270042d983cf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Load Dependencies",
   "id": "6e0771597592ec33"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-31T19:50:18.568643Z",
     "start_time": "2025-03-31T19:50:18.559980Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "import os\n",
    "from pathlib import Path\n",
    "import openai\n",
    "import pandas as pd\n",
    "import time\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n"
   ],
   "id": "63d1f338a9f2a8ca",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 1. Load File",
   "id": "c6aca1d3fff8a8ac"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# === 1. Load the Raw Text ===\n",
    "def load_text( file_path ):\n",
    "\twith open( file_path, 'r', encoding='utf-8' ) as f:\n",
    "\t\treturn f.read( )"
   ],
   "id": "a0478bcf04eb5543"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 2. Clean Document",
   "id": "5e61d59191d25e5c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def clean_text( text: str ) -> str:\n",
    "    # Step 1: Normalize normalized\n",
    "    text = text.replace('\\r\\n', '\\n').replace('\\r', '\\n')\n",
    "\n",
    "    # Step 2: Remove page headers and footers (Public Law-specific)\n",
    "    text = re.sub(r'PUBLIC LAW 118–32.*?\\n', '', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\n\\s*\\d+\\s*\\n', '\\n', text)  # Remove page numbers between lines\n",
    "\n",
    "    # Step 3: Remove hyphenation at line breaks (e.g., \"appropria-\\ntion\")\n",
    "    text = re.sub(r'(\\w+)-\\n(\\w+)', r'\\1\\2', text)\n",
    "\n",
    "    # Step 4: Merge broken lines where sentence continues\n",
    "    text = re.sub(r'(?<!\\n)\\n(?![\\n])', ' ', text)\n",
    "\n",
    "    # Step 5: Collapse excessive whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "    return text.strip()\n"
   ],
   "id": "42c34418a1218398"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 3. Chunk File",
   "id": "9bf93b3631a66b6a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Simple chunking by words assuming ~1.3 words per token\n",
    "def chunk_text( text, max_tokens=512 ):\n",
    "\twords = text.split( )\n",
    "\tchunk_size = int( max_tokens * 1.3 )\n",
    "\tchunks = [ ' '.join( words[ i:i + chunk_size ] ) for i in range( 0, len( words ), chunk_size ) ]\n",
    "\treturn chunks\n"
   ],
   "id": "bdf2dd9223fede76"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### 🔍 Example",
   "id": "842abb4747e1ed6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "file_path = 'path_to/Public_Law_118-32.txt'\n",
    "raw_text = load_text( file_path )\n",
    "cleaned_text = clean_text( raw_text )\n",
    "chunks = chunk_text( cleaned_text )\n",
    "print( f'Total Chunks: {len( chunks )}' )\n",
    "print( 'Sample chunk:\\n', chunks[ 0 ][ :1000 ] )"
   ],
   "id": "1620253dd31900a3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "fc9d08f36dde47ba"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "#  OpenAI Embedding\n",
    "___"
   ],
   "id": "9f097164e0f1b335"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### API key",
   "id": "10f8b0039c619d8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create client\n",
    "client = OpenAI( )\n",
    "client.api_key = os.getenv( 'OPENAI_API_KEY' )"
   ],
   "id": "6f7c70b192899b4b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 1. Define embedding function",
   "id": "e1c76c3548b771af"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def embed_texts( texts, model='text-embedding-3-small', batch_size=10, sleep=1 ):\n",
    "    embeddings = []\n",
    "    for i in range( 0, len( texts ), batch_size ):\n",
    "        batch = texts[ i:i+batch_size ]\n",
    "        try:\n",
    "            response = openai.embeddings.create( input=batch, model=model )\n",
    "            batch_embeddings = [ e.embedding for e in response.data ]\n",
    "            embeddings.extend( batch_embeddings )\n",
    "        except Exception as e:\n",
    "            print( f'Error at batch {i}: {e}' )\n",
    "            # Retry or sleep to avoid rate limits\n",
    "            time.sleep( sleep )\n",
    "            continue\n",
    "\n",
    "    return embeddings\n"
   ],
   "id": "3d19dce1b72312ca"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 2. Embed chunks",
   "id": "7a1cbc3787a81ed2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 2. Embed chunks\n",
    "embeddings = embed_texts( chunks )"
   ],
   "id": "2e8dd3b9c7453b0c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 3.  Create DataFrame",
   "id": "7b29c98e70fdc973"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 3. Create DataFrame\n",
    "df_embeddings = pd.DataFrame( { chunks, embeddings } )\n"
   ],
   "id": "998cc4a4eb1c122f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 3. Save\n",
   "id": "ca13ccc289f545b4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 3. Save\n",
    "df_embeddings.to_parquet( 'public_law_118_32_embeddings.parquet', index=False )\n"
   ],
   "id": "eac78df8b7b91593"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 4. Preview",
   "id": "ae06c79ed3506313"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 4. Preview\n",
    "df_embeddings.head(2)"
   ],
   "id": "e9935e9a89b6aac5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Text Cleaning\n",
    "___"
   ],
   "id": "1f2312a84c08528a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Load Dependencies",
   "id": "b6af68746614991a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-31T20:29:54.041366Z",
     "start_time": "2025-03-31T20:29:50.950333Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import re\n",
    "import sentence_transformers\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "import pickle"
   ],
   "id": "27a64e0eee5267f6",
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import transformers.modeling_utils because of the following error (look up to see its traceback):\ncannot import name 'float8_e4m3b11fnuz' from 'tensorflow.python.framework.dtypes' (C:\\Users\\terry\\source\\compilers\\py\\anaconda\\Lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py)",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mImportError\u001B[39m                               Traceback (most recent call last)",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\source\\compilers\\py\\anaconda\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1968\u001B[39m, in \u001B[36m_LazyModule._get_module\u001B[39m\u001B[34m(self, module_name)\u001B[39m\n\u001B[32m   1967\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1968\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mimportlib\u001B[49m\u001B[43m.\u001B[49m\u001B[43mimport_module\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m.\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m \u001B[49m\u001B[43m+\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodule_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[34;43m__name__\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m   1969\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\source\\compilers\\py\\anaconda\\Lib\\importlib\\__init__.py:90\u001B[39m, in \u001B[36mimport_module\u001B[39m\u001B[34m(name, package)\u001B[39m\n\u001B[32m     89\u001B[39m         level += \u001B[32m1\u001B[39m\n\u001B[32m---> \u001B[39m\u001B[32m90\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_bootstrap\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_gcd_import\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m[\u001B[49m\u001B[43mlevel\u001B[49m\u001B[43m:\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpackage\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlevel\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m<frozen importlib._bootstrap>:1387\u001B[39m, in \u001B[36m_gcd_import\u001B[39m\u001B[34m(name, package, level)\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m<frozen importlib._bootstrap>:1360\u001B[39m, in \u001B[36m_find_and_load\u001B[39m\u001B[34m(name, import_)\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m<frozen importlib._bootstrap>:1331\u001B[39m, in \u001B[36m_find_and_load_unlocked\u001B[39m\u001B[34m(name, import_)\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m<frozen importlib._bootstrap>:935\u001B[39m, in \u001B[36m_load_unlocked\u001B[39m\u001B[34m(spec)\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m<frozen importlib._bootstrap_external>:995\u001B[39m, in \u001B[36mexec_module\u001B[39m\u001B[34m(self, module)\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m<frozen importlib._bootstrap>:488\u001B[39m, in \u001B[36m_call_with_frames_removed\u001B[39m\u001B[34m(f, *args, **kwds)\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\source\\compilers\\py\\anaconda\\Lib\\site-packages\\transformers\\modeling_utils.py:61\u001B[39m\n\u001B[32m     57\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01mintegrations\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mtensor_parallel\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[32m     58\u001B[39m     SUPPORTED_TP_STYLES,\n\u001B[32m     59\u001B[39m     shard_and_distribute_module,\n\u001B[32m     60\u001B[39m )\n\u001B[32m---> \u001B[39m\u001B[32m61\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01mloss\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mloss_utils\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m LOSS_MAPPING\n\u001B[32m     62\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01mpytorch_utils\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m (  \u001B[38;5;66;03m# noqa: F401\u001B[39;00m\n\u001B[32m     63\u001B[39m     Conv1D,\n\u001B[32m     64\u001B[39m     apply_chunking_to_forward,\n\u001B[32m   (...)\u001B[39m\u001B[32m     69\u001B[39m     prune_linear_layer,\n\u001B[32m     70\u001B[39m )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\source\\compilers\\py\\anaconda\\Lib\\site-packages\\transformers\\loss\\loss_utils.py:19\u001B[39m\n\u001B[32m     17\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtorch\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mnn\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m BCEWithLogitsLoss, MSELoss\n\u001B[32m---> \u001B[39m\u001B[32m19\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01mloss_deformable_detr\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m DeformableDetrForObjectDetectionLoss, DeformableDetrForSegmentationLoss\n\u001B[32m     20\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01mloss_for_object_detection\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m ForObjectDetectionLoss, ForSegmentationLoss\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\source\\compilers\\py\\anaconda\\Lib\\site-packages\\transformers\\loss\\loss_deformable_detr.py:4\u001B[39m\n\u001B[32m      2\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtorch\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mnn\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mnn\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m4\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mimage_transforms\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m center_to_corners_format\n\u001B[32m      5\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mutils\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m is_scipy_available\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\source\\compilers\\py\\anaconda\\Lib\\site-packages\\transformers\\image_transforms.py:48\u001B[39m\n\u001B[32m     47\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m is_tf_available():\n\u001B[32m---> \u001B[39m\u001B[32m48\u001B[39m     \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtensorflow\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtf\u001B[39;00m\n\u001B[32m     50\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m is_flax_available():\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\source\\compilers\\py\\anaconda\\Lib\\site-packages\\tensorflow\\__init__.py:55\u001B[39m\n\u001B[32m     54\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtensorflow\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01m_api\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mv2\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m bitwise\n\u001B[32m---> \u001B[39m\u001B[32m55\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtensorflow\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01m_api\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mv2\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m compat\n\u001B[32m     56\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtensorflow\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01m_api\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mv2\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m config\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\source\\compilers\\py\\anaconda\\Lib\\site-packages\\tensorflow\\_api\\v2\\compat\\__init__.py:8\u001B[39m\n\u001B[32m      6\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01msys\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m_sys\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m8\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtensorflow\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01m_api\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mv2\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mcompat\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m v1\n\u001B[32m      9\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtensorflow\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01m_api\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mv2\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mcompat\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m v2\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\source\\compilers\\py\\anaconda\\Lib\\site-packages\\tensorflow\\_api\\v2\\compat\\v1\\__init__.py:30\u001B[39m\n\u001B[32m     29\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtensorflow\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01m_api\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mv2\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mcompat\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mv1\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m bitwise\n\u001B[32m---> \u001B[39m\u001B[32m30\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtensorflow\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01m_api\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mv2\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mcompat\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mv1\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m compat\n\u001B[32m     31\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtensorflow\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01m_api\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mv2\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mcompat\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mv1\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m config\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\source\\compilers\\py\\anaconda\\Lib\\site-packages\\tensorflow\\_api\\v2\\compat\\v1\\compat\\__init__.py:8\u001B[39m\n\u001B[32m      6\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01msys\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m_sys\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m8\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtensorflow\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01m_api\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mv2\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mcompat\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mv1\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mcompat\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m v1\n\u001B[32m      9\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtensorflow\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01m_api\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mv2\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mcompat\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mv1\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mcompat\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m v2\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\source\\compilers\\py\\anaconda\\Lib\\site-packages\\tensorflow\\_api\\v2\\compat\\v1\\compat\\v1\\__init__.py:36\u001B[39m\n\u001B[32m     35\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtensorflow\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01m_api\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mv2\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mcompat\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mv1\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m distributions\n\u001B[32m---> \u001B[39m\u001B[32m36\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtensorflow\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01m_api\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mv2\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mcompat\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mv1\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m dtypes\n\u001B[32m     37\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtensorflow\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01m_api\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mv2\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mcompat\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mv1\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m errors\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\source\\compilers\\py\\anaconda\\Lib\\site-packages\\tensorflow\\_api\\v2\\compat\\v1\\dtypes\\__init__.py:8\u001B[39m\n\u001B[32m      6\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01msys\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m_sys\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m8\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtensorflow\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01m_api\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mv2\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mcompat\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mv1\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mdtypes\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m experimental\n\u001B[32m      9\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtensorflow\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mpython\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mops\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mgen_string_ops\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m as_string \u001B[38;5;66;03m# line: 29\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\source\\compilers\\py\\anaconda\\Lib\\site-packages\\tensorflow\\_api\\v2\\compat\\v1\\dtypes\\experimental\\__init__.py:8\u001B[39m\n\u001B[32m      6\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01msys\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m_sys\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m8\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtensorflow\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mpython\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mframework\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mdtypes\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m float8_e4m3b11fnuz \u001B[38;5;66;03m# line: 465\u001B[39;00m\n\u001B[32m      9\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtensorflow\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mpython\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mframework\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mdtypes\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m float8_e4m3fn \u001B[38;5;66;03m# line: 439\u001B[39;00m\n",
      "\u001B[31mImportError\u001B[39m: cannot import name 'float8_e4m3b11fnuz' from 'tensorflow.python.framework.dtypes' (C:\\Users\\terry\\source\\compilers\\py\\anaconda\\Lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[31mRuntimeError\u001B[39m                              Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[5]\u001B[39m\u001B[32m, line 3\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mlangchain\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mtext_splitter\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m RecursiveCharacterTextSplitter\n\u001B[32m      2\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mre\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m3\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01msentence_transformers\u001B[39;00m\n\u001B[32m      4\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01msqlite3\u001B[39;00m\n\u001B[32m      5\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mnumpy\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mnp\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\source\\compilers\\py\\anaconda\\Lib\\site-packages\\sentence_transformers\\__init__.py:14\u001B[39m\n\u001B[32m      7\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mos\u001B[39;00m\n\u001B[32m      9\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01msentence_transformers\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mbackend\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[32m     10\u001B[39m     export_dynamic_quantized_onnx_model,\n\u001B[32m     11\u001B[39m     export_optimized_onnx_model,\n\u001B[32m     12\u001B[39m     export_static_quantized_openvino_model,\n\u001B[32m     13\u001B[39m )\n\u001B[32m---> \u001B[39m\u001B[32m14\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01msentence_transformers\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mcross_encoder\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[32m     15\u001B[39m     CrossEncoder,\n\u001B[32m     16\u001B[39m     CrossEncoderModelCardData,\n\u001B[32m     17\u001B[39m     CrossEncoderTrainer,\n\u001B[32m     18\u001B[39m     CrossEncoderTrainingArguments,\n\u001B[32m     19\u001B[39m )\n\u001B[32m     20\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01msentence_transformers\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mdatasets\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m ParallelSentencesDataset, SentencesDataset\n\u001B[32m     21\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01msentence_transformers\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mLoggingHandler\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m LoggingHandler\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\source\\compilers\\py\\anaconda\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\__init__.py:3\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m__future__\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m annotations\n\u001B[32m----> \u001B[39m\u001B[32m3\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01mCrossEncoder\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m CrossEncoder\n\u001B[32m      4\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01mmodel_card\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m CrossEncoderModelCardData\n\u001B[32m      5\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01mtrainer\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m CrossEncoderTrainer\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\source\\compilers\\py\\anaconda\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:15\u001B[39m\n\u001B[32m     13\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtorch\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m nn\n\u001B[32m     14\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtqdm\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mautonotebook\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m trange\n\u001B[32m---> \u001B[39m\u001B[32m15\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtransformers\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[32m     16\u001B[39m     AutoConfig,\n\u001B[32m     17\u001B[39m     AutoModelForSequenceClassification,\n\u001B[32m     18\u001B[39m     AutoTokenizer,\n\u001B[32m     19\u001B[39m     PretrainedConfig,\n\u001B[32m     20\u001B[39m     PreTrainedModel,\n\u001B[32m     21\u001B[39m )\n\u001B[32m     22\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtransformers\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mutils\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m PushToHubMixin\n\u001B[32m     23\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtyping_extensions\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m deprecated\n",
      "\u001B[36mFile \u001B[39m\u001B[32m<frozen importlib._bootstrap>:1412\u001B[39m, in \u001B[36m_handle_fromlist\u001B[39m\u001B[34m(module, fromlist, import_, recursive)\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\source\\compilers\\py\\anaconda\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1956\u001B[39m, in \u001B[36m_LazyModule.__getattr__\u001B[39m\u001B[34m(self, name)\u001B[39m\n\u001B[32m   1954\u001B[39m     value = Placeholder\n\u001B[32m   1955\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m name \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m._class_to_module.keys():\n\u001B[32m-> \u001B[39m\u001B[32m1956\u001B[39m     module = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_get_module\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_class_to_module\u001B[49m\u001B[43m[\u001B[49m\u001B[43mname\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1957\u001B[39m     value = \u001B[38;5;28mgetattr\u001B[39m(module, name)\n\u001B[32m   1958\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m name \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m._modules:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\source\\compilers\\py\\anaconda\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1970\u001B[39m, in \u001B[36m_LazyModule._get_module\u001B[39m\u001B[34m(self, module_name)\u001B[39m\n\u001B[32m   1968\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m importlib.import_module(\u001B[33m\"\u001B[39m\u001B[33m.\u001B[39m\u001B[33m\"\u001B[39m + module_name, \u001B[38;5;28mself\u001B[39m.\u001B[34m__name__\u001B[39m)\n\u001B[32m   1969\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m-> \u001B[39m\u001B[32m1970\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[32m   1971\u001B[39m         \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mFailed to import \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m.\u001B[34m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodule_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m because of the following error (look up to see its\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   1972\u001B[39m         \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m traceback):\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00me\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m\n\u001B[32m   1973\u001B[39m     ) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01me\u001B[39;00m\n",
      "\u001B[31mRuntimeError\u001B[39m: Failed to import transformers.modeling_utils because of the following error (look up to see its traceback):\ncannot import name 'float8_e4m3b11fnuz' from 'tensorflow.python.framework.dtypes' (C:\\Users\\terry\\source\\compilers\\py\\anaconda\\Lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py)"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 1.  Strip irrelevant content  while preserving structure.",
   "id": "f57da739bd434166"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def clean_text( text ):\n",
    "    # Remove form feeds, line breaks, etc.\n",
    "    text = re.sub( r'\\f+', ' ', text )\n",
    "    text = re.sub( r'\\n+', ' ', text )\n",
    "    text = re.sub( r'\\s{2,}', ' ', text )\n",
    "\n",
    "    # Normalize Section markers, Title headers, etc.\n",
    "    text = re.sub( r'SECTION\\.\\s+(\\d+)\\.', r'Section \\1:', text )\n",
    "    return text.strip( )\n"
   ],
   "id": "936a3035cb982382"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 2. Chunk the Text",
   "id": "8c21a4c303c952e7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- Large documents need to be chunked (for context window limits during embedding).\n",
    "- Use semantic or structural chunking."
   ],
   "id": "19cbc1c90cb94e79"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "\n",
    "seps =[ '\\n\\n', '\\n', '.', ' ' ]\n",
    "splitter = RecursiveCharacterTextSplitter( chunk_size=1000, chunk_overlap=200, separators=seps )\n",
    "cleaned = clean_text( raw_text )\n",
    "chunks = splitter.split_text( cleaned )\n"
   ],
   "id": "40beb2e1042742e5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 3. Generate Embeddings",
   "id": "120bc6a0a62a9729"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- Use a language model (e.g., OpenAI, HuggingFace) to create vector representations of each chunk.",
   "id": "f62aab776049a508"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "\n",
    "model = SentenceTransformer( 'all-MiniLM-L6-v2' )\n",
    "embeddings = model.encode( chunks, show_progress_bar=True )\n"
   ],
   "id": "a67fd85d25aafd1c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 4. Create SQLite Database",
   "id": "d76b8831817cd4d9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- Design a table that links text chunks to their embeddings.",
   "id": "f06349ccd853f770"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "conn = sqlite3.connect( 'embeddings.db' )\n",
    "cursor = conn.cursor( )\n",
    "sql_create = '''\n",
    "CREATE TABLE IF NOT EXISTS law_embeddings\n",
    "(\n",
    "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    chunk TEXT NOT NULL,\n",
    "    embedding BLOB NOT NULL\n",
    ")\n",
    "'''\n",
    "\n",
    "cursor.execute( sql_create )\n",
    "\n",
    "for chunk, vector in zip( chunks, embeddings ):\n",
    "    blob = pickle.dumps( vector )\n",
    "    cursor.execute( 'INSERT INTO law_embeddings ( chunk, embedding ) VALUES (?, ?)', ( chunk, blob ) )\n",
    "\n",
    "conn.commit( )\n",
    "conn.close( )\n"
   ],
   "id": "28ca2f108b874b20"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "####  Retrieval (Vector Search in SQLite)",
   "id": "ca5ad81f911f7250"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- You can perform semantic search by encoding a query and comparing via cosine similarity:",
   "id": "12113e4d3566bd2f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def cosine_similarity( a, b ):\n",
    "    return np.dot( a, b ) / ( np.linalg.norm( a ) * np.linalg.norm( b ) )"
   ],
   "id": "70665fe56a643b3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "query = 'Appropriations for Department of Defense'\n",
    "query_vec = model.encode( [ query ] )[ 0 ]\n",
    "\n",
    "conn = sqlite3.connect( 'embeddings.db' )\n",
    "cursor = conn.cursor( )\n",
    "cursor.execute( 'SELECT id, chunk, embedding FROM law_embeddings' )\n",
    "\n",
    "results = []\n",
    "for row in cursor.fetchall( ):\n",
    "    chunk_id, chunk_text, blob = row\n",
    "    stored_vec = pickle.loads( blob )\n",
    "    sim = cosine_similarity( query_vec, stored_vec )\n",
    "    results.append( ( sim, chunk_text ) )\n",
    "\n",
    "# Sort and get top N\n",
    "top_matches = sorted( results, key=lambda x: x[ 0 ], reverse=True )[ :5 ]\n"
   ],
   "id": "675d18dac5bd124d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#   Embedding-Pipeline Script\n",
    "___"
   ],
   "id": "3a7f735556cd0d3f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Load Dependencies\n",
    "\n"
   ],
   "id": "e593513dcb937f5f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-31T20:28:22.663413Z",
     "start_time": "2025-03-31T20:28:22.654167Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "import sqlite3\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import openai\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ],
   "id": "cf12b8bc356c9181",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Configuration",
   "id": "99ef339bdfea1fd8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "a764c56d86acecd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Define paths\n",
    "TEXT_FILE = 'PublicLaw_118-42.txt'\n",
    "DB_FILE = 'law_embeddings.db'\n",
    "EMBEDDING_MODEL = 'all-MiniLM-L6-v2'\n",
    "CHUNK_SIZE = 1000\n",
    "CHUNK_OVERLAP = 200\n"
   ],
   "id": "e8ee553da46c407c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Load and Clean Raw Text",
   "id": "842a900a1b80cde5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def load_and_clean_text( filepath ):\n",
    "    with open( filepath, 'r', encoding='utf-8' ) as file:\n",
    "        raw_text = file.read( )\n",
    "\n",
    "    # Basic normalization\n",
    "    text = re.sub( r'\\f+', ' ', raw_text )\n",
    "    text = re.sub( r'\\n+', ' ', text )\n",
    "    text = re.sub( r'\\s{2,}', ' ', text )\n",
    "    return text.strip( )"
   ],
   "id": "c202cc689865f32"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Chunk the Clean Text\n",
   "id": "e4eb076550224fc5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def chunk_text( clean_text ):\n",
    "    splitter = RecursiveCharacterTextSplitter( chunk_size=1000, chunk_overlap=200,\n",
    "\t    separators=[ '\\n\\n', '\\n', '.', ' ' ] )\n",
    "\n",
    "    return splitter.split_text( clean_text )\n"
   ],
   "id": "e77fae8c30040047"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Generate Embeddings",
   "id": "ae3ddba5e85404c6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def get_embedding( text, model=OPENAI_MODEL ):\n",
    "    response = openai.Embedding.create( input=text, model=model )\n",
    "    return response[ 'data' [ 0 ][ 'embedding' ] ]\n",
    "\n",
    "\n",
    "def embed_chunks( chunks ):\n",
    "    embeddings = [ ]\n",
    "    for chunk in tqdm( chunks, desc='EmbeddingRequest chunks via OpenAI' ):\n",
    "        try:\n",
    "            embedding = get_embedding( chunk )\n",
    "            embeddings.append( embedding )\n",
    "        except Exception as e:\n",
    "            print( f'Error embedding chunk: {e}' )\n",
    "            embeddings.append( [ 0.0 ] * 1536 )  # Placeholder for failed requests\n",
    "    return embeddings"
   ],
   "id": "8fa7587e7a3094be"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Create SQLite DB\n",
    "\n"
   ],
   "id": "3c985e6d3ecd5e8f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def create_and_populate_db( chunks, embeddings, db_path ):\n",
    "    conn = sqlite3.connect( db_path )\n",
    "    cursor = conn.cursor( )\n",
    "    sql_create = '''\n",
    "    CREATE TABLE IF NOT EXISTS law_embeddings\n",
    "    (\n",
    "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "        chunk TEXT NOT NULL,\n",
    "        embedding BLOB NOT NULL\n",
    "    )\n",
    "    '''\n",
    "\n",
    "    cursor.execute( sql_create )\n",
    "    for chunk, vector in zip( chunks, embeddings ):\n",
    "        blob = pickle.dumps( vector )\n",
    "        sql_insert = 'INSERT INTO law_embeddings ( chunk, embedding ) VALUES ( ?, ? )'\n",
    "        cursor.execute( sql_insert, ( chunk, blob ) )\n",
    "\n",
    "    conn.commit( )\n",
    "    conn.close( )\n"
   ],
   "id": "ea15943e543e0d70"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Script",
   "id": "e82751929d4fd563"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# === MAIN ===\n",
    "def main():\n",
    "    print('Step 1: Load and clean text')\n",
    "    cleaned_text = load_and_clean_text(TEXT_FILE)\n",
    "\n",
    "    print('Step 2: Chunking text')\n",
    "    chunks = chunk_text(cleaned_text)\n",
    "    print(f'Total chunks: {len(chunks)}')\n",
    "\n",
    "    print('Step 3: EmbeddingRequest with OpenAI API')\n",
    "    embeddings = embed_chunks(chunks)\n",
    "\n",
    "    print('Step 4: Saving to SQLite')\n",
    "    create_and_populate_db(chunks, embeddings, DB_FILE)\n",
    "\n",
    "    print(f'Pipeline complete. Embeddings stored in: {DB_FILE}')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ],
   "id": "5f2d269aaa88212b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "de305bd9c7ccbd96"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# === 1. Load Model ===\n",
    "# You can try other models like 'all-MiniLM-L6-v2', 'all-mpnet-base-v2', or 'multi-qa-MiniLM-L6-cos-v1'\n",
    "model = SentenceTransformer( 'all-MiniLM-L6-v2' )\n",
    "\n",
    "# === 2. Embed Chunks ===\n",
    "def embed_with_sentence_transformers( texts, model ):\n",
    "    return model.encode( texts, show_progress_bar=True, convert_to_numpy=True )\n",
    "\n",
    "local_embeddings = embed_with_sentence_transformers( chunks, model )\n",
    "\n",
    "# === 3. Save in a DataFrame ===\n",
    "df_local = pd.DataFrame({\n",
    "    'chunk': chunks,\n",
    "    'embedding': list( local_embeddings )  # numpy arrays to list for DataFrame compatibility\n",
    "})\n",
    "\n",
    "# === 4. Save to Disk ===\n",
    "df_local.to_parquet( 'public_law_118_32_local_embeddings.parquet', index=False )\n",
    "\n",
    "# === 5. Preview ===\n",
    "df_local.head( 2 )\n",
    "\n"
   ],
   "id": "47df028a12155604"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Fine-Tuning",
   "id": "e1efd28f65f33d39"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import json\n",
    "import openai\n",
    "import os\n",
    "import pandas as pd\n",
    "from pprint import pprint\n"
   ],
   "id": "51f033fc98145091"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "client = openai.OpenAI(\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    "    organization=\"<org id>\",\n",
    "    project=\"<project id>\",\n",
    ")"
   ],
   "id": "b11d00974dbaa480"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Read in the dataset we'll use for this task.\n",
    "# This will be the RecipesNLG dataset, which we've cleaned to only contain documents from www.cookbooks.com\n",
    "recipe_df = pd.read_csv(\"data/cookbook_recipes_nlg_10k.csv\")\n",
    "\n",
    "recipe_df.head()"
   ],
   "id": "aa84f0c255158f4f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "system_message = \"You are a helpful recipe assistant. You are to extract the generic ingredients from each of the recipes provided.\"\n",
    "\n",
    "\n",
    "def create_user_message(row):\n",
    "    return f\"Title: {row['title']}\\n\\nIngredients: {row['ingredients']}\\n\\nGeneric ingredients: \"\n",
    "\n",
    "\n",
    "def prepare_example_conversation(row):\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": system_message},\n",
    "            {\"role\": \"user\", \"content\": create_user_message(row)},\n",
    "            {\"role\": \"assistant\", \"content\": row[\"NER\"]},\n",
    "        ]\n",
    "    }\n",
    "\n"
   ],
   "id": "d29e9579f6f0e9dd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# use the first 100 rows of the dataset for training\n",
    "training_df = recipe_df.loc[0:100]\n",
    "\n",
    "# apply the prepare_example_conversation function to each row of the training_df\n",
    "training_data = training_df.apply(prepare_example_conversation, axis=1).tolist()\n",
    "\n",
    "for example in training_data[:5]:\n",
    "    print(example)"
   ],
   "id": "23bdc89006166778"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "validation_df = recipe_df.loc[101:200]\n",
    "validation_data = validation_df.apply(\n",
    "    prepare_example_conversation, axis=1).tolist()"
   ],
   "id": "4773228f51c07750"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def write_jsonl(data_list: list, filename: str) -> None:\n",
    "    with open(filename, \"w\") as out:\n",
    "        for ddict in data_list:\n",
    "            jout = json.dumps(ddict) + \"\\n\"\n",
    "            out.write(jout)"
   ],
   "id": "6dcf445d39eee404"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "652e2134d8966009"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "training_file_name = \"tmp_recipe_finetune_training.jsonl\"\n",
    "write_jsonl(training_data, training_file_name)\n",
    "\n",
    "validation_file_name = \"tmp_recipe_finetune_validation.jsonl\"\n",
    "write_jsonl(validation_data, validation_file_name)"
   ],
   "id": "a492a14231147287"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "170bad89408062ff"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def upload_file(file_name: str, purpose: str) -> str:\n",
    "    with open(file_name, \"rb\") as file_fd:\n",
    "        response = client.files.create(file=file_fd, purpose=purpose)\n",
    "    return response.id"
   ],
   "id": "10f941082775954c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "751aceb620d9c7bb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "training_file_id = upload_file(training_file_name, \"fine-tune\")\n",
    "validation_file_id = upload_file(validation_file_name, \"fine-tune\")"
   ],
   "id": "c8dbd2395ef59d03"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "feadf4f400f8454c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "MODEL = \"gpt-4o-mini-2024-07-18\"\n",
    "\n",
    "response = client.fine_tuning.jobs.create(\n",
    "    training_file=training_file_id,\n",
    "    validation_file=validation_file_id,\n",
    "    model=MODEL,\n",
    "    suffix=\"recipe-ner\",\n",
    ")\n",
    "\n",
    "job_id = response.id"
   ],
   "id": "5ca162663d00cad3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
