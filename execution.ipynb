{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Budget Execution\n",
    "#### A Machine Learning Model\n",
    "___"
   ],
   "id": "663821422c5ef21c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### Required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, mean_squared_error, r2_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier, LocalOutlierFactor, KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import (RandomForestClassifier, GradientBoostingClassifier, IsolationForest,\n",
    "                              HistGradientBoostingClassifier, AdaBoostClassifier, BaggingClassifier)\n",
    "\n",
    "from sklearn.svm import SVC, SVR, OneClassSVM\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.decomposition import PCA, IncrementalPCA, TruncatedSVD, FactorAnalysis\n",
    "from sklearn.manifold import Isomap, TSNE\n",
    "from sklearn.cluster import (KMeans, MiniBatchKMeans, DBSCAN, OPTICS,\n",
    "                             AgglomerativeClustering, Birch, SpectralClustering)\n",
    "\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import (LogisticRegression, RidgeClassifier, SGDClassifier, Perceptron,\n",
    "                                  LinearRegression, Ridge, Lasso, ElasticNet, BayesianRidge,\n",
    "                                  HuberRegressor, SGDRegressor)\n",
    "from sklearn.ensemble import (RandomForestRegressor, GradientBoostingRegressor,\n",
    "                              HistGradientBoostingRegressor, AdaBoostRegressor)\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.inspection import permutation_importance\n",
    "from xgboost import XGBClassifier\n",
    "from scipy.stats import zscore, ttest_ind, f_oneway, chi2_contingency, pearsonr, spearmanr\n",
    "import warnings\n",
    "\n",
    "hdr = '\\r\\n' + '-' * 120 + '\\r\\n'\n",
    "nwln = '\\r\\n'\n",
    "warnings.filterwarnings( 'ignore' )"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## I. Abstract\n",
    "\n",
    "In the evolving landscape of public financial management, data-driven decision-making is increasingly essential. The United States federal budget, comprising trillions of dollars annually, is executed across thousands of accounts managed by hundreds of agencies. Despite decades of modernization, the analytical capacity to predict spending trends, identify execution anomalies, and classify budget behaviors remains underutilized.\n",
    "\n",
    "This study presents a comprehensive application of machine learning (ML) and statistical modeling techniques to analyze federal account balances. Drawing on a full fiscal year's worth of Treasury Account Symbol (TAS)-level data, we demonstrate how a suite of supervised and unsupervised models can illuminate budgetary trends, detect execution outliers, reduce dimensional complexity, and enhance interpretability.\n",
    "\n",
    "## II. Methods\n",
    "\n",
    "### Data Source and Preparation\n",
    "\n",
    "The dataset used comprises 56,987 records from the Treasury's Federal Account Symbols, with variables including AnnualAppropriations, BorrowingAuthority, ContractAuthority, OffsettingReceipts, Obligations, UnobligatedBalance, and Outlays. All missing values were imputed using SimpleImputer (mean strategy), and features were scaled using StandardScaler.\n",
    "\n",
    "### Dimensionality Reduction\n",
    "\n",
    "To facilitate visualization and mitigate multicollinearity, we applied the following techniques:\n",
    "- Principal Component Analysis (PCA)\n",
    "- Incremental PCA\n",
    "- Truncated Singular Value Decomposition (TruncatedSVD)\n",
    "- Factor Analysis\n",
    "- Isometric Mapping (Isomap)\n",
    "- t-distributed Stochastic Neighbor Embedding (t-SNE)\n",
    "\n",
    "Each method projected the financial feature space into 2D, enabling visual inspection of structure and clustering potential.\n",
    "\n",
    "### Classification and Clustering\n",
    "\n",
    "A binary classification task was formulated to identify accounts with above-median obligations. Models included:\n",
    "- Linear: LogisticRegression, RidgeClassifier, SGDClassifier\n",
    "- Nonlinear: KNeighbors, DecisionTree, RandomForest, SVC\n",
    "- Ensemble: AdaBoost, Bagging, GradientBoosting, XGBoost\n",
    "- Neural Networks: MLPClassifier\n",
    "- Probabilistic: Naive Bayes, LDA, QDA\n",
    "- Clustering: KMeans, DBSCAN, OPTICS, Spectral, Birch\n",
    "\n",
    "Each classifier was evaluated using accuracy and classification reports, while clustering methods used unsupervised feature embeddings.\n",
    "\n",
    "### Anomaly Detection\n",
    "\n",
    "To flag abnormal execution patterns, we implemented:\n",
    "- Isolation Forest\n",
    "- One-Class SVM\n",
    "- Local Outlier Factor\n",
    "- Elliptic Envelope\n",
    "\n",
    "These models estimated execution outliers based on deviation from the multivariate normal or density distributions.\n",
    "\n",
    "### Regression Analysis\n",
    "\n",
    "To predict Obligations, we trained 16 regressors, including:\n",
    "- Linear models (Linear, Ridge, Lasso, ElasticNet)\n",
    "- Robust models (BayesianRidge, Huber, SGD)\n",
    "- Tree-based (DecisionTree, RandomForest, GradientBoosting, HistGBR, AdaBoost, XGBoost)\n",
    "- Non-parametric (KNN, SVR, MLP)\n",
    "\n",
    "Model performance was evaluated using **R²** and **RMSE**, and feature importance was interpreted using:\n",
    "- Tree-based .feature_importances_\n",
    "- Permutation importance\n",
    "- SHAP values\n",
    "\n",
    "## III. Results\n",
    "\n",
    "- PCA and t-SNE uncovered latent account structures, validating dimensionality reduction utility.\n",
    "- Top classification models (GradientBoosting, RandomForest, LogisticRegression) achieved >90% accuracy in identifying high-obligation accounts.\n",
    "- Anomaly detectors flagged <5% of records as outliers; Isolation Forest showed highest interpretability.\n",
    "- Regression models explained up to 96.8% of the variance (R²) in obligations, with XGBoost and RandomForest leading.\n",
    "- Feature importance consistently ranked AnnualAppropriations and BorrowingAuthority as the most influential predictors.\n",
    "- SHAP and permutation analyses reinforced model trust and transparency.\n",
    "\n",
    "## IV. Discussion\n",
    "\n",
    "Our findings confirm the viability of ML techniques for federal budget analysis. Dimensionality reduction aids exploratory understanding, while classification models can segment accounts based on spending intensity. Anomaly detection contributes to audit risk analysis by flagging atypical execution patterns. Regression models are highly effective for predicting obligations, especially when enhanced with ensemble and gradient methods.\n",
    "\n",
    "Importantly, this study emphasizes interpretability. Through SHAP and permutation analysis, we provide not just prediction, but explanation—an essential requirement in public sector analytics.\n",
    "\n",
    "Future work will focus on temporal modeling (e.g., forecasting Outlays over time), incorporating account-level metadata (e.g., agency mission), and scaling the framework across fiscal years for comparative analytics.\n",
    "\n",
    "---\n",
    "\n",
    "Keywords: federal budget, machine learning, anomaly detection, regression, PCA, XGBoost, SHAP, obligations, appropriations\n"
   ],
   "id": "c1cf14eb1d464af3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Data Processing & Cleaning",
   "id": "bf4c8489f45dfa04"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Load the Excel file\n",
    "file_path = r'C:\\Users\\terry\\Desktop\\Account Balances.xlsx'\n",
    "xls = pd.ExcelFile( file_path )\n",
    "\n",
    "# Display all sheet names\n",
    "sheet_names = xls.sheet_names\n",
    "\n",
    "# Load all sheets into a dictionary of DataFrames\n",
    "dfs = { sheet: xls.parse( sheet ) for sheet in sheet_names }\n",
    "\n",
    "# Display summary of each sheet: name, shape, and column headers\n",
    "sheet_summaries = {\n",
    "    sheet: {\n",
    "        \"shape\": dfs[ sheet ].shape,\n",
    "        \"columns\": dfs[ sheet ].columns.tolist( )\n",
    "    }\n",
    "    for sheet in dfs\n",
    "}\n",
    "\n",
    "sheet_summaries\n"
   ],
   "id": "81cabe1a034479a4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Load the primary data sheet\n",
    "df = dfs['Data']\n",
    "\n",
    "# Select a representative subset of columns for preprocessing and modeling\n",
    "selected_columns = [\n",
    "    'AnnualAppropriations', 'BorrowingAuthority', 'ContractAuthority',\n",
    "    'OffsettingReceipts', 'Obligations', 'Recoveries', 'UnobligatedBalance', 'Outlays'\n",
    "]\n",
    "df_numeric = df[selected_columns]\n",
    "\n",
    "# Drop rows with all NaNs in the selected columns\n",
    "df_numeric = df_numeric.dropna(how='all')\n",
    "\n",
    "# Prepare imputers\n",
    "simple_imputer = SimpleImputer(strategy='mean')\n",
    "knn_imputer = KNNImputer(n_neighbors=5)\n",
    "\n",
    "# Prepare scalers\n",
    "standard_scaler = StandardScaler()\n",
    "minmax_scaler = MinMaxScaler()\n",
    "\n",
    "# Impute missing values\n",
    "df_simple_imputed = pd.DataFrame(simple_imputer.fit_transform(df_numeric), columns=df_numeric.columns)\n",
    "df_knn_imputed = pd.DataFrame(knn_imputer.fit_transform(df_numeric), columns=df_numeric.columns)\n",
    "\n",
    "# Scale the data\n",
    "df_standard_scaled = pd.DataFrame(standard_scaler.fit_transform(df_simple_imputed), columns=df_numeric.columns)\n",
    "df_minmax_scaled = pd.DataFrame(minmax_scaler.fit_transform(df_simple_imputed), columns=df_numeric.columns)\n",
    "\n",
    "# PCA on standard scaled data\n",
    "pca = PCA(n_components=2)\n",
    "pca_result = pca.fit_transform(df_standard_scaled)\n",
    "\n",
    "# Truncated SVD on min-max scaled data\n",
    "svd = TruncatedSVD(n_components=2)\n",
    "svd_result = svd.fit_transform(df_minmax_scaled)\n",
    "\n",
    "# Create plots\n",
    "# 1. PCA Scatter Plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x=pca_result[:, 0], y=pca_result[:, 1])\n",
    "plt.title(\"PCA - Standard Scaled Data\")\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# 2. Truncated SVD Scatter Plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x=svd_result[:, 0], y=svd_result[:, 1])\n",
    "plt.title(\"Truncated SVD - MinMax Scaled Data\")\n",
    "plt.xlabel(\"Component 1\")\n",
    "plt.ylabel(\"Component 2\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# 3. Correlation Heatmap (Standard Scaled Data)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(df_standard_scaled.corr(), annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
    "plt.title(\"Correlation Heatmap (Standard Scaled)\")\n",
    "plt.show()\n",
    "\n",
    "# 4. Histogram of Annual Appropriations (MinMax Scaled)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.histplot(df_minmax_scaled['AnnualAppropriations'], kde=True, bins=30)\n",
    "plt.title(\"Distribution of Annual Appropriations (MinMax Scaled)\")\n",
    "plt.xlabel(\"Annual Appropriations (Normalized)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ],
   "id": "eb4cb7ac1a134696"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "###  Exploratory Analysis",
   "id": "b7a487af16e6cb98"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Step 1: Prepare data\n",
    "data = df_simple_imputed.copy()\n",
    "X = data[['AnnualAppropriations', 'BorrowingAuthority', 'ContractAuthority']]\n",
    "y = data['Obligations']\n",
    "\n",
    "# Step 2: Z-score normalization\n",
    "z_scores = pd.DataFrame(zscore(data), columns=data.columns)\n",
    "\n",
    "# Step 3: T-test (high vs low appropriations)\n",
    "threshold = data['AnnualAppropriations'].median()\n",
    "group1 = data[data['AnnualAppropriations'] > threshold]['Obligations']\n",
    "group2 = data[data['AnnualAppropriations'] <= threshold]['Obligations']\n",
    "t_stat, t_pval = ttest_ind(group1, group2, nan_policy='omit')\n",
    "\n",
    "# Step 4: One-way ANOVA on Obligations across Appropriation quartiles\n",
    "quartiles = pd.qcut(data['AnnualAppropriations'], 4, labels=False, duplicates='drop')\n",
    "anova_groups = [data.loc[quartiles == i, 'Obligations'] for i in np.unique(quartiles)]\n",
    "anova_stat, anova_pval = f_oneway(*anova_groups)\n",
    "\n",
    "# Step 5: Chi-square test using binned values\n",
    "binned1 = pd.qcut(data['AnnualAppropriations'], 4, labels=False, duplicates='drop')\n",
    "binned2 = pd.qcut(data['Outlays'], 4, labels=False, duplicates='drop')\n",
    "contingency_table = pd.crosstab(binned1, binned2)\n",
    "chi2_stat, chi2_pval, _, _ = chi2_contingency(contingency_table)\n",
    "\n",
    "# Step 6: Correlation analysis\n",
    "pearson_corr, pearson_pval = pearsonr(data['AnnualAppropriations'], data['Obligations'])\n",
    "spearman_corr, spearman_pval = spearmanr(data['AnnualAppropriations'], data['Obligations'])\n",
    "\n",
    "# Step 7: Regression Model\n",
    "model = LinearRegression().fit(X, y)\n",
    "y_pred = model.predict(X)\n",
    "r2 = model.score(X, y)\n",
    "n, k = X.shape\n",
    "adj_r2 = 1 - (1 - r2) * (n - 1) / (n - k - 1)\n",
    "rss = np.sum((y - y_pred) ** 2)\n",
    "tss = np.sum((y - y.mean()) ** 2)\n",
    "explained_var = tss - rss\n",
    "f_stat = (explained_var / k) / (rss / (n - k - 1))\n",
    "\n",
    "# Step 8: Coefficient p-values with statsmodels\n",
    "X_const = sm.add_constant(X)\n",
    "ols_model = sm.OLS(y, X_const).fit()\n",
    "coeff_pvals = ols_model.pvalues\n",
    "\n",
    "# Step 9: Pearson correlation heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(data.corr(), annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
    "plt.title(\"Pearson Correlation Heatmap\")\n",
    "plt.show()\n",
    "\n",
    "# Step 10: Summary Output\n",
    "results_summary = {\n",
    "    \"T-test p-value\": t_pval,\n",
    "    \"ANOVA p-value\": anova_pval,\n",
    "    \"Chi-square p-value\": chi2_pval,\n",
    "    \"Pearson correlation\": pearson_corr,\n",
    "    \"Spearman correlation\": spearman_corr,\n",
    "    \"R²\": r2,\n",
    "    \"Adjusted R²\": adj_r2,\n",
    "    \"F-statistic\": f_stat,\n",
    "    \"Coefficient p-values\": coeff_pvals.to_dict()\n",
    "}\n",
    "\n",
    "results_summary\n"
   ],
   "id": "8596c91511bec281"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Clustering & Classification",
   "id": "e348de5878497722"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# STEP 1: Preprocess Data\n",
    "df = df_simple_imputed.copy()\n",
    "df['Target'] = (df['Obligations'] > df['Obligations'].median()).astype(int)\n",
    "\n",
    "features = ['AnnualAppropriations', 'BorrowingAuthority', 'ContractAuthority', 'OffsettingReceipts']\n",
    "X = df[features]\n",
    "y = df['Target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.3, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "X_full_scaled = scaler.fit_transform(X)  # For clustering\n",
    "\n",
    "\n",
    "# STEP 2: Define Models\n",
    "\n",
    "classification_models = {\n",
    "    \"LogisticRegression\": LogisticRegression(max_iter=1000),\n",
    "    \"RidgeClassifier\": RidgeClassifier(),\n",
    "    \"SGDClassifier\": SGDClassifier(max_iter=1000, tol=1e-3),\n",
    "    \"Perceptron\": Perceptron(),\n",
    "    \"KNeighborsClassifier\": KNeighborsClassifier(),\n",
    "    \"DecisionTreeClassifier\": DecisionTreeClassifier(),\n",
    "    \"RandomForestClassifier\": RandomForestClassifier(),\n",
    "    \"GradientBoostingClassifier\": GradientBoostingClassifier(),\n",
    "    \"HistGradientBoostingClassifier\": HistGradientBoostingClassifier(),\n",
    "    \"AdaBoostClassifier\": AdaBoostClassifier(),\n",
    "    \"BaggingClassifier\": BaggingClassifier(),\n",
    "    \"XGBClassifier\": XGBClassifier(use_label_encoder=False, eval_metric='logloss', verbosity=0),\n",
    "    \"SVC\": SVC(),\n",
    "    \"GaussianNB\": GaussianNB(),\n",
    "    \"BernoulliNB\": BernoulliNB(),\n",
    "    \"MLPClassifier\": MLPClassifier(max_iter=500),\n",
    "    \"QuadraticDiscriminantAnalysis\": QuadraticDiscriminantAnalysis(),\n",
    "    \"LinearDiscriminantAnalysis\": LinearDiscriminantAnalysis()\n",
    "}\n",
    "\n",
    "clustering_models = {\n",
    "    \"KMeans\": KMeans(n_clusters=2, random_state=42),\n",
    "    \"MiniBatchKMeans\": MiniBatchKMeans(n_clusters=2, random_state=42),\n",
    "    \"DBSCAN\": DBSCAN(eps=0.5, min_samples=5),\n",
    "    \"OPTICS\": OPTICS(min_samples=10),\n",
    "    \"AgglomerativeClustering\": AgglomerativeClustering(n_clusters=2),\n",
    "    \"Birch\": Birch(n_clusters=2),\n",
    "    \"SpectralClustering\": SpectralClustering(n_clusters=2, assign_labels='discretize', random_state=42)\n",
    "}\n",
    "\n",
    "# STEP 3: Run Classification\n",
    "classification_results = {}\n",
    "\n",
    "for name, model in classification_models.items():\n",
    "    try:\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        classification_results[name] = {\n",
    "            \"Accuracy\": accuracy,\n",
    "            \"Report\": classification_report(y_test, y_pred, output_dict=True)\n",
    "        }\n",
    "    except Exception as e:\n",
    "        classification_results[name] = {\"Error\": str(e)}\n",
    "\n",
    "# STEP 4: Run Clustering\n",
    "\n",
    "clustering_results = {}\n",
    "\n",
    "for name, model in clustering_models.items():\n",
    "    try:\n",
    "        labels = model.fit_predict(X_full_scaled)\n",
    "        clustering_results[name] = {\n",
    "            \"Labels\": labels[:10].tolist(),  # show first 10 to summarize\n",
    "            \"n_clusters\": len(np.unique(labels))\n",
    "        }\n",
    "    except Exception as e:\n",
    "        clustering_results[name] = {\"Error\": str(e)}\n",
    "\n",
    "# STEP 5: Summaries\n",
    "classification_summary = {\n",
    "    name: {\"Accuracy\": res.get(\"Accuracy\", None)}\n",
    "    for name, res in classification_results.items()\n",
    "}\n",
    "\n",
    "clustering_summary = {\n",
    "    name: {\n",
    "        \"n_clusters\": res[\"n_clusters\"] if \"n_clusters\" in res else None,\n",
    "        \"SampleLabels\": res[\"Labels\"] if \"Labels\" in res else None,\n",
    "        \"Error\": res.get(\"Error\", None)\n",
    "    }\n",
    "    for name, res in clustering_results.items()\n",
    "}\n",
    "\n",
    "# Output (for notebook use)\n",
    "print(\"Classification Summary:\")\n",
    "print(pd.DataFrame(classification_summary).T.sort_values(by=\"Accuracy\", ascending=False))\n",
    "\n",
    "print(\"\\nClustering Summary:\")\n",
    "print(pd.DataFrame(clustering_summary).T)\n"
   ],
   "id": "bec15e937bfe0e37"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Anomaly & Outlier Detection",
   "id": "c6d54affd0b6b270"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# Subset and scale the original imputed data\n",
    "anomaly_features = ['AnnualAppropriations', 'BorrowingAuthority', 'ContractAuthority', 'OffsettingReceipts']\n",
    "X_anomaly = df_simple_imputed[anomaly_features]\n",
    "X_anomaly_scaled = StandardScaler().fit_transform(X_anomaly)\n",
    "\n",
    "# Dictionary of anomaly detection models\n",
    "anomaly_models = {\n",
    "    \"IsolationForest\": IsolationForest(contamination=0.05, random_state=42),\n",
    "    \"OneClassSVM\": OneClassSVM(nu=0.05, kernel=\"rbf\", gamma='scale'),\n",
    "    \"LocalOutlierFactor\": LocalOutlierFactor(n_neighbors=20, contamination=0.05, novelty=True),\n",
    "    \"EllipticEnvelope\": EllipticEnvelope(contamination=0.05, random_state=42)\n",
    "}\n",
    "\n",
    "# Fit and predict anomaly scores\n",
    "anomaly_results = {}\n",
    "\n",
    "for name, model in anomaly_models.items():\n",
    "    try:\n",
    "        model.fit(X_anomaly_scaled)\n",
    "        labels = model.predict(X_anomaly_scaled)  # -1 for outliers, 1 for inliers\n",
    "        n_outliers = (labels == -1).sum()\n",
    "        anomaly_results[name] = {\n",
    "            \"OutliersDetected\": int(n_outliers),\n",
    "            \"InliersDetected\": int((labels == 1).sum())\n",
    "        }\n",
    "    except Exception as e:\n",
    "        anomaly_results[name] = {\"Error\": str(e)}\n",
    "\n",
    "anomaly_results\n"
   ],
   "id": "b5b3c32006ecb43f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Dimensionality Reduction",
   "id": "8a708878d6ff1259"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 1. Load and scale data\n",
    "df = df_simple_imputed.copy()\n",
    "features = ['AnnualAppropriations', 'BorrowingAuthority', 'ContractAuthority', 'OffsettingReceipts']\n",
    "X = df[features]\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "# 2. Apply Dimensionality Reduction\n",
    "reduction_methods = {\n",
    "    \"PCA\": PCA(n_components=2),\n",
    "    \"IncrementalPCA\": IncrementalPCA(n_components=2, batch_size=200),\n",
    "    \"TruncatedSVD\": TruncatedSVD(n_components=2),\n",
    "    \"FactorAnalysis\": FactorAnalysis(n_components=2),\n",
    "    \"Isomap\": Isomap(n_components=2, n_neighbors=10),\n",
    "    \"TSNE\": TSNE(n_components=2, perplexity=30, random_state=42)\n",
    "}\n",
    "\n",
    "reduced_data = {}\n",
    "\n",
    "for name, reducer in reduction_methods.items():\n",
    "    try:\n",
    "        X_red = reducer.fit_transform(X_scaled)\n",
    "        reduced_data[name] = X_red\n",
    "\n",
    "        # Plot each result\n",
    "        plt.figure(figsize=(6, 5))\n",
    "        plt.scatter(X_red[:, 0], X_red[:, 1], alpha=0.5)\n",
    "        plt.title(f\"{name} - 2D Projection\")\n",
    "        plt.xlabel(\"Component 1\")\n",
    "        plt.ylabel(\"Component 2\")\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"{name} failed: {e}\")\n"
   ],
   "id": "4b18f0e3734628c7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Regression Analysis",
   "id": "cba9aaedc10ee91b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# --- STEP 1: Load & Preprocess Data ---\n",
    "df = df_simple_imputed.copy()\n",
    "X = df[['AnnualAppropriations', 'BorrowingAuthority', 'ContractAuthority', 'OffsettingReceipts']]\n",
    "y = df['Obligations']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# --- STEP 2: Define Regression Models ---\n",
    "regressors = {\n",
    "    \"LinearRegression\": LinearRegression(),\n",
    "    \"Ridge\": Ridge(),\n",
    "    \"Lasso\": Lasso(),\n",
    "    \"ElasticNet\": ElasticNet(),\n",
    "    \"BayesianRidge\": BayesianRidge(),\n",
    "    \"HuberRegressor\": HuberRegressor(),\n",
    "    \"SGDRegressor\": SGDRegressor(max_iter=1000, tol=1e-3),\n",
    "    \"DecisionTreeRegressor\": DecisionTreeRegressor(),\n",
    "    \"RandomForestRegressor\": RandomForestRegressor(),\n",
    "    \"GradientBoostingRegressor\": GradientBoostingRegressor(),\n",
    "    \"HistGradientBoostingRegressor\": HistGradientBoostingRegressor(),\n",
    "    \"AdaBoostRegressor\": AdaBoostRegressor(),\n",
    "    \"XGBRegressor\": XGBRegressor(verbosity=0, eval_metric='rmse'),\n",
    "    \"KNeighborsRegressor\": KNeighborsRegressor(),\n",
    "    \"SVR\": SVR(),\n",
    "    \"MLPRegressor\": MLPRegressor(max_iter=500)\n",
    "}\n",
    "\n",
    "# --- STEP 3: Train & Evaluate Models ---\n",
    "results = {}\n",
    "trained_models = {}\n",
    "\n",
    "for name, model in regressors.items():\n",
    "    try:\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "        results[name] = {\"R²\": round(r2, 5), \"RMSE\": round(rmse, 2)}\n",
    "        trained_models[name] = model\n",
    "    except Exception as e:\n",
    "        results[name] = {\"Error\": str(e)}\n",
    "\n",
    "# --- STEP 4: Print Summary ---\n",
    "print(\"\\n📊 Regression Summary (R² sorted):\")\n",
    "summary_df = pd.DataFrame(results).T.sort_values(by=\"R²\", ascending=False)\n",
    "print(summary_df)\n",
    "\n",
    "# --- STEP 5: Tree-Based Feature Importances ---\n",
    "tree_models = [\"RandomForestRegressor\", \"GradientBoostingRegressor\",\n",
    "               \"HistGradientBoostingRegressor\", \"AdaBoostRegressor\", \"XGBRegressor\"]\n",
    "\n",
    "for name in tree_models:\n",
    "    if name in trained_models and hasattr(trained_models[name], \"feature_importances_\"):\n",
    "        importances = trained_models[name].feature_importances_\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        plt.barh(X.columns, importances)\n",
    "        plt.title(f\"{name} - Tree-Based Feature Importances\")\n",
    "        plt.xlabel(\"Importance\")\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# --- STEP 6: Permutation Importance (Model Agnostic) ---\n",
    "baseline_model = trained_models[\"RandomForestRegressor\"]\n",
    "perm = permutation_importance(baseline_model, X_test_scaled, y_test, n_repeats=10, random_state=42)\n",
    "\n",
    "perm_sorted_idx = perm.importances_mean.argsort()\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.barh(np.array(X.columns)[perm_sorted_idx], perm.importances_mean[perm_sorted_idx])\n",
    "plt.title(\"Permutation Importance - RandomForest\")\n",
    "plt.xlabel(\"Mean Importance\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- STEP 7: SHAP Values (Model Agnostic) ---\n",
    "shap_model = trained_models[\"GradientBoostingRegressor\"]\n",
    "explainer = shap.Explainer(shap_model.predict, X_test_scaled)\n",
    "shap_values = explainer(X_test_scaled)\n",
    "\n",
    "# Summary Plot (SHAP)\n",
    "shap.plots.beeswarm(shap_values, max_display=10)\n",
    "\n"
   ],
   "id": "1791f6f169094e23"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
